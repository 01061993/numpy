/*
 * This file contains low-level loops for copying and byte-swapping
 * strided data.
 */

#define PY_SSIZE_T_CLEAN
#include "Python.h"
#include "structmember.h"

#define _MULTIARRAYMODULE
#include <numpy/ndarrayobject.h>
#include <numpy/ufuncobject.h>
#include <numpy/npy_cpu.h>

#include "lowlevel_strided_loops.h"

#if ! (defined(NPY_CPU_X86) || defined(NPY_CPU_AMD64))
//#  define NPY_USE_UNALIGNED_ACCESS
#endif

#define _NPY_NOP1(x) (x)
#define _NPY_NOP2(x) (x)
#define _NPY_NOP4(x) (x)
#define _NPY_NOP8(x) (x)

#define _NPY_SWAP2(x) (((((npy_uint16)x)&0xffu) << 8) | \
                       (((npy_uint16)x) >> 8))

#define _NPY_SWAP4(x) (((((npy_uint32)x)&0xffu) << 24) | \
                       ((((npy_uint32)x)&0xff00u) << 8) | \
                       ((((npy_uint32)x)&0xff0000u) >> 8) | \
                       (((npy_uint32)x) >> 24))

#define _NPY_SWAP_PAIR4(x) (((((npy_uint32)x)&0xffu) << 8) | \
                       ((((npy_uint32)x)&0xff00u) >> 8) | \
                       ((((npy_uint32)x)&0xff0000u) << 8) | \
                       ((((npy_uint32)x)&0xff000000u) >> 8))

#define _NPY_SWAP8(x) (((((npy_uint64)x)&0xffu) << 56) | \
                       ((((npy_uint64)x)&0xff00u) << 40) | \
                       ((((npy_uint64)x)&0xff0000u) << 24) | \
                       ((((npy_uint64)x)&0xff000000u) << 8) | \
                       ((((npy_uint64)x)&0xff00000000u) >> 8) | \
                       ((((npy_uint64)x)&0xff0000000000u) >> 24) | \
                       ((((npy_uint64)x)&0xff000000000000u) >> 40) | \
                       (((npy_uint64)x) >> 56))

#define _NPY_SWAP_PAIR8(x) (((((npy_uint64)x)&0xffu) << 24) | \
                       ((((npy_uint64)x)&0xff00u) << 8) | \
                       ((((npy_uint64)x)&0xff0000u) >> 8) | \
                       ((((npy_uint64)x)&0xff000000u) >> 24) | \
                       ((((npy_uint64)x)&0xff00000000u) << 24) | \
                       ((((npy_uint64)x)&0xff0000000000u) << 8) | \
                       ((((npy_uint64)x)&0xff000000000000u) >> 8) | \
                       ((((npy_uint64)x)&0xff00000000000000u) >> 24))

#define _NPY_SWAP_INPLACE2(x) { \
        char a = (x)[0]; (x)[0] = (x)[1]; (x)[1] = a; \
        }

#define _NPY_SWAP_INPLACE4(x) { \
        char a = (x)[0]; (x)[0] = (x)[3]; (x)[3] = a; \
        a = (x)[1]; (x)[1] = (x)[2]; (x)[2] = a; \
        }

#define _NPY_SWAP_INPLACE8(x) { \
        char a = (x)[0]; (x)[0] = (x)[7]; (x)[7] = a; \
        a = (x)[1]; (x)[1] = (x)[6]; (x)[6] = a; \
        a = (x)[2]; (x)[2] = (x)[5]; (x)[5] = a; \
        a = (x)[3]; (x)[3] = (x)[4]; (x)[4] = a; \
        }

#define _NPY_SWAP_INPLACE16(x) { \
        char a = (x)[0]; (x)[0] = (x)[15]; (x)[15] = a; \
        a = (x)[1]; (x)[1] = (x)[14]; (x)[14] = a; \
        a = (x)[2]; (x)[2] = (x)[13]; (x)[13] = a; \
        a = (x)[3]; (x)[3] = (x)[12]; (x)[12] = a; \
        a = (x)[4]; (x)[4] = (x)[11]; (x)[11] = a; \
        a = (x)[5]; (x)[5] = (x)[10]; (x)[10] = a; \
        a = (x)[6]; (x)[6] = (x)[9]; (x)[9] = a; \
        a = (x)[7]; (x)[7] = (x)[8]; (x)[8] = a; \
        }

/************* STRIDED COPYING/SWAPPING SPECIALIZED FUNCTIONS *************/

/**begin repeat
 * #elsize = 1, 2, 4, 8, 16#
 * #elsize_half = 0, 1, 2, 4, 8#
 * #type = npy_uint8, npy_uint16, npy_uint32, npy_uint64, npy_uint128#
 */
/**begin repeat1
 * #oper = strided_to_strided, strided_to_contig,
 *         contig_to_strided, contig_to_contig#
 * #src_contig = 0, 0, 1 ,1#
 * #dst_contig = 0, 1, 0 ,1#
 */
/**begin repeat2
 * #swap = _NPY_NOP, _NPY_NOP, _NPY_SWAP_INPLACE, _NPY_SWAP,
 *         _NPY_SWAP_INPLACE, _NPY_SWAP_PAIR#
 * #prefix = , _aligned, _swap, _aligned_swap, _swap_pair, _aligned_swap_pair#
 * #is_aligned = 0, 1, 0, 1, 0, 1#
 * #minelsize = 1, 1, 2, 2, 4, 4#
 * #is_swap = 0, 0, 1, 1, 2, 2#
 */

#if (@elsize@ >= @minelsize@) && \
    (@elsize@ > 1 || @is_aligned@) && \
    (!defined(NPY_USE_UNALIGNED_ACCESS) || @is_aligned@)


#if @is_swap@ || @src_contig@ == 0 || @dst_contig@ == 0
static void
@prefix@_@oper@_size@elsize@(char *dst, npy_intp dst_stride,
                        char *src, npy_intp src_stride,
                        npy_intp N, npy_intp NPY_UNUSED(itemsize),
                        void *NPY_UNUSED(data))
{
    /*printf("fn @prefix@_@oper@_size@elsize@\n");*/
    while (N > 0) {
#if @is_aligned@

        /* aligned copy and swap */
#  if @elsize@ != 16
        (*((@type@ *)dst)) = @swap@@elsize@(*((@type@ *)src));
#  else
#    if @is_swap@ == 0
        (*((npy_uint64 *)dst)) = (*((npy_uint64 *)src));
        (*((npy_uint64 *)dst + 1)) = (*((npy_uint64 *)src + 1));
#    elif @is_swap@ == 1
        (*((npy_uint64 *)dst)) = _NPY_SWAP8(*((npy_uint64 *)src + 1));
        (*((npy_uint64 *)dst + 1)) = _NPY_SWAP8(*((npy_uint64 *)src));
#    elif @is_swap@ == 2
        (*((npy_uint64 *)dst)) = _NPY_SWAP8(*((npy_uint64 *)src));
        (*((npy_uint64 *)dst + 1)) = _NPY_SWAP8(*((npy_uint64 *)src + 1));
#    endif
#  endif

#else

        /* unaligned copy and swap */
        memcpy(dst, src, @elsize@);
#  if @is_swap@ == 1
        @swap@@elsize@(dst);
#  elif @is_swap@ == 2
        @swap@@elsize_half@(dst);
        @swap@@elsize_half@(dst + @elsize_half@);
#  endif

#endif

#if @dst_contig@
        dst += @elsize@;
#else
        dst += dst_stride;
#endif

#if @src_contig@
        src += @elsize@;
#else
        src += src_stride;
#endif

        --N;
    }
}
#endif


/* specialized copy and swap for source stride 0 */
#if (@src_contig@ == 0) && @is_aligned@
static void
@prefix@_@oper@_size@elsize@_srcstride0(char *dst,
                        npy_intp dst_stride,
                        char *src, npy_intp NPY_UNUSED(src_stride),
                        npy_intp N, npy_intp NPY_UNUSED(itemsize),
                        void *NPY_UNUSED(data))
{
#if @elsize@ != 16
    @type@ temp = @swap@@elsize@(*((@type@ *)src));
#else
    npy_uint64 temp0, temp1;
#    if @is_swap@ == 0
        temp0 = (*((npy_uint64 *)src));
        temp1 = (*((npy_uint64 *)src + 1));
#    elif @is_swap@ == 1
        temp0 = _NPY_SWAP8(*((npy_uint64 *)src + 1));
        temp1 = _NPY_SWAP8(*((npy_uint64 *)src));
#    elif @is_swap@ == 2
        temp0 = _NPY_SWAP8(*((npy_uint64 *)src));
        temp1 = _NPY_SWAP8(*((npy_uint64 *)src + 1));
#    endif
#endif
    while (N > 0) {
#if @elsize@ != 16
        *((@type@ *)dst) = temp;
#else
        *((npy_uint64 *)dst) = temp0;
        *((npy_uint64 *)dst + 1) = temp1;
#endif
#if @dst_contig@
        dst += @elsize@;
#else
        dst += dst_stride;
#endif
        --N;
    }
}
#endif

#endif/* @elsize@ >= @minelsize@ */

/**end repeat2**/
/**end repeat1**/
/**end repeat**/

static void
_strided_to_strided(char *dst, npy_intp dst_stride,
                        char *src, npy_intp src_stride,
                        npy_intp N, npy_intp itemsize,
                        void *NPY_UNUSED(data))
{
    while (N > 0) {
        memcpy(dst, src, itemsize);
        dst += dst_stride;
        src += src_stride;
        --N;
    }
}

static void
_swap_strided_to_strided(char *dst, npy_intp dst_stride,
                        char *src, npy_intp src_stride,
                        npy_intp N, npy_intp itemsize,
                        void *NPY_UNUSED(data))
{
    char *a, *b, c;

    while (N > 0) {
        memcpy(dst, src, itemsize);
        /* general in-place swap */
        a = dst;
        b = dst + itemsize - 1;
        while (a < b) {
            c = *a;
            *a = *b;
            *b = c;
            ++a; --b;
        }
        dst += dst_stride;
        src += src_stride;
        --N;
    }
}

static void
_swap_pair_strided_to_strided(char *dst, npy_intp dst_stride,
                        char *src, npy_intp src_stride,
                        npy_intp N, npy_intp itemsize,
                        void *NPY_UNUSED(data))
{
    char *a, *b, c;
    npy_intp itemsize_half = itemsize / 2;

    while (N > 0) {
        memcpy(dst, src, itemsize);
        /* general in-place swap */
        a = dst;
        b = dst + itemsize_half - 1;
        while (a < b) {
            c = *a;
            *a = *b;
            *b = c;
            ++a; --b;
        }
        /* general in-place swap */
        a = dst + itemsize_half;
        b = dst + 2*itemsize_half - 1;
        while (a < b) {
            c = *a;
            *a = *b;
            *b = c;
            ++a; --b;
        }
        dst += dst_stride;
        src += src_stride;
        --N;
    }
}

static void
_strided_to_contig(char *dst, npy_intp NPY_UNUSED(dst_stride),
                        char *src, npy_intp src_stride,
                        npy_intp N, npy_intp itemsize,
                        void *NPY_UNUSED(data))
{
    while (N > 0) {
        memcpy(dst, src, itemsize);
        dst += itemsize;
        src += src_stride;
        --N;
    }
}

static void
_contig_to_strided(char *dst, npy_intp dst_stride,
                        char *src, npy_intp NPY_UNUSED(src_stride),
                        npy_intp N, npy_intp itemsize,
                        void *NPY_UNUSED(data))
{
    while (N > 0) {
        memcpy(dst, src, itemsize);
        dst += dst_stride;
        src += itemsize;
        --N;
    }
}

static void
_contig_to_contig(char *dst, npy_intp NPY_UNUSED(dst_stride),
                        char *src, npy_intp NPY_UNUSED(src_stride),
                        npy_intp N, npy_intp itemsize,
                        void *NPY_UNUSED(data))
{
    memcpy(dst, src, itemsize*N);
}


NPY_NO_EXPORT PyArray_StridedTransferFn
PyArray_GetStridedCopyFn(npy_intp aligned, npy_intp src_stride,
                         npy_intp dst_stride, npy_intp itemsize)
{
/*
 * Skip the "unaligned" versions on CPUs which support unaligned
 * memory accesses.
 */
#ifndef NPY_USE_UNALIGNED_ACCESS
    if (aligned) {
#endif/*!NPY_USE_UNALIGNED_ACCESS*/

        /* contiguous dst */
        if (itemsize != 0 && dst_stride == itemsize) {
            /* constant src */
            if (src_stride == 0) {
                switch (itemsize) {
/**begin repeat
 * #elsize = 1, 2, 4, 8, 16#
 */
                    case @elsize@:
                        return
                          &_aligned_strided_to_contig_size@elsize@_srcstride0;
/**end repeat**/
                }
            }
            /* contiguous src */
            else if (src_stride == itemsize) {
                return &_contig_to_contig;
            }
            /* general src */
            else {
                switch (itemsize) {
/**begin repeat
 * #elsize = 1, 2, 4, 8, 16#
 */
                    case @elsize@:
                        return &_aligned_strided_to_contig_size@elsize@;
/**end repeat**/
                }
            }

            return &_strided_to_contig;
        }
        /* general dst */
        else {
            /* constant src */
            if (src_stride == 0) {
                switch (itemsize) {
/**begin repeat
 * #elsize = 1, 2, 4, 8, 16#
 */
                    case @elsize@:
                        return
                          &_aligned_strided_to_strided_size@elsize@_srcstride0;
/**end repeat**/
                }
            }
            /* contiguous src */
            else if (src_stride == itemsize) {
                switch (itemsize) {
/**begin repeat
 * #elsize = 1, 2, 4, 8, 16#
 */
                    case @elsize@:
                        return &_aligned_contig_to_strided_size@elsize@;
/**end repeat**/
                }

                return &_contig_to_strided;
            }
            else {
                switch (itemsize) {
/**begin repeat
 * #elsize = 1, 2, 4, 8, 16#
 */
                    case @elsize@:
                        return &_aligned_strided_to_strided_size@elsize@;
/**end repeat**/
                }
            }
        }

#ifndef NPY_USE_UNALIGNED_ACCESS
    }
    else {
        /* contiguous dst */
        if (itemsize != 0 && dst_stride == itemsize) {
            /* contiguous src */
            if (itemsize != 0 && src_stride == itemsize) {
                return &_contig_to_contig;
            }
            /* general src */
            else {
                switch (itemsize) {
                    case 1:
                        return &_aligned_strided_to_contig_size1;
/**begin repeat
 * #elsize = 2, 4, 8, 16#
 */
                    case @elsize@:
                        return &_strided_to_contig_size@elsize@;
/**end repeat**/
                }
            }

            return &_strided_to_contig;
        }
        /* general dst */
        else {
            /* contiguous src */
            if (itemsize != 0 && src_stride == itemsize) {
                switch (itemsize) {
                    case 1:
                        return &_aligned_contig_to_strided_size1;
/**begin repeat
 * #elsize = 2, 4, 8, 16#
 */
                    case @elsize@:
                        return &_contig_to_strided_size@elsize@;
/**end repeat**/
                }

                return &_contig_to_strided;
            }
            /* general src */
            else {
                switch (itemsize) {
                    case 1:
                        return &_aligned_strided_to_strided_size1;
/**begin repeat
 * #elsize = 2, 4, 8, 16#
 */
                    case @elsize@:
                        return &_strided_to_strided_size@elsize@;
/**end repeat**/
                }
            }
        }
    }
#endif/*!NPY_USE_UNALIGNED_ACCESS*/

    return &_strided_to_strided;
}

/*
 * PyArray_GetStridedCopySwapFn and PyArray_GetStridedCopySwapPairFn are
 * nearly identical, so can do a repeat for them.
 */
/**begin repeat
 * #function = PyArray_GetStridedCopySwapFn, PyArray_GetStridedCopySwapPairFn#
 * #tag = , _pair#
 * #not_pair = 1, 0#
 */

NPY_NO_EXPORT PyArray_StridedTransferFn
@function@(npy_intp aligned, npy_intp src_stride,
                             npy_intp dst_stride, npy_intp itemsize)
{
/*
 * Skip the "unaligned" versions on CPUs which support unaligned
 * memory accesses.
 */
#ifndef NPY_USE_UNALIGNED_ACCESS
    if (aligned) {
#endif/*!NPY_USE_UNALIGNED_ACCESS*/

        /* contiguous dst */
        if (itemsize != 0 && dst_stride == itemsize) {
            /* constant src */
            if (src_stride == 0) {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return
                 &_aligned_swap@tag@_strided_to_contig_size@elsize@_srcstride0;
#endif
/**end repeat1**/
                }
            }
            /* contiguous src */
            else if (src_stride == itemsize) {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return &_aligned_swap@tag@_contig_to_contig_size@elsize@;
#endif
/**end repeat1**/
                }
            }
            /* general src */
            else {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return &_aligned_swap@tag@_strided_to_contig_size@elsize@;
#endif
/**end repeat1**/
                }
            }
        }
        /* general dst */
        else {
            /* constant src */
            if (src_stride == 0) {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return
                &_aligned_swap@tag@_strided_to_strided_size@elsize@_srcstride0;
#endif
/**end repeat1**/
                }
            }
            /* contiguous src */
            else if (src_stride == itemsize) {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return &_aligned_swap@tag@_contig_to_strided_size@elsize@;
#endif
/**end repeat1**/
                }

                return &_contig_to_strided;
            }
            else {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return &_aligned_swap@tag@_strided_to_strided_size@elsize@;
#endif
/**end repeat1**/
                }
            }
        }

#ifndef NPY_USE_UNALIGNED_ACCESS
    }
    else {
        /* contiguous dst */
        if (itemsize != 0 && dst_stride == itemsize) {
            /* contiguous src */
            if (itemsize != 0 && src_stride == itemsize) {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return &_swap@tag@_contig_to_contig_size@elsize@;
#endif
/**end repeat1**/
                }
            }
            /* general src */
            else {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                    case @elsize@:
                        return &_swap@tag@_strided_to_contig_size@elsize@;
#endif
/**end repeat1**/
                }
            }

            return &_strided_to_contig;
        }
        /* general dst */
        else {
            /* contiguous src */
            if (itemsize != 0 && src_stride == itemsize) {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return &_swap@tag@_contig_to_strided_size@elsize@;
#endif
/**end repeat1**/
                }

                return &_contig_to_strided;
            }
            /* general src */
            else {
                switch (itemsize) {
/**begin repeat1
 * #elsize = 2, 4, 8, 16#
 */
#if @not_pair@ || @elsize@ > 2
                case @elsize@:
                    return &_swap@tag@_strided_to_strided_size@elsize@;
#endif
/**end repeat1**/
                }
            }
        }
    }
#endif/*!NPY_USE_UNALIGNED_ACCESS*/

    return &_swap@tag@_strided_to_strided;
}

/**end repeat**/

/* Does a simple aligned cast */
typedef struct {
    void *freefunc;
    PyArray_VectorUnaryFunc *castfunc;
} _strided_cast_data;

static void
_aligned_strided_to_strided_cast(char *dst, npy_intp dst_stride,
                        char *src, npy_intp src_stride,
                        npy_intp N, npy_intp itemsize,
                        void *data)
{
    PyArray_VectorUnaryFunc *castfunc = ((_strided_cast_data *)data)->castfunc;

    while (N > 0) {
        castfunc(src, dst, 1, NULL, NULL);
        dst += dst_stride;
        src += src_stride;
        --N;
    }
}

static void
_aligned_contig_to_contig_cast(char *dst, npy_intp NPY_UNUSED(dst_stride),
                        char *src, npy_intp NPY_UNUSED(src_stride),
                        npy_intp N, npy_intp NPY_UNUSED(itemsize),
                        void *data)
{
    PyArray_VectorUnaryFunc *castfunc = ((_strided_cast_data *)data)->castfunc;

    castfunc(src, dst, N, NULL, NULL);
}

/* Wraps a transfer function + data in alignment code */
typedef struct {
    void *freefunc;
    PyArray_StridedTransferFn wrapped,
                tobuffer, frombuffer;
    void *wrappeddata;
    npy_intp src_itemsize, dst_itemsize;
    char *bufferin, *bufferout;
} _align_wrap_data;

/* transfer data free function */
void _align_wrap_data_free(_align_wrap_data *data)
{
    PyArray_FreeStridedTransferData(data->wrappeddata);
    PyArray_free(data);
}

static void
_strided_to_strided_contig_align_wrap(char *dst, npy_intp dst_stride,
                        char *src, npy_intp src_stride,
                        npy_intp N, npy_intp itemsize,
                        void *data)
{
    _align_wrap_data *d = (_align_wrap_data *)data;
    PyArray_StridedTransferFn wrapped = d->wrapped,
            tobuffer = d->tobuffer,
            frombuffer = d->frombuffer;
    npy_intp src_itemsize = d->src_itemsize, dst_itemsize = d->dst_itemsize;
    void *wrappeddata = d->wrappeddata;
    char *bufferin = d->bufferin, *bufferout = d->bufferout;

    for(;;) {
        if (N > 32) {
            tobuffer(bufferin, src_itemsize, src, src_stride, 32,
                                            src_itemsize, NULL);
            wrapped(bufferout, dst_itemsize, bufferin, src_itemsize, 32,
                                            itemsize, wrappeddata);
            frombuffer(dst, dst_stride, bufferout, dst_itemsize, 32,
                                            dst_itemsize, NULL);
            N -= 32;
            src += 32*src_stride;
            dst += 32*dst_stride;
        }
        else {
            tobuffer(bufferin, src_itemsize, src, src_stride, N,
                                            src_itemsize, NULL);
            wrapped(bufferout, dst_itemsize, bufferin, src_itemsize, N,
                                            itemsize, wrappeddata);
            frombuffer(dst, dst_stride, bufferout, dst_itemsize, N,
                                            dst_itemsize, NULL);
            return;
        }
    }
}

/*
 * Wraps an aligned contig to contig transfer function between either
 * copies or byte swaps to temporary buffers.
 *
 * src_itemsize/dst_itemsize - The sizes of the src and dst datatypes.
 * tobuffer - copy/swap function from src to an aligned contiguous buffer.
 *            data passed to 'tobuffer' is NULL.
 * frombuffer - copy/swap function from an aligned contiguous buffer to dst.
 *              data passed to 'frombuffer' is NULL.
 * wrapped - contig to contig transfer function being wrapped
 * wrappeddata - data for wrapped
 */
NPY_NO_EXPORT void
PyArray_WrapTransferFunction(npy_intp src_itemsize, npy_intp dst_itemsize,
            PyArray_StridedTransferFn tobuffer,
            PyArray_StridedTransferFn frombuffer,
            PyArray_StridedTransferFn wrapped, void *wrappeddata,
            PyArray_StridedTransferFn *outstransfer,
            void **outtransferdata)
{
    _align_wrap_data *data;
    npy_intp basedatasize, datasize;

    /* Round up the structure size to 16-byte boundary */
    basedatasize = (sizeof(_align_wrap_data)+15)&(-0x10);
    /* Add space for two 32-element buffers */
    datasize = basedatasize + 32*src_itemsize + 32*dst_itemsize;

    /* Allocate the data, and populate it */
    data = (_align_wrap_data *)PyArray_malloc(datasize);
    data->freefunc = (void *)&_align_wrap_data_free;
    data->tobuffer = tobuffer;
    data->frombuffer = frombuffer;
    data->wrapped = wrapped;
    data->wrappeddata = wrappeddata;
    data->src_itemsize = src_itemsize;
    data->dst_itemsize = dst_itemsize;
    data->bufferin = (char *)data + basedatasize;
    data->bufferout = data->bufferin + 32*src_itemsize;

    /* Set the function and data */
    *outstransfer = &_strided_to_strided_contig_align_wrap;
    *outtransferdata = data;
}


NPY_NO_EXPORT int
PyArray_GetTransferFunction(int aligned,
                            npy_intp src_stride, npy_intp dst_stride,
                            PyArray_Descr *src_dtype, PyArray_Descr *dst_dtype,
                            PyArray_StridedTransferFn *outstransfer,
                            void **outtransferdata)
{
    npy_intp src_itemsize = src_dtype->elsize,
             dst_itemsize = dst_dtype->elsize;
    int src_type_num = src_dtype->type_num,
        dst_type_num = dst_dtype->type_num;

    /* First look at the possibilities of just a copy or swap */
    if (src_itemsize == dst_itemsize && src_type_num < NPY_OBJECT &&
                                      dst_type_num < NPY_OBJECT &&
                                      src_dtype->kind == dst_dtype->kind) {
        /* This is a straight copy */
        if (src_itemsize == 1 || PyArray_ISNBO(src_dtype->byteorder) ==
                                 PyArray_ISNBO(dst_dtype->byteorder)) {
            *outstransfer = PyArray_GetStridedCopyFn(aligned,
                                        src_stride, dst_stride,
                                        src_itemsize);
            *outtransferdata = NULL;
            return (*outstransfer == NULL) ? NPY_FAIL : NPY_SUCCEED;
        }
        /* This is a straight copy + byte swap */
        else if (!PyTypeNum_ISCOMPLEX(src_type_num)) {
            *outstransfer = PyArray_GetStridedCopySwapFn(aligned,
                                        src_stride, dst_stride,
                                        src_itemsize);
            *outtransferdata = NULL;
            return (*outstransfer == NULL) ? NPY_FAIL : NPY_SUCCEED;
        }
        /* This is a straight copy + element pair byte swap */
        else {
            *outstransfer = PyArray_GetStridedCopySwapPairFn(aligned,
                                        src_stride, dst_stride,
                                        src_itemsize);
            *outtransferdata = NULL;
            return (*outstransfer == NULL) ? NPY_FAIL : NPY_SUCCEED;
        }
    }

    /* Check whether a simple cast and some swaps will suffice */
    if (src_type_num < NPY_OBJECT && dst_type_num < NPY_OBJECT) {
        _strided_cast_data *data;
        PyArray_VectorUnaryFunc *castfunc;

        /* Get the cast function */
        castfunc = PyArray_GetCastFunc(src_dtype, dst_type_num);
        if (!castfunc) {
            *outstransfer = NULL;
            *outtransferdata = NULL;
            return NPY_FAIL;
        }

        /* Allocate the data for the casting */
        data = PyArray_malloc(sizeof(_strided_cast_data));
        if (data == NULL) {
            PyErr_NoMemory();
            *outstransfer = NULL;
            *outtransferdata = NULL;
            return NPY_FAIL;
        }
        data->freefunc = (void*)&(PyArray_free);
        data->castfunc = castfunc;


        /* If it's aligned and all native byte order, we're all done */
        if (aligned && PyArray_ISNBO(src_dtype->byteorder) &&
                       PyArray_ISNBO(dst_dtype->byteorder)) {
            /* Choose the contiguous cast if we can */
            if (src_stride == src_itemsize && dst_stride == dst_itemsize) {
                *outstransfer = _aligned_contig_to_contig_cast;
            }
            else {
                *outstransfer = _aligned_strided_to_strided_cast;
            }
            *outtransferdata = data;

            return NPY_SUCCEED;
        }
        /* Otherwise, we have to copy and/or swap to aligned temporaries */
        else {
            PyArray_StridedTransferFn tobuffer, frombuffer, casttransfer;

            /* Get the copy/swap operation from src */
            if (src_itemsize == 1 || PyArray_ISNBO(src_dtype->byteorder)) {
                tobuffer = PyArray_GetStridedCopyFn(aligned,
                                            src_stride, src_itemsize,
                                            src_itemsize);
            }
            else if(!PyTypeNum_ISCOMPLEX(src_type_num)) {
                tobuffer = PyArray_GetStridedCopySwapFn(aligned,
                                            src_stride, src_itemsize,
                                            src_itemsize);
            }
            else {
                tobuffer = PyArray_GetStridedCopySwapPairFn(aligned,
                                            src_stride, src_itemsize,
                                            src_itemsize);
            }

            /* Get the copy/swap operation to dst */
            if (dst_itemsize == 1 || PyArray_ISNBO(dst_dtype->byteorder)) {
                frombuffer = PyArray_GetStridedCopyFn(aligned,
                                            dst_itemsize, dst_stride,
                                            dst_itemsize);
            }
            else if(!PyTypeNum_ISCOMPLEX(dst_type_num)) {
                frombuffer = PyArray_GetStridedCopySwapFn(aligned,
                                            dst_itemsize, dst_stride,
                                            dst_itemsize);
            }
            else {
                frombuffer = PyArray_GetStridedCopySwapPairFn(aligned,
                                            dst_itemsize, dst_stride,
                                            dst_itemsize);
            }

            /* Use the aligned contiguous cast */
            casttransfer = &_aligned_contig_to_contig_cast;

            /* Wrap it all up in a new transfer function + data */
            PyArray_WrapTransferFunction(src_itemsize, dst_itemsize,
                                tobuffer, frombuffer,
                                casttransfer, data,
                                outstransfer, outtransferdata);

            return NPY_SUCCEED;
        }
    }

    /* TODO check for fields & subarrays */

    /* TODO: write the more complicated transfer code! */
    *outstransfer = NULL;
    *outtransferdata = NULL;
    PyErr_SetString(PyExc_RuntimeError,
            "General transfer function support has not been written yet");
    return NPY_FAIL;
}

typedef void (*_npy_stridedtransfer_dealloc)(void *);
NPY_NO_EXPORT void
PyArray_FreeStridedTransferData(void *transferdata)
{
    if (transferdata != NULL) {
        _npy_stridedtransfer_dealloc dealloc =
                        *((_npy_stridedtransfer_dealloc *)transferdata);
        dealloc(transferdata);
    }
}



NPY_NO_EXPORT npy_intp
PyArray_TransferNDimToStrided(npy_intp ndim,
                char *dst, npy_intp dst_stride,
                char *src, npy_intp *src_strides, npy_intp src_strides_inc,
                npy_intp *coords, npy_intp coords_inc,
                npy_intp *shape, npy_intp shape_inc,
                npy_intp count, npy_intp itemsize,
                PyArray_StridedTransferFn stransfer,
                void *data)
{
    npy_intp i, M, N, coord0, shape0, src_stride0, coord1, shape1, src_stride1;

    /* Finish off dimension 0 */
    coord0 = coords[0];
    shape0 = shape[0];
    src_stride0 = src_strides[0];
    N = shape0 - coord0;
    if (N >= count) {
        stransfer(dst, dst_stride, src, src_stride0, count, itemsize, data);
        return 0;
    }
    stransfer(dst, dst_stride, src, src_stride0, N, itemsize, data);
    count -= N;

    /* If it's 1-dimensional, there's no more to copy */
    if (ndim == 1) {
        return count;
    }

    /* Adjust the src and dst pointers */
    coord1 = (coords + coords_inc)[0];
    shape1 = (shape + shape_inc)[0];
    src_stride1 = (src_strides + src_strides_inc)[0];
    src = src - coord0*src_stride0 + src_stride1;
    dst += N*dst_stride;

    /* Finish off dimension 1 */
    M = (shape1 - coord1 - 1);
    N = shape0*M;
    for (i = 0; i < M; ++i) {
        if (shape0 >= count) {
            stransfer(dst, dst_stride, src, src_stride0,
                        count, itemsize, data);
            return 0;
        }
        else {
            stransfer(dst, dst_stride, src, src_stride0,
                        shape0, itemsize, data);
        }
        count -= shape0;
        src += src_stride1;
        dst += shape0*dst_stride;
    }

    /* If it's 2-dimensional, there's no more to copy */
    if (ndim == 2) {
        return count;
    }

    /* General-case loop for everything else */
    else {
        /* Iteration structure for dimensions 2 and up */
        struct {
            npy_intp coord, shape, src_stride;
        } it[NPY_MAXDIMS];

        /* Copy the coordinates and shape */
        coords += 2*coords_inc;
        shape += 2*shape_inc;
        src_strides += 2*src_strides_inc;
        for (i = 0; i < ndim-2; ++i) {
            it[i].coord = coords[0];
            it[i].shape = shape[0];
            it[i].src_stride = src_strides[0];
            coords += coords_inc;
            shape += shape_inc;
            src_strides += src_strides_inc;
        }

        for (;;) {
            /* Adjust the src pointer from the dimension 0 and 1 loop */
            src = src - shape1*src_stride1;

            /* Increment to the next coordinate */
            for (i = 0; i < ndim-2; ++i) {
                src += it[i].src_stride;
                if (++it[i].coord >= it[i].shape) {
                    it[i].coord = 0;
                    src -= it[i].src_stride*it[i].shape;
                }
                else {
                    break;
                }
            }
            /* If the last dimension rolled over, we're done */
            if (i == ndim-2) {
                return count;
            }

            /* A loop for dimensions 0 and 1 */
            for (i = 0; i < shape1; ++i) {
                if (shape0 >= count) {
                    stransfer(dst, dst_stride, src, src_stride0,
                                count, itemsize, data);
                    return 0;
                }
                else {
                    stransfer(dst, dst_stride, src, src_stride0,
                                shape0, itemsize, data);
                }
                count -= shape0;
                src += src_stride1;
                dst += shape0*dst_stride;
            }
        }
    }
}

NPY_NO_EXPORT npy_intp
PyArray_TransferStridedToNDim(npy_intp ndim,
                char *dst, npy_intp *dst_strides, npy_intp dst_strides_inc,
                char *src, npy_intp src_stride,
                npy_intp *coords, npy_intp coords_inc,
                npy_intp *shape, npy_intp shape_inc,
                npy_intp count, npy_intp itemsize,
                PyArray_StridedTransferFn stransfer,
                void *data)
{
    npy_intp i, M, N, coord0, shape0, dst_stride0, coord1, shape1, dst_stride1;

    /* Finish off dimension 0 */
    coord0 = coords[0];
    shape0 = shape[0];
    dst_stride0 = dst_strides[0];
    N = shape0 - coord0;
    if (N >= count) {
        stransfer(dst, dst_stride0, src, src_stride, count, itemsize, data);
        return 0;
    }
    stransfer(dst, dst_stride0, src, src_stride, N, itemsize, data);
    count -= N;

    /* If it's 1-dimensional, there's no more to copy */
    if (ndim == 1) {
        return count;
    }

    /* Adjust the src and dst pointers */
    coord1 = (coords + coords_inc)[0];
    shape1 = (shape + shape_inc)[0];
    dst_stride1 = (dst_strides + dst_strides_inc)[0];
    dst = dst - coord0*dst_stride0 + dst_stride1;
    src += N*src_stride;

    /* Finish off dimension 1 */
    M = (shape1 - coord1 - 1);
    N = shape0*M;
    for (i = 0; i < M; ++i) {
        if (shape0 >= count) {
            stransfer(dst, dst_stride0, src, src_stride,
                        count, itemsize, data);
            return 0;
        }
        else {
            stransfer(dst, dst_stride0, src, src_stride,
                        shape0, itemsize, data);
        }
        count -= shape0;
        dst += dst_stride1;
        src += shape0*src_stride;
    }

    /* If it's 2-dimensional, there's no more to copy */
    if (ndim == 2) {
        return count;
    }

    /* General-case loop for everything else */
    else {
        /* Iteration structure for dimensions 2 and up */
        struct {
            npy_intp coord, shape, dst_stride;
        } it[NPY_MAXDIMS];

        /* Copy the coordinates and shape */
        coords += 2*coords_inc;
        shape += 2*shape_inc;
        dst_strides += 2*dst_strides_inc;
        for (i = 0; i < ndim-2; ++i) {
            it[i].coord = coords[0];
            it[i].shape = shape[0];
            it[i].dst_stride = dst_strides[0];
            coords += coords_inc;
            shape += shape_inc;
            dst_strides += dst_strides_inc;
        }

        for (;;) {
            /* Adjust the dst pointer from the dimension 0 and 1 loop */
            dst = dst - shape1*dst_stride1;

            /* Increment to the next coordinate */
            for (i = 0; i < ndim-2; ++i) {
                dst += it[i].dst_stride;
                if (++it[i].coord >= it[i].shape) {
                    it[i].coord = 0;
                    dst -= it[i].dst_stride*it[i].shape;
                }
                else {
                    break;
                }
            }
            /* If the last dimension rolled over, we're done */
            if (i == ndim-2) {
                return count;
            }

            /* A loop for dimensions 0 and 1 */
            for (i = 0; i < shape1; ++i) {
                if (shape0 >= count) {
                    stransfer(dst, dst_stride0, src, src_stride,
                                count, itemsize, data);
                    return 0;
                }
                else {
                    stransfer(dst, dst_stride0, src, src_stride,
                                shape0, itemsize, data);
                }
                count -= shape0;
                dst += dst_stride1;
                src += shape0*src_stride;
            }
        }
    }
}
