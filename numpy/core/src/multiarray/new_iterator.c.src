#define PY_SSIZE_T_CLEAN
#include "Python.h"
#include "structmember.h"

#define _MULTIARRAYMODULE
#include <numpy/ndarrayobject.h>

#include "new_iterator.h"
#include "lowlevel_strided_loops.h"

/* Rounds up a number of bytes to be divisible by sizeof intp */
#if NPY_SIZEOF_INTP == 4
#define NPY_INTP_ALIGNED(size) ((size + 0x3)&(-0x4))
#else
#define NPY_INTP_ALIGNED(size) ((size + 0x7)&(-0x8))
#endif

/* Internal iterator flags */

/* The perm is the identity */
#define NPY_ITFLAG_IDENTPERM   0x001
/* The perm has negative entries (indicating flipped axes) */
#define NPY_ITFLAG_NEGPERM     0x002
/* The iterator is tracking an index */
#define NPY_ITFLAG_HASINDEX    0x004
/* The iterator is tracking coordinates */
#define NPY_ITFLAG_HASCOORDS   0x008
/* The iterator returns offsets instead of pointers */
#define NPY_ITFLAG_HASOFFSETS  0x010
/* The iteration order was forced on construction */
#define NPY_ITFLAG_FORCEDORDER 0x020
/* The inner loop is handled outside the iterator */
#define NPY_ITFLAG_NOINNER     0x040
/* The iterator is buffered */
#define NPY_ITFLAG_BUFFER      0x080
/* The iterator is buffered */
#define NPY_ITFLAG_GROWINNER   0x100

/* Internal iterator per-operand iterator flags */

/* The operand will be written to */
#define NPY_OP_ITFLAG_WRITE        0x01
/* The operand will be read from */
#define NPY_OP_ITFLAG_READ         0x02
/* The operand may have a temporary copy made */
#define NPY_OP_ITFLAG_COPY         0x04
/* The operand needs casting */
#define NPY_OP_ITFLAG_CAST         0x08
/* The operand needs a byte swap and/or alignment operation */
#define NPY_OP_ITFLAG_COPYSWAP     0x10
/* The operand never needs buffering */
#define NPY_OP_ITFLAG_BUFNEVER     0x20
/* The operand is aligned */
#define NPY_OP_ITFLAG_ALIGNED      0x40

/* Internal flag, for the type of operands */
#define NPY_ITER_OP_ARRAY         0
#define NPY_ITER_OP_NULL          1

/*
 * The data layout of the iterator is fully specified by
 * a triple (itflags, ndim, niter).  These three variables
 * are expected to exist in all functions calling these macros,
 * either as true variables initialized to the correct values
 * from the iterator, or as constants in the case of specialized
 * functions such as the various iternext functions.
 *
 * If this layout changes, the function 'npyiter_shrink_ndim'
 * needs to be changed as well.
 */

/* Byte sizes of the iterator members */
#define NIT_ITERSIZE_SIZEOF() \
        (NPY_SIZEOF_INTP)
#define NIT_PERM_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(ndim))
#define NIT_DTYPES_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(niter))
#define NIT_RESETDATAPTR_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(niter+1))
#define NIT_OBJECTS_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(niter))
#define NIT_OPITFLAGS_SIZEOF(itflags, ndim, niter) \
        (NPY_INTP_ALIGNED(niter))
#define NIT_BUFFERDATA_SIZEOF(itflags, ndim, niter) \
        ((itflags&NPY_ITFLAG_BUFFER) ? ((NPY_SIZEOF_INTP)*(3 + 7*niter)) : 0)

/* Byte offsets of the iterator members */
#define NIT_ITERSIZE_OFFSET() \
        (8)
#define NIT_PERM_OFFSET() \
        (NIT_ITERSIZE_OFFSET() + \
         NIT_ITERSIZE_SIZEOF())
#define NIT_DTYPES_OFFSET(itflags, ndim, niter) \
        (NIT_PERM_OFFSET() + \
         NIT_PERM_SIZEOF(itflags, ndim, niter))
#define NIT_RESETDATAPTR_OFFSET(itflags, ndim, niter) \
        (NIT_DTYPES_OFFSET(itflags, ndim, niter) + \
         NIT_DTYPES_SIZEOF(itflags, ndim, niter))
#define NIT_OBJECTS_OFFSET(itflags, ndim, niter) \
        (NIT_RESETDATAPTR_OFFSET(itflags, ndim, niter) + \
         NIT_RESETDATAPTR_SIZEOF(itflags, ndim, niter))
#define NIT_OPITFLAGS_OFFSET(itflags, ndim, niter) \
        (NIT_OBJECTS_OFFSET(itflags, ndim, niter) + \
         NIT_OBJECTS_SIZEOF(itflags, ndim, niter))
#define NIT_BUFFERDATA_OFFSET(itflags, ndim, niter) \
        (NIT_OPITFLAGS_OFFSET(itflags, ndim, niter) + \
         NIT_OPITFLAGS_SIZEOF(itflags, ndim, niter))
#define NIT_AXISDATA_OFFSET(itflags, ndim, niter) \
        (NIT_BUFFERDATA_OFFSET(itflags, ndim, niter) + \
         NIT_BUFFERDATA_SIZEOF(itflags, ndim, niter))

/* Internal-only ITERATOR DATA MEMBER ACCESS */
#define NIT_ITFLAGS(iter) \
        (*((npy_uint32*)(iter)))
#define NIT_NDIM(iter) \
        (*((npy_uint16*)(iter) + 2))
#define NIT_NITER(iter) \
        (*((npy_uint16*)(iter) + 3))
#define NIT_ITERSIZE(iter) (*((npy_intp *)( \
        (char*)(iter) + NIT_ITERSIZE_OFFSET())))
#define NIT_PERM(iter)  ((npy_intp*)( \
        (char*)(iter) + NIT_PERM_OFFSET()))
#define NIT_DTYPES(iter) ((PyArray_Descr **)( \
        (char*)(iter) + NIT_DTYPES_OFFSET(itflags, ndim, niter)))
#define NIT_RESETDATAPTR(iter) ((char **)( \
        (char*)(iter) + NIT_RESETDATAPTR_OFFSET(itflags, ndim, niter)))
#define NIT_OBJECTS(iter) ((PyArrayObject **)( \
        (char*)(iter) + NIT_OBJECTS_OFFSET(itflags, ndim, niter)))
#define NIT_OPITFLAGS(iter) ( \
        (char*)(iter) + NIT_OPITFLAGS_OFFSET(itflags, ndim, niter))
#define NIT_BUFFERDATA(iter) \
        ((char*)(iter) + NIT_BUFFERDATA_OFFSET(itflags, ndim, niter))
#define NIT_AXISDATA(iter) \
        ((char*)(iter) + NIT_AXISDATA_OFFSET(itflags, ndim, niter))

/* Internal-only BUFFERDATA MEMBER ACCESS */
#define NBF_BUFFERSIZE(bufferdata) (*((npy_intp *)(bufferdata)))
#define NBF_SIZE(bufferdata) (*((npy_intp *)(bufferdata) + 1))
#define NBF_POS(bufferdata) (*((npy_intp *)(bufferdata) + 2))
#define NBF_STRIDES(bufferdata) ((npy_intp *)(bufferdata) + 3)
#define NBF_PTRS(bufferdata) ((char **)(bufferdata) + 3 + 1*(niter))
#define NBF_READTRANSFERFN(bufferdata) \
                ((PyArray_StridedTransferFn *)(bufferdata) + 3 + 2*(niter))
#define NBF_READTRANSFERDATA(bufferdata) \
                ((void **)(bufferdata) + 3 + 3*(niter))
#define NBF_WRITETRANSFERFN(bufferdata) \
                ((PyArray_StridedTransferFn *)(bufferdata) + 3 + 4*(niter))
#define NBF_WRITETRANSFERDATA(bufferdata) \
                ((void **)(bufferdata) + 3 + 5*(niter))
#define NBF_BUFFERS(bufferdata) ((char **)(bufferdata) + 3 + 6*(niter))

/* Internal-only AXISDATA MEMBER ACCESS. */
#define NAD_SHAPE(axisdata) (*((npy_intp *)(axisdata)))
#define NAD_COORD(axisdata) (*((npy_intp *)(axisdata) + 1))
#define NAD_STRIDES(axisdata) ((npy_intp *)(axisdata) + 2)
#define NAD_PTRS(axisdata) \
        ((char**)(axisdata) + 2 + NAD_NSTRIDES())
#define NAD_NSTRIDES() \
        ((niter) + ((itflags&NPY_ITFLAG_HASINDEX) ? 1 : 0))

/* Size of one AXISDATA struct within the iterator */
#define NIT_SIZEOF_AXISDATA(itflags, ndim, niter) (( \
        /* intp shape */ \
        1 + \
        /* intp coord */ \
        1 + \
        /* intp stride[niter] AND char* ptr[niter] */ \
        2*(niter) + \
        /* intp indexstride AND intp index (when index is provided) */ \
        ((itflags&NPY_ITFLAG_HASINDEX) ? 2 : 0) \
        )*NPY_SIZEOF_INTP ) \

/* Size of the whole iterator */
#define NIT_SIZEOF_ITERATOR(itflags, ndim, niter) ( \
        NIT_AXISDATA_OFFSET(itflags, ndim, niter) + \
        NIT_SIZEOF_AXISDATA(itflags, ndim, niter)*(ndim))

/* Internal helper functions */
static int
pyiter_check_global_flags(npy_uint32 flags, npy_uint32* itflags);
static int
npyiter_check_per_op_flags(npy_uint32 flags, char *op_itflags);
static int
pyiter_prepare_operand(PyArrayObject **op, PyArray_Descr *op_request_dtype,
                       PyArray_Descr** op_dtype, int* op_type,
                       npy_intp* op_ndim,
                       npy_uint32 op_flags, char *op_itflags);
static int
npyiter_check_casting(npy_intp niter, PyArrayObject **op,
                    PyArray_Descr **op_dtype,
                    NPY_CASTING casting,
                    char *op_itflags);
static int
npyiter_fill_axisdata(NpyIter *iter, PyArrayObject **op,
                      npy_intp *op_ndim, char **op_dataptr,
                      npy_uint32 *op_flags, npy_intp **op_axes);
static void
npyiter_replace_axisdata(NpyIter *iter, npy_intp iiter,
                      PyArrayObject *op,
                      npy_intp op_ndim, char *op_dataptr,
                      npy_intp *op_axes);
static void
npyiter_compute_index_strides(NpyIter *iter, npy_uint32 flags);
static void
npyiter_apply_forced_iteration_order(NpyIter *iter, NPY_ORDER order);

static void
npyiter_flip_negative_strides(NpyIter *iter);
static void
npyiter_reverse_axis_ordering(NpyIter *iter);
static void 
npyiter_find_best_axis_ordering(NpyIter *iter);
static void 
npyiter_coalesce_axes(NpyIter *iter);
static void
npyiter_shrink_ndim(NpyIter *iter, npy_intp new_ndim);

static PyArray_Descr *
npyiter_get_common_dtype(npy_intp niter, PyArrayObject **op, npy_intp *op_ndim,
                        char *op_itflags, PyArray_Descr **op_dtype,
                        int only_inputs);
static int
npyiter_promote_types(int type1, int type2);
static int
npyiter_can_cast(PyArray_Descr *from, PyArray_Descr *to, NPY_CASTING casting);

static PyArrayObject *
npyiter_new_temp_array(NpyIter *iter, PyTypeObject *subtype,
                npy_intp op_ndim, npy_intp *shape,
                PyArray_Descr *op_dtype, npy_intp *op_axes);

static int
npyiter_allocate_buffers(NpyIter *iter);
static int
npyiter_jumpforward(NpyIter *iter, npy_intp count);
static void
npyiter_copy_from_buffers(NpyIter *iter);
static void
npyiter_copy_to_buffers(NpyIter *iter);

/* The constructor for an iterator over multiple objects */
NpyIter*
NpyIter_MultiNew(npy_intp niter, PyArrayObject **op_in, npy_uint32 flags,
                 NPY_ORDER order, NPY_CASTING casting,
                 npy_uint32 *op_flags,
                 PyArray_Descr **op_request_dtypes,
                 npy_intp oa_ndim, npy_intp **op_axes, npy_intp buffersize)
{
    npy_uint32 itflags = NPY_ITFLAG_IDENTPERM;
    npy_intp idim, ndim = 0;
    npy_intp iiter;

    /* The iterator being constructed */
    NpyIter *iter;

    /* Per-operand values */
    PyArrayObject *op[NPY_MAXARGS];
    PyArray_Descr *op_dtype[NPY_MAXARGS];
    int op_type[NPY_MAXARGS];
    char op_itflags[NPY_MAXARGS];
    npy_intp op_ndim[NPY_MAXARGS];
    char **op_dataptr;

    npy_intp *perm;
    char *bufferdata = NULL;
    char axes_dupcheck[NPY_MAXDIMS];
    int any_allocate_if_null = 0, any_missing_dtypes = 0,
            allocate_output_scalars = 0;
    /* The subtype for automatically allocated outputs */
    double subtype_priority = NPY_PRIORITY;
    PyTypeObject *subtype = &PyArray_Type;

    if (niter > NPY_MAXARGS) {
        PyErr_Format(PyExc_ValueError,
            "Cannot construct an iterator with more than %d operands "
            "(%d were requested)", (int)NPY_MAXARGS, (int)niter);
        return NULL;
    }

    /* Error check 'oa_ndim' and 'op_axes', which must be used together */
    if (oa_ndim == 0 && op_axes != NULL) {
        PyErr_Format(PyExc_ValueError,
                "If 'op_axes' is not NULL in the iterator constructor, "
                "'oa_ndim' must be greater than zero");
        return NULL;
    }
    else if (oa_ndim > 0) {
        if (oa_ndim > NPY_MAXDIMS) {
            PyErr_Format(PyExc_ValueError,
                "Cannot construct an iterator with more than %d dimensions "
                "(%d were requested for op_axes)",
                (int)NPY_MAXDIMS, (int)oa_ndim);
            return NULL;
        }
        else if (op_axes == NULL) {
            PyErr_Format(PyExc_ValueError,
                    "If 'oa_ndim' is greater than zero in the iterator "
                    "constructor, then op_axes cannot be NULL");
            return NULL;
        }

        /* Check that there are no duplicates in op_axes */
        for (iiter = 0; iiter < niter; ++iiter) {
            npy_intp *axes = op_axes[iiter];
            if (axes != NULL) {
                memset(axes_dupcheck, 0, oa_ndim);
                for (idim = 0; idim < oa_ndim; ++idim) {
                    npy_intp i = axes[idim];
                    if (i >= 0) {
                        if (i >= NPY_MAXDIMS ||
                                axes_dupcheck[i] == 1) {
                            PyErr_Format(PyExc_ValueError,
                                    "The 'op_axes' provided to the iterator "
                                    "constructor contained duplicate "
                                    "or invalid values");
                            return NULL;
                        }
                        else {
                            axes_dupcheck[i] = 1;
                        }
                    }
                }
            }
        }
    }

    /* Checks the global iterator flags */
    if (!pyiter_check_global_flags(flags, &itflags)) {
        return NULL;
    }

    /* If offsets were requested, make sure copying is disabled */
    if (itflags&NPY_ITFLAG_HASOFFSETS) {
        for (iiter = 0; iiter < niter; ++iiter) {
            if (op_flags[iiter]&(NPY_ITER_COPY|
                        NPY_ITER_UPDATEIFCOPY)) {
                PyErr_SetString(PyExc_ValueError,
                        "If the iterator flag NPY_ITER_OFFSETS is used, "
                        "copying and buffering must not be enabled");
                return NULL;
            }
        }
    }

    /*
     * If buffering is enabled, and no buffersize was given, use a default
     * chosen to be big enough to get some amortization benefits, but
     * small enough to be cache-friendly.
     */
    if (itflags&NPY_ITFLAG_BUFFER && buffersize <= 0) {
        buffersize = 1 << 12;
    }

    /* Prepare all the operands */
    for (iiter = 0; iiter < niter; ++iiter) {
        /*
         * Make a copy of the input operands so we can substitute
         * new values in place when necessary without affecting
         * the caller's array.
         */
        op[iiter] = op_in[iiter];
        Py_XINCREF(op[iiter]);
        op_dtype[iiter] = NULL;

        /* Check the readonly/writeonly flags, and fill in op_itflags */
        if (!npyiter_check_per_op_flags(op_flags[iiter], &op_itflags[iiter])) {
            npy_intp i;

            for (i = 0; i <= iiter; ++i) {
                Py_XDECREF(op[i]);
                Py_XDECREF(op_dtype[i]);
            }
            return NULL;
        }

        /*
         * Prepare the operand.  This produces an op_dtype[iiter] reference
         * on success.
         */
        if (!pyiter_prepare_operand(&op[iiter],
                        op_request_dtypes ? op_request_dtypes[iiter] : NULL,
                        &op_dtype[iiter], &op_type[iiter],
                        &op_ndim[iiter],
                        op_flags[iiter], &op_itflags[iiter])) {
            npy_intp i;

            for (i = 0; i <= iiter; ++i) {
                Py_XDECREF(op[i]);
                Py_XDECREF(op_dtype[i]);
            }
            return NULL;
        }
        /* The iterator dimensions is the maximum of all the inputs */
        if (op_ndim[iiter] > ndim) {
            ndim = op_ndim[iiter];
        }
    }


    /* If all the operands were NULL, it's an error */
    if (op_type[0] == NPY_ITER_OP_NULL) {
        int all_null = 1;
        for (iiter = 1; iiter < niter; ++iiter) {
            if (op_type[iiter] != NPY_ITER_OP_NULL) {
                all_null = 0;
                break;
            }
        }
        if (all_null) {
            npy_intp i;

            for (i = 0; i < niter; ++i) {
                Py_XDECREF(op[i]);
                Py_XDECREF(op_dtype[i]);
            }
            PyErr_SetString(PyExc_ValueError,
                    "At least one iterator input must be non-NULL");
            return NULL;
        }
    }

    /* If 'op_axes' is being used, force 'ndim' */
    if (oa_ndim > 0) {
        ndim = oa_ndim;
    }
    /* If 'ndim' is still zero, any outputs should scalars */
    else if (ndim == 0) {
        allocate_output_scalars = 1;
        ndim = 1;
    }

    /* Allocate memory for the iterator */
    iter = (NpyIter*)
                PyArray_malloc(NIT_SIZEOF_ITERATOR(itflags, ndim, niter));

    /* Fill in the base data */
    NIT_ITFLAGS(iter) = itflags;
    NIT_NDIM(iter) = ndim;
    NIT_NITER(iter) = niter;
    NIT_ITERSIZE(iter) = 1;
    op_dataptr = NIT_RESETDATAPTR(iter);
    /* The iterator takes over ownership of the function's 'op_dtype'
     * and 'op' references */
    for (iiter = 0; iiter < niter; ++iiter) {
        NIT_DTYPES(iter)[iiter] = op_dtype[iiter];
        NIT_OBJECTS(iter)[iiter] = op[iiter];

        if (itflags&NPY_ITFLAG_HASOFFSETS) {
            /* If offsets were requested, set the data pointers to zero */
            op_dataptr[iiter] = 0;
        }
        else {
            /* Get the data pointer for this operand */
            switch (op_type[iiter]) {
                case NPY_ITER_OP_ARRAY:
                    /*
                     * Array casting/copying is handled later, once the
                     * iteration order is finalized.  Here, we
                     * optimistically assume the array will be used
                     * as is.
                     */
                    op_dataptr[iiter] = PyArray_DATA(op[iiter]);
                    break;

                case NPY_ITER_OP_NULL:
                    op_dataptr[iiter] = NULL;
                    /* Now that ndim is fixed, outputs get the full ndim */
                    if (allocate_output_scalars) {
                        op_ndim[iiter] = 0;
                    }
                    else {
                        op_ndim[iiter] = ndim;
                    }
                    /* Flag this so later we can avoid flipping axes */
                    any_allocate_if_null = 1;
                    /*
                     * If the data type wasn't provided, will need to
                     * calculate it later.
                     */
                    if (op_dtype[iiter] == NULL) {
                        any_missing_dtypes = 1;
                    }
                    break;
            }
        }
    }
    /* Set resetindex to zero as well (it's just after the resetdataptr) */
    op_dataptr[niter] = 0;

    /*
     * Initialize buffer data (must set the buffers and transferdata
     * to NULL before we might deallocate the iterator).
     */
    if (itflags&NPY_ITFLAG_BUFFER) {
        bufferdata = NIT_BUFFERDATA(iter);
        memset(NBF_BUFFERS(bufferdata), 0, (niter+1)*NPY_SIZEOF_INTP);
        for (iiter = 0; iiter < niter; ++iiter) {
            NBF_READTRANSFERDATA(bufferdata)[iiter] = NULL;
            NBF_WRITETRANSFERDATA(bufferdata)[iiter] = NULL;
        }
    }

    /* Fill in the AXISDATA arrays and set the ITERSIZE field */
    if (!npyiter_fill_axisdata(iter, op, op_ndim, op_dataptr,
                                                        op_flags, op_axes)) {
        NpyIter_Deallocate(iter);
        return NULL;
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        /* No point in a buffer bigger than the iteration size */
        if (buffersize > NIT_ITERSIZE(iter)) {
            buffersize = NIT_ITERSIZE(iter);
        }
        NBF_BUFFERSIZE(bufferdata) = buffersize;
    }

    /*
     * If an index was requested, compute the strides for it.
     * Note that we must do this before changing the order of the
     * axes
     */
    npyiter_compute_index_strides(iter, flags);

    /* Initialize the perm to the identity */
    perm = NIT_PERM(iter);
    for(idim = 0; idim < ndim; ++idim) {
        perm[idim] = idim;
    }

    /*
     * If an iteration order is being forced, apply it.
     */
    npyiter_apply_forced_iteration_order(iter, order);
    itflags = NIT_ITFLAGS(iter);

    /*
     * If the ordering was not forced, reorder the axes
     * and flip negative strides to find the best one.
     */
    if (!(itflags&NPY_ITFLAG_FORCEDORDER)) {
        if (ndim > 1) {
            npyiter_find_best_axis_ordering(iter);
        }
        /*
         * If there's an output being allocated, we must not negate
         * any strides.
         */
        if (!any_allocate_if_null) {
            npyiter_flip_negative_strides(iter);
        }
        itflags = NIT_ITFLAGS(iter);
    }

    if (any_allocate_if_null) {
        /*
         * The __array_priority__ attribute of the inputs determines
         * the subtype of any output arrays.  Take the subtype
         * with highest priority.
         */
        for (iiter = 0; iiter < niter; ++iiter) {
            if (op_itflags[iiter]&NPY_OP_ITFLAG_READ) {
                double priority =
                            PyArray_GetPriority((PyObject *)op[iiter], 0.0);
                if (priority > subtype_priority) {
                    subtype_priority = priority;
                    subtype = Py_TYPE(op[iiter]);
                }
            }
        }
    }

    /*
     * If an automatically allocated output didn't have a specified
     * dtype, we need to figure it out now, before allocating the outputs.
     */
    if (any_missing_dtypes || (flags&NPY_ITER_COMMON_DATA_TYPE)) {
        PyArray_Descr *dtype;
        int only_inputs = !(flags&NPY_ITER_COMMON_DATA_TYPE);

        dtype = npyiter_get_common_dtype(niter, op, op_ndim,
                                    op_itflags, op_dtype,
                                    only_inputs);
        if (dtype == NULL) {
            NpyIter_Deallocate(iter);
            return NULL;
        }
        if (flags&NPY_ITER_COMMON_DATA_TYPE) {
            /* Replace all the data types */
            for (iiter = 0; iiter < niter; ++iiter) {
                Py_XDECREF(op_dtype[iiter]);
                Py_INCREF(dtype);
                op_dtype[iiter] = dtype;
                NIT_DTYPES(iter)[iiter] = dtype;
            }
        }
        else {
            /* Replace the NULL data types */
            for (iiter = 0; iiter < niter; ++iiter) {
                if (op_dtype[iiter] == NULL) {
                    Py_INCREF(dtype);
                    op_dtype[iiter] = dtype;
                    NIT_DTYPES(iter)[iiter] = dtype;
                }
            }
        }
        Py_DECREF(dtype);
    }


    /*
     * All of the data types have been settled, so it's time
     * to check that data type conversions are following the
     * casting rules.
     */
    if (!npyiter_check_casting(niter, op, op_dtype, casting, op_itflags)) {
        NpyIter_Deallocate(iter);
        return NULL;
    }

    /*
     * At this point, the iteration order has been finalized. so
     * any allocation of ops that were NULL, or any temporary
     * copying due to casting/byte order/alignment can be
     * done now using a memory layout matching the iterator.
     */
    for (iiter = 0; iiter < niter; ++iiter) {
        if (op_type[iiter] == NPY_ITER_OP_NULL) {
            PyArrayObject *out;
            PyTypeObject *op_subtype;

            /* Check whether the subtype was disabled */
            if (op_flags[iiter]&NPY_ITER_NO_SUBTYPE) {
                op_subtype = &PyArray_Type;
            }
            else {
                op_subtype = subtype;
            }

            /* Allocate the output array, if possible */
            out = npyiter_new_temp_array(iter, op_subtype,
                                        op_ndim[iiter], NULL,
                                        op_dtype[iiter],
                                        op_axes ? op_axes[iiter] : NULL);
            if (out == NULL) {
                NpyIter_Deallocate(iter);
                return NULL;
            }

            op[iiter] = out;
            NIT_OBJECTS(iter)[iiter] = out;

            /*
             * Now we need to replace the pointers and strides with values
             * from the new array.
             */
            npyiter_replace_axisdata(iter, iiter, op[iiter], op_ndim[iiter],
                    PyArray_DATA(op[iiter]), op_axes ? op_axes[iiter] : NULL);

            /* New arrays are aligned and need no swapping or casting */
            op_itflags[iiter] |= NPY_OP_ITFLAG_ALIGNED;
            op_itflags[iiter] &= ~(NPY_OP_ITFLAG_COPYSWAP|NPY_OP_ITFLAG_CAST);
        }
        else if ((op_itflags[iiter]&
                        (NPY_OP_ITFLAG_CAST|NPY_OP_ITFLAG_COPYSWAP)) &&
                              (op_itflags[iiter]&NPY_OP_ITFLAG_COPY)) {
            PyArrayObject *temp;

            /* Allocate the temporary array, if possible */
            temp = npyiter_new_temp_array(iter, &PyArray_Type,
                                        PyArray_NDIM(op[iiter]),
                                        PyArray_DIMS(op[iiter]),
                                        op_dtype[iiter],
                                        op_axes ? op_axes[iiter] : NULL);
            if (temp == NULL) {
                NpyIter_Deallocate(iter);
                return NULL;
            }

            /* If the data will be read, copy it into temp */
            if (op_itflags[iiter]&NPY_OP_ITFLAG_READ) {
                if (PyArray_CopyInto(temp, op[iiter]) != 0) {
                    Py_DECREF(temp);
                    NpyIter_Deallocate(iter);
                    return NULL;
                }
            }
            /* If the data will be written to, set UPDATEIFCOPY */
            if (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE) {
                PyArray_FLAGS(temp) |= NPY_UPDATEIFCOPY;
                PyArray_FLAGS(op[iiter]) &= ~NPY_WRITEABLE;
                Py_INCREF(op[iiter]);
                temp->base = (PyObject *)op[iiter];
            }

            Py_DECREF(op[iiter]);
            op[iiter] = temp;
            NIT_OBJECTS(iter)[iiter] = temp;

            /*
             * Now we need to replace the pointers and strides with values
             * from the temporary array.
             */
            npyiter_replace_axisdata(iter, iiter, op[iiter], op_ndim[iiter],
                    PyArray_DATA(op[iiter]), op_axes ? op_axes[iiter] : NULL);

            /* Now it is aligned, and no longer needs a swap or cast */
            op_itflags[iiter] |= NPY_OP_ITFLAG_ALIGNED;
            op_itflags[iiter] &= ~(NPY_OP_ITFLAG_COPYSWAP|NPY_OP_ITFLAG_CAST);
        }
        else {
            /*
             * Buffering must be enabled for casting/conversion if copy
             * wasn't specified.
             */
            if (op_itflags[iiter]&
                        (NPY_OP_ITFLAG_CAST|NPY_OP_ITFLAG_COPYSWAP) &&
                          !(itflags&NPY_ITFLAG_BUFFER)) {
                PyErr_SetString(PyExc_TypeError,
                        "Iterator input required copying or buffering, "
                        "but neither copying nor buffering was enabled");
                NpyIter_Deallocate(iter);
                return NULL;
            }

            /*
             * If the operand is aligned, any buffering can use aligned
             * optimizations.
             */
            if (PyArray_ISALIGNED(op[iiter])) {
                op_itflags[iiter] |= NPY_OP_ITFLAG_ALIGNED;
            }
        }

        /*
         * If no alignment, byte swap, or casting is needed, and
         * the inner stride of this operand works for the whole
         * array, we can set NPY_OP_ITFLAG_BUFNEVER.
         * But, if buffering is enabled, write-buffering must be
         * one-to-one, because the buffering write back won't combine
         * values correctly. This test doesn't catch everything, but it will
         * catch the most common case of a broadcasting a write-buffered
         * dimension.
         */
        if ((itflags&NPY_ITFLAG_BUFFER) &&
                        (!(op_itflags[iiter]&(NPY_OP_ITFLAG_CAST|
                                             NPY_OP_ITFLAG_COPYSWAP)) ||
                          (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE))) {
            int is_one_to_one = 1;
            npy_intp stride, shape, innerstride = 0, innershape;
            char *axisdata = NIT_AXISDATA(iter);
            npy_intp sizeof_axisdata =
                                NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
            /* Find stride of the first non-empty shape */
            for (idim = 0; idim < ndim; ++idim) {
                innershape = NAD_SHAPE(axisdata);
                if (innershape != 1) {
                    innerstride = NAD_STRIDES(axisdata)[iiter];
                    if (innerstride == 0) {
                        is_one_to_one = 0;
                    }
                    break;
                }
                axisdata += sizeof_axisdata;
            }
            ++idim;
            axisdata += sizeof_axisdata;
            /* Check that everything could have coalesced together */
            for (; idim < ndim; ++idim) {
                stride = NAD_STRIDES(axisdata)[iiter];
                shape = NAD_SHAPE(axisdata);
                if (shape != 1) {
                    if (stride == 0) {
                        is_one_to_one = 0;
                    }
                    /*
                     * If N times the inner stride doesn't equal this
                     * stride, the multi-dimensionality is needed.
                     */
                    if (innerstride*innershape != stride) {
                        break;
                    }
                    else {
                        innershape *= shape;
                    }
                }
                axisdata += sizeof_axisdata;
            }
            /*
             * If we looped all the way to the end, one stride works.
             * Set that stride, because it may not belong to the first
             * dimension.
             */
            if (idim == ndim &&
                        !(op_itflags[iiter]&(NPY_OP_ITFLAG_CAST|
                                             NPY_OP_ITFLAG_COPYSWAP))) {
                op_itflags[iiter] |= NPY_OP_ITFLAG_BUFNEVER;
                NBF_STRIDES(bufferdata)[iiter] = innerstride;
            }
            else if (!is_one_to_one &&
                        (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE)) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator operand requires write buffering, "
                        "but has dimensions which have been broadcasted "
                        "and would be combined incorrectly");
                NpyIter_Deallocate(iter);
                return NULL;
            }
        }
    }

    /*
     * Finally, if coords weren't requested,
     * it may be possible to coalesce some axes together.
     */
    if (ndim > 1 && !(itflags&NPY_ITFLAG_HASCOORDS)) {
        npyiter_coalesce_axes(iter);
        itflags = NIT_ITFLAGS(iter);
        ndim = NIT_NDIM(iter);
    }

    /* Now that the axes are finished, adjust ITERSIZE if necessary */
    if ((itflags&NPY_ITFLAG_NOINNER) && !(itflags&NPY_ITFLAG_BUFFER)) {
        char *axisdata = NIT_AXISDATA(iter);
        NIT_ITERSIZE(iter) /= NAD_SHAPE(axisdata);
    }

    /* Copy the per-op itflags into the iterator */
    memcpy(NIT_OPITFLAGS(iter), op_itflags, niter);

    if (itflags&NPY_ITFLAG_BUFFER) {
        /* Allocate the buffers */
        if (!npyiter_allocate_buffers(iter)) {
            NpyIter_Deallocate(iter);
            return NULL;
        }

        /* BUFFERED + NOINNER may not have a predictable itersize */
        if (itflags&NPY_ITFLAG_NOINNER) {
            NIT_ITERSIZE(iter) = 0;
        }

        /* Prepare the next buffers and set pos/size */
        npyiter_copy_to_buffers(iter);
    }

    return iter;
}

/* The constructor for an iterator over one object */
NpyIter*
NpyIter_New(PyArrayObject *op, npy_uint32 flags,
                  NPY_ORDER order, NPY_CASTING casting,
                  PyArray_Descr* dtype,
                  npy_intp a_ndim, npy_intp *axes, npy_intp buffersize)
{
    /* Split the flags into separate global and op flags */
    npy_uint32 op_flags = flags&NPY_ITER_PER_OP_FLAGS;
    flags &= NPY_ITER_GLOBAL_FLAGS;

    if (a_ndim > 0) {
        return NpyIter_MultiNew(1, &op, flags, order, casting,
                                &op_flags, &dtype,
                                a_ndim, &axes, buffersize);
    }
    else {
        return NpyIter_MultiNew(1, &op, flags, order, casting,
                                &op_flags, &dtype,
                                0, NULL, buffersize);
    }
}

int NpyIter_Deallocate(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    PyArray_Descr **dtype = NIT_DTYPES(iter);
    PyArrayObject **object = NIT_OBJECTS(iter);

    /* Deallocate any buffers and buffering data */
    if (itflags&NPY_ITFLAG_BUFFER) {
        char *bufferdata = NIT_BUFFERDATA(iter);
        char **buffers;
        void **transferdata;

        /* buffers */
        buffers = NBF_BUFFERS(bufferdata);
        for(iiter = 0; iiter < niter; ++iiter, ++buffers) {
            if (*buffers) {
                PyArray_free(*buffers);
            }
        }
        /* read bufferdata */
        transferdata = NBF_READTRANSFERDATA(bufferdata);
        for(iiter = 0; iiter < niter; ++iiter, ++transferdata) {
            if (*transferdata) {
                PyArray_FreeStridedTransferData(*transferdata);
            }
        }
        /* write bufferdata */
        transferdata = NBF_WRITETRANSFERDATA(bufferdata);
        for(iiter = 0; iiter < niter; ++iiter, ++transferdata) {
            if (*transferdata) {
                PyArray_FreeStridedTransferData(*transferdata);
            }
        }
    }

    /* Deallocate all the dtypes and objects that were iterated */
    for(iiter = 0; iiter < niter; ++iiter, ++dtype, ++object) {
        Py_XDECREF(*dtype);
        Py_XDECREF(*object);
    }

    /* Deallocate the iterator memory */
    PyArray_free(iter);

    return NPY_SUCCEED;
}

/* Removes coords support from the iterator */
int NpyIter_RemoveCoords(NpyIter *iter)
{
    npy_uint32 itflags;

    /* Make sure the iterator is reset */
    NpyIter_Reset(iter);

    itflags = NIT_ITFLAGS(iter);
    if (itflags&NPY_ITFLAG_HASCOORDS) {
        NIT_ITFLAGS(iter) = itflags & ~NPY_ITFLAG_HASCOORDS;
        npyiter_coalesce_axes(iter);
    }

    return NPY_SUCCEED;
}

/* Removes the inner loop handling (adds NPY_ITER_NO_INNER_ITERATION) */
int NpyIter_RemoveInnerLoop(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);;
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    /* Check conditions under which this can be done */
    if (itflags&(NPY_ITFLAG_HASINDEX|NPY_ITFLAG_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Iterator flag NO_INNER_ITERATION cannot be used "
                "if coords or an index is being tracked");
        return NPY_FAIL;
    }
    /* Set the flag */
    if (!(itflags&NPY_ITFLAG_NOINNER)) {
        itflags |= NPY_ITFLAG_NOINNER;
        NIT_ITFLAGS(iter) = itflags;
        
        /* Adjust ITERSIZE */
        if (itflags&NPY_ITFLAG_BUFFER) {
            /* BUFFERED + NOINNER may not have a predictable itersize */
            NIT_ITERSIZE(iter) = 0;
                                            
        }
        else {
            char *axisdata = NIT_AXISDATA(iter);
            NIT_ITERSIZE(iter) /= NAD_SHAPE(axisdata);
        }
    }

    /* Reset the iterator */
    NpyIter_Reset(iter);

    return NPY_SUCCEED;
}

/* Resets the iterator to its initial state */
void NpyIter_Reset(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char **resetdataptr;
    char *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp istrides, nstrides;

    resetdataptr = NIT_RESETDATAPTR(iter);
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    nstrides = NAD_NSTRIDES();

    if (itflags&NPY_ITFLAG_BUFFER) {
        /* Copy any data from the buffers back to the arrays */
        npyiter_copy_from_buffers(iter);
    }
    
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        char **ptrs;
        NAD_COORD(axisdata) = 0;
        ptrs = NAD_PTRS(axisdata);
        for (istrides = 0; istrides < nstrides; ++istrides) {
            ptrs[istrides] = resetdataptr[istrides];
        }
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        /* Prepare the next buffers and set pos/size */
        npyiter_copy_to_buffers(iter);
    }
}

/*
 * Sets the iterator to the specified coordinates, which must have the
 * correct number of entries for 'ndim'.  It is only valid
 * when NPY_ITER_COORDS was passed to the constructor.  This operation
 * fails if the coordinates are out of bounds.
 *
 * Returns NPY_SUCCEED on success, NPY_FAIL on failure.
 */
int NpyIter_GotoCoords(NpyIter *iter, npy_intp *coords)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char **dataptr;
    char *ad, *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp istrides, nstrides;
    npy_intp *perm, perm_coords[NPY_MAXDIMS];

    if (!(itflags&NPY_ITFLAG_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoCoords on an iterator without "
                "requesting coordinates in the constructor");
        return NPY_FAIL;
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoCoords on an iterator which "
                "is buffered");
        return NPY_FAIL;
    }

    perm = NIT_PERM(iter);
    dataptr = NIT_RESETDATAPTR(iter);
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    /* Permute the input coordinates to match the iterator */
    ad = axisdata;
    for (idim = 0; idim < ndim; ++idim, ad += sizeof_axisdata) {
        npy_intp p = perm[idim], i, shape;
        if (p < 0) {
            /* If the perm entry is negative, reverse the coordinate */
            shape = NAD_SHAPE(ad);
            i = shape - coords[ndim+p] - 1;
        }
        else {
            shape = NAD_SHAPE(ad);
            i = coords[ndim-p-1];
        }

        /* Bounds-check this coordinate */
        if (i >= 0 && i < shape) {
            perm_coords[idim] = i;
        }
        else {
            PyErr_SetString(PyExc_IndexError,
                    "Iterator GotoCoords called with out-of-bounds "
                    "coordinates.");
            return NPY_FAIL;
        }
    }

    nstrides = NAD_NSTRIDES();

    /*
     * Set the coordinates, from the slowest-changing to the
     * fastest-changing.  The successive pointers accumulate
     * the offsets, starting from the original data pointers.
     */
    axisdata += (ndim-1)*sizeof_axisdata;
    for (idim = ndim-1; idim >= 0; --idim, axisdata -= sizeof_axisdata) {
        npy_intp i, *strides;
        char **ptrs;

        NAD_COORD(axisdata) = i = perm_coords[idim];
        strides = NAD_STRIDES(axisdata);
        ptrs = NAD_PTRS(axisdata);
        for (istrides = 0; istrides < nstrides; ++istrides) {
            ptrs[istrides] = dataptr[istrides] + i*strides[istrides];
        }

        dataptr = ptrs;
    }

    return NPY_SUCCEED;
}

/* If the iterator is tracking an index, sets the iterator
 * to the specified index.
 *
 * Returns NPY_SUCCEED on success, NPY_FAIL on failure.
 */
int NpyIter_GotoIndex(NpyIter *iter, npy_intp index)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char **dataptr;
    char *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp istrides, nstrides;

    if (!(itflags&NPY_ITFLAG_HASINDEX)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoIndex on an iterator without "
                "requesting an index in the constructor");
        return NPY_FAIL;
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoIndex on an iterator which "
                "is buffered");
        return NPY_FAIL;
    }

    if (index < 0 || index >= NIT_ITERSIZE(iter)) {
        PyErr_SetString(PyExc_IndexError,
                "Iterator GotoIndex called with out-of-bounds "
                "index.");
        return NPY_FAIL;
    }

    dataptr = NIT_RESETDATAPTR(iter);
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    nstrides = NAD_NSTRIDES();

    /*
     * Set the coordinates, from the slowest-changing to the
     * fastest-changing.  The successive pointers accumulate
     * the offsets, starting from the original data pointers.
     */
    axisdata += (ndim-1)*sizeof_axisdata;
    for (idim = 0; idim < ndim; ++idim, axisdata -= sizeof_axisdata) {
        npy_intp i, shape, iterstride, *strides;
        char **ptrs;

        shape = NAD_SHAPE(axisdata);
        strides = NAD_STRIDES(axisdata);
        ptrs = NAD_PTRS(axisdata);
        iterstride = strides[niter];

        /* Extract the coordinate from the index */
        if (iterstride == 0) {
            i = 0;
        }
        else if (iterstride < 0) {
            i = shape - (index/(-iterstride))%shape - 1;
        }
        else {
            i = (index/iterstride)%shape;
        }

        NAD_COORD(axisdata) = i;

        for (istrides = 0; istrides < nstrides; ++istrides) {
            ptrs[istrides] = dataptr[istrides] + i*strides[istrides];
        }

        dataptr = ptrs;
    }

    return NPY_SUCCEED;
}

/* SPECIALIZED iternext functions that handle the non-buffering part */

/**begin repeat
 * #const_itflags = 0,
 *                  NPY_ITFLAG_HASINDEX,
 *                  NPY_ITFLAG_NOINNER#
 * #tag_itflags = 0, IND, NOINN#
 */
/**begin repeat1
 * #const_ndim = 1, 2, 100#
 * #tag_ndim = 1, 2, ANY#
 */
/**begin repeat2
 * #const_niter = 1, 2, 100#
 * #tag_niter = 1, 2, ANY#
 */

/* Specialized iternext (@const_itflags@,@tag_ndim@,@tag_niter@) */
NPY_NO_EXPORT int
npyiter_iternext_itflags@tag_itflags@_dims@tag_ndim@_iters@tag_niter@(
                                                      NpyIter *iter)
{
    const npy_uint32 itflags = @const_itflags@;
#if @const_ndim@ < 100
    const npy_intp ndim = @const_ndim@;
#else
    npy_intp idim, ndim = NIT_NDIM(iter);
#endif
#if @const_niter@ < 100
    const npy_intp niter = @const_niter@;
#else
    npy_intp niter = NIT_NITER(iter);
#endif

    npy_intp istrides, nstrides, sizeof_axisdata;
#if @const_ndim@ > 0
    char* axisdata0;
#endif
#if @const_ndim@ > 1
    char* axisdata1;
#endif
#if @const_ndim@ > 2
    char* axisdata2;
#endif

    nstrides = NAD_NSTRIDES();
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    axisdata0 = NIT_AXISDATA(iter);
#  if !(@const_itflags@&NPY_ITFLAG_NOINNER)
    /* Increment coordinate 0 */
    NAD_COORD(axisdata0)++;
    /* Increment pointer 0 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata0)[istrides] += NAD_STRIDES(axisdata0)[istrides];
    }
#  endif

#if @const_ndim@ == 1

#  if !(@const_itflags@&NPY_ITFLAG_NOINNER)
    /* Finished when the coordinate equals the shape */
    return NAD_COORD(axisdata0) < NAD_SHAPE(axisdata0);
#  else
    /* Get rid of unused variable warning */
    istrides = 0;

    return 0;
#  endif

#else

#  if !(@const_itflags@&NPY_ITFLAG_NOINNER)
    if (NAD_COORD(axisdata0) < NAD_SHAPE(axisdata0)) {
        return 1;
    }
#  endif

    axisdata1 = axisdata0 + sizeof_axisdata;
    /* Increment coordinate 1 */
    NAD_COORD(axisdata1)++;
    /* Increment pointer 1 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata1)[istrides] += NAD_STRIDES(axisdata1)[istrides];
    }

    if (NAD_COORD(axisdata1) < NAD_SHAPE(axisdata1)) {
        /* Reset the 1st coordinate to 0 */
        NAD_COORD(axisdata0) = 0;
        /* Reset the 1st pointer to the value of the 2nd */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata0)[istrides] = NAD_PTRS(axisdata1)[istrides];
        }
        return 1;
    }

# if @const_ndim@ == 2
    return 0;
# else
    
    axisdata2 = axisdata1 + sizeof_axisdata;
    /* Increment coordinate 2 */
    NAD_COORD(axisdata2)++;
    /* Increment pointer 2 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata2)[istrides] += NAD_STRIDES(axisdata2)[istrides];
    }

    if (NAD_COORD(axisdata2) < NAD_SHAPE(axisdata2)) {
        /* Reset the 1st and 2nd coordinates to 0 */
        NAD_COORD(axisdata0) = 0;
        NAD_COORD(axisdata1) = 0;
        /* Reset the 1st and 2nd pointers to the value of the 3nd */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata0)[istrides] = NAD_PTRS(axisdata2)[istrides];
            NAD_PTRS(axisdata1)[istrides] = NAD_PTRS(axisdata2)[istrides];
        }
        return 1;
    }

    for (idim = 3; idim < ndim; ++idim) {
        axisdata2 += sizeof_axisdata;
        /* Increment the coordinate */
        NAD_COORD(axisdata2)++;
        /* Increment the pointer */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata2)[istrides] += NAD_STRIDES(axisdata2)[istrides];
        }


        if (NAD_COORD(axisdata2) < NAD_SHAPE(axisdata2)) {
            /* Reset the coordinates and pointers of all previous axisdatas */
            axisdata1 = axisdata2;
            do {
                axisdata1 -= sizeof_axisdata;
                /* Reset the coordinate to 0 */
                NAD_COORD(axisdata1) = 0;
                /* Reset the pointer to the updated value */
                for (istrides = 0; istrides < nstrides; ++istrides) {
                    NAD_PTRS(axisdata1)[istrides] =
                                        NAD_PTRS(axisdata2)[istrides];
                }
            } while (axisdata1 != axisdata0);

            return 1;
        }
    }

    return 0;

# endif /* ndim != 2 */
    
#endif /* ndim != 1 */
}

/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/* iternext function that handle the buffering part */
NPY_NO_EXPORT int
npyiter_buffered_iternext(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char *bufferdata = NIT_BUFFERDATA(iter);

    /*
     * If the iterator handles the inner loop, need to increment all
     * the coordinates and pointers
     */
    if (!(itflags&NPY_ITFLAG_NOINNER)) {
        /* Increment within the buffer */
        if (++NBF_POS(bufferdata) < NBF_SIZE(bufferdata)) {
            npy_intp iiter, *strides;
            char **ptrs;

            strides = NBF_STRIDES(bufferdata);
            ptrs = NBF_PTRS(bufferdata);
            for (iiter = 0; iiter < niter; ++iiter) {
                ptrs[iiter] += strides[iiter];
            }
            return 1;
        }
    }

    /* Write back to the arrays */
    npyiter_copy_from_buffers(iter);

    /* Increment to the next buffer */
    if (!npyiter_jumpforward(iter, NBF_SIZE(bufferdata))) {
        NBF_POS(bufferdata) = 0;
        NBF_SIZE(bufferdata) = 0;
        return 0;
    }

    /* Prepare the next buffers and set pos/size */
    npyiter_copy_to_buffers(iter);

    return 1;
}

/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/* Specialization of iternext for when the iteration size is 1 */
NPY_NO_EXPORT int
npyiter_iternext_sizeone(NpyIter *iter)
{
    return 0;
}

/* Returns a specialized iternext function */
NpyIter_IterNext_Fn NpyIter_GetIterNext(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    /*
     * When there is just one element being iterated,
     * the iternext function is very simple
     */
    if (NIT_ITERSIZE(iter) == 1) {
        return &npyiter_iternext_sizeone;
    }

    /*
     * If buffering is enabled, don't specialize further.
     */
    if (itflags&NPY_ITFLAG_BUFFER) {
        return &npyiter_buffered_iternext;
    }

    /*
     * Ignore all the flags that don't affect the iterator memory
     * layout or the iternext function.  Currently only HASINDEX,
     * NOINNER, and BUFFER affect them.
     */
    itflags &= (NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NOINNER|NPY_ITFLAG_BUFFER);

    /* Switch statements let the compiler optimize this most effectively */
    switch (itflags) {
    /* The combination HASINDEX | NOINNER is excluded in the New functions */
/**begin repeat
 * #const_itflags = 0,
 *                  NPY_ITFLAG_HASINDEX,
 *                  NPY_ITFLAG_NOINNER#
 * #tag_itflags = 0, IND, NOINN#
 */
        case @const_itflags@:
            switch (ndim) {
/**begin repeat1
 * #const_ndim = 1, 2#
 * #tag_ndim = 1, 2#
 */
                case @const_ndim@:
                    switch (niter) {
/**begin repeat2
 * #const_niter = 1, 2#
 * #tag_niter = 1, 2#
 */
                        case @const_niter@:
                            return &npyiter_iternext_itflags@tag_itflags@_dims@tag_ndim@_iters@tag_niter@;
/**end repeat2**/
                        /* Not specialized on niter */
                        default:
                            return &npyiter_iternext_itflags@tag_itflags@_dims@tag_ndim@_itersANY;
                    }
/**end repeat1**/
                /* Not specialized on ndim */
                default:
                    switch (niter) {
/**begin repeat1
 * #const_niter = 1, 2#
 * #tag_niter = 1, 2#
 */
                        case @const_niter@:
                            return &npyiter_iternext_itflags@tag_itflags@_dimsANY_iters@tag_niter@;
/**end repeat1**/
                        /* Not specialized on niter */
                        default:
                            return &npyiter_iternext_itflags@tag_itflags@_dimsANY_itersANY;
                    }
            }
/**end repeat**/
    }
    /* The switch above should have caught all the possibilities. */
    PyErr_Format(PyExc_ValueError,
            "GetIterNext internal iterator error - unexpected "
            "itflags/ndim/niter combination (%04x/%d/%d)",
            (int)itflags, (int)ndim, (int)niter);
    return NULL;
}


/* SPECIALIZED getcoord functions */

/**begin repeat
 * #const_itflags = 0,
 *    NPY_ITFLAG_HASINDEX,
 *    NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER#
 * #tag_itflags = 0, IND, IDP, INDuIDP, NEGP, INDuNEGP,
 *                BUF, INDuBUF, IDPuBUF, INDuIDPuBUF, NEGPuBUF, INDuNEGPuBUF#
 */
NPY_NO_EXPORT void
npyiter_getcoord_itflags@tag_itflags@(NpyIter *iter, npy_intp *outcoord)
{
    const npy_uint32 itflags = @const_itflags@;
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp idim, sizeof_axisdata;
    char* axisdata;
#if !((@const_itflags@)&NPY_ITFLAG_IDENTPERM)
    npy_intp* perm = NIT_PERM(iter);
#endif

    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
#if ((@const_itflags@)&NPY_ITFLAG_IDENTPERM)
    outcoord += ndim-1;
    for(idim = 0; idim < ndim; ++idim, --outcoord,
                                    axisdata += sizeof_axisdata) {
        *outcoord = NAD_COORD(axisdata);
    }
#elif !((@const_itflags@)&NPY_ITFLAG_NEGPERM)
    for(idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        npy_intp p = perm[idim];
        outcoord[ndim-p-1] = NAD_COORD(axisdata);
    }
#else
    for(idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        npy_intp p = perm[idim];
        if (p < 0) {
            /* If the perm entry is negative, reverse the coordinate */
            outcoord[ndim+p] = NAD_SHAPE(axisdata) - NAD_COORD(axisdata) - 1;
        }
        else {
            outcoord[ndim-p-1] = NAD_COORD(axisdata);
        }
    }
#endif /* not ident perm */
}
/**end repeat**/

/* Returns a specialized getcoord function */
NpyIter_GetCoords_Fn NpyIter_GetGetCoords(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    if (!(itflags&NPY_ITFLAG_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot retrieve a GetCoords function for an iterator "
                "that doesn't track coordinates.");
        return NULL;
    }

    /*
     * Only these flags affect the iterator memory layout or
     * the getcoords behavior. IDENTPERM and NEGPERM are mutually
     * exclusive, so that reduces the number of cases slightly.
     */
    itflags &= (NPY_ITFLAG_HASINDEX |
                NPY_ITFLAG_IDENTPERM |
                NPY_ITFLAG_NEGPERM |
                NPY_ITFLAG_BUFFER);
    
    switch (itflags) {
/**begin repeat
 * #const_itflags = 0,
 *    NPY_ITFLAG_HASINDEX,
 *    NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER#
 * #tag_itflags = 0, IND, IDP, INDuIDP, NEGP, INDuNEGP,
 *                BUF, INDuBUF, IDPuBUF, INDuIDPuBUF, NEGPuBUF, INDuNEGPuBUF#
 */
        case @const_itflags@:
            return npyiter_getcoord_itflags@tag_itflags@;
/**end repeat**/
    }
    /* The switch above should have caught all the possibilities. */
    PyErr_Format(PyExc_ValueError,
            "GetGetCoords internal iterator error - unexpected "
            "itflags/ndim/niter combination (%04x/%d/%d)",
            (int)itflags, (int)ndim, (int)niter);
    return NULL;

}

int NpyIter_HasInnerLoop(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_NOINNER) == 0;
}

int NpyIter_HasCoords(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_HASCOORDS) != 0;
}

int NpyIter_HasIndex(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_HASINDEX) != 0;
}

int NpyIter_HasOffsets(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_HASOFFSETS) != 0;
}

npy_intp NpyIter_GetNDim(NpyIter *iter)
{
    return NIT_NDIM(iter);
}

npy_intp NpyIter_GetNIter(NpyIter *iter)
{
    return NIT_NITER(iter);
}

npy_intp NpyIter_GetIterSize(NpyIter *iter)
{
    return NIT_ITERSIZE(iter);
}

int NpyIter_GetShape(NpyIter *iter, npy_intp *outshape)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp idim, sizeof_axisdata;
    char* axisdata;
    npy_intp* perm;

    if (!(itflags&NPY_ITFLAG_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot get the shape of an iterator "
                "without coordinates requested in the constructor");
        return NPY_FAIL;
    }

    perm = NIT_PERM(iter);
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    for(idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        npy_intp p = perm[idim];
        if (p < 0) {
            outshape[ndim+p] = NAD_SHAPE(axisdata);
        }
        else {
            outshape[ndim-p-1] = NAD_SHAPE(axisdata);
        }
    }

    return NPY_SUCCEED;
}

char **NpyIter_GetDataPtrArray(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char* data;

    if (itflags&NPY_ITFLAG_BUFFER) {
        data = NIT_BUFFERDATA(iter);
        return NBF_PTRS(data);
    }
    else {
        data = NIT_AXISDATA(iter);
        return NAD_PTRS(data);
    }
}

PyArray_Descr **NpyIter_GetDescrArray(NpyIter *iter)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    /*npy_intp niter = NIT_NITER(iter);*/

    return NIT_DTYPES(iter);
}

PyArrayObject **NpyIter_GetObjectArray(NpyIter *iter)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    return NIT_OBJECTS(iter);
}

PyArrayObject *NpyIter_GetIterView(NpyIter *iter, npy_intp i)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp shape[NPY_MAXDIMS], strides[NPY_MAXDIMS];
    PyArrayObject *obj, *view;
    PyArray_Descr *dtype;
    char *dataptr, *axisdata;
    npy_intp sizeof_axisdata;
    int writeable;

    if (i < 0 || i >= niter) {
        PyErr_SetString(PyExc_IndexError,
                "index provided for an iterator view was out of bounds");
        return NULL;
    }

    /* Need data pointers to make the views */
    if (itflags&NPY_ITFLAG_HASOFFSETS) {
        PyErr_SetString(PyExc_IndexError,
                "cannot provide an iterator view when tracking offsets");
        return NULL;
    }

    /* Don't provide views if buffering is enabled */
    if (itflags&NPY_ITFLAG_BUFFER) {
        PyErr_SetString(PyExc_ValueError,
                "cannot provide an iterator view when buffering is enabled");
        return NULL;
    }

    obj = NIT_OBJECTS(iter)[i];
    dtype = PyArray_DESCR(obj);
    writeable = NIT_OPITFLAGS(iter)[i]&NPY_OP_ITFLAG_WRITE;
    dataptr = NIT_RESETDATAPTR(iter)[i];
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    /* Retrieve the shape and strides from the axisdata */
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        shape[ndim-idim-1] = NAD_SHAPE(axisdata);
        strides[ndim-idim-1] = NAD_STRIDES(axisdata)[i];
    }
    
    Py_INCREF(dtype);
    view = (PyArrayObject *)PyArray_NewFromDescr(&PyArray_Type, dtype, ndim,
                                shape, strides, dataptr,
                                writeable ? NPY_WRITEABLE : 0,
                                NULL);
    if (view == NULL) {
        return NULL;
    }
    /* Tell the view who owns the data */
    Py_INCREF(obj);
    view->base = (PyObject *)obj;
    /* Make sure all the flags are good */
    PyArray_UpdateFlags(view, NPY_UPDATE_ALL);

    return view;
}

npy_intp *NpyIter_GetIndexPtr(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char* axisdata = NIT_AXISDATA(iter);

    if (itflags&NPY_ITFLAG_HASINDEX) {
        /* The index is just after the data pointers */
        return (npy_intp*)NAD_PTRS(axisdata) + niter;
    }
    else {
        return NULL;
    }
}

void NpyIter_GetReadFlags(NpyIter *iter, char *outreadflags)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);

    for (iiter = 0; iiter < niter; ++iiter) {
        outreadflags[iiter] = (op_itflags[iiter]&NPY_OP_ITFLAG_READ) != 0;
    }
}

void NpyIter_GetWriteFlags(NpyIter *iter, char *outwriteflags)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);

    for (iiter = 0; iiter < niter; ++iiter) {
        outwriteflags[iiter] = (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE) != 0;
    }
}


npy_intp *NpyIter_GetInnerStrideArray(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char* data;

    if (itflags&NPY_ITFLAG_BUFFER) {
        data = NIT_BUFFERDATA(iter);
        return NBF_STRIDES(data);
    }
    else {
        data = NIT_AXISDATA(iter);
        return NAD_STRIDES(data);
    }
}

npy_intp* NpyIter_GetInnerLoopSizePtr(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char* data;
    
    if (itflags&NPY_ITFLAG_BUFFER) {
        data = NIT_BUFFERDATA(iter);
        return &NBF_SIZE(data);
    }
    else {
        data = NIT_AXISDATA(iter);
        return &NAD_SHAPE(data);
    }
}

/* Checks 'flags' for (C|F)_ORDER_INDEX, COORDS, and NO_INNER_ITERATION,
 * setting the appropriate internal flags in 'itflags'.
 *
 * Returns 1 on success, 0 on error.
 */
static int
pyiter_check_global_flags(npy_uint32 flags, npy_uint32* itflags)
{
    if ((flags&NPY_ITER_PER_OP_FLAGS) != 0) {
        PyErr_SetString(PyExc_ValueError,
                    "A per-operand flag was passed as a global flag "
                    "to the iterator constructor");
        return 0;
    }

    /* Check for an index */
    if (flags&(NPY_ITER_C_ORDER_INDEX | NPY_ITER_F_ORDER_INDEX)) {
        if ((flags&(NPY_ITER_C_ORDER_INDEX | NPY_ITER_F_ORDER_INDEX)) ==
                    (NPY_ITER_C_ORDER_INDEX | NPY_ITER_F_ORDER_INDEX)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flags C_ORDER_INDEX and "
                    "F_ORDER_INDEX cannot both be specified");
            return 0;
        }
        (*itflags) |= NPY_ITFLAG_HASINDEX;
    }
    /* Check if coordinates were requested */
    if (flags&NPY_ITER_COORDS) {
        /*
         * This flag primarily disables dimension manipulations that
         * would produce a different set of coordinates.
         */
        (*itflags) |= NPY_ITFLAG_HASCOORDS;
    }
    /* Check if offsets were requested instead of pointers */
    if (flags&NPY_ITER_OFFSETS) {
        /* Buffering is incompatible with offsets */
        if (flags&(NPY_ITER_BUFFERED|NPY_ITER_BUFFERED_GROWINNER)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flag BUFFERED cannot be used "
                    "with the flag OFFSETS");
            return 0;
        }
        (*itflags) |= NPY_ITFLAG_HASOFFSETS;
    }
    /* Check if the caller wants to handle inner iteration */
    if (flags&NPY_ITER_NO_INNER_ITERATION) {
        if ((*itflags)&(NPY_ITFLAG_HASINDEX|NPY_ITFLAG_HASCOORDS)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flag NO_INNER_ITERATION cannot be used "
                    "if coords or an index is being tracked");
            return 0;
        }
        (*itflags) |= NPY_ITFLAG_NOINNER;
    }
    /* Buffering */
    if (flags&(NPY_ITER_BUFFERED|NPY_ITER_BUFFERED_GROWINNER)) {
        (*itflags) |= NPY_ITFLAG_BUFFER;
        if (flags&NPY_ITER_BUFFERED_GROWINNER) {
            (*itflags) |= NPY_ITFLAG_GROWINNER;
        }
    }

    return 1;
}

/*
 * Checks the per-operand input flags, and fills in op_itflags.
 *
 * Returns 1 on success, 0 on failure.
 */
static int
npyiter_check_per_op_flags(npy_uint32 op_flags, char *op_itflags)
{
    if ((op_flags&NPY_ITER_GLOBAL_FLAGS) != 0) {
        PyErr_SetString(PyExc_ValueError,
                    "A global iterator flag was passed as a per-operand flag "
                    "to the iterator constructor");
        return 0;
    }

    /* Check the read/write flags */
    if (op_flags&NPY_ITER_READONLY) {
        /* The read/write flags are mutually exclusive */
        if (op_flags&(NPY_ITER_READWRITE|NPY_ITER_WRITEONLY)) {
            PyErr_SetString(PyExc_ValueError,
                    "Only one of the iterator flags READWRITE, "
                    "READONLY, and WRITEONLY may be "
                    "specified for an operand");
            return 0;
        }

        *op_itflags = NPY_OP_ITFLAG_READ;
    }
    else if (op_flags&NPY_ITER_READWRITE) {
        /* The read/write flags are mutually exclusive */
        if (op_flags&NPY_ITER_WRITEONLY) {
            PyErr_SetString(PyExc_ValueError,
                    "Only one of the iterator flags READWRITE, "
                    "READONLY, and WRITEONLY may be "
                    "specified for an operand");
            return 0;
        }

        *op_itflags = NPY_OP_ITFLAG_READ|NPY_OP_ITFLAG_WRITE;
    }
    else if(op_flags&NPY_ITER_WRITEONLY) {
        *op_itflags = NPY_OP_ITFLAG_WRITE;
    }
    else {
        PyErr_SetString(PyExc_ValueError,
                "None of the iterator flags READWRITE, "
                "READONLY, or WRITEONLY were "
                "specified for an operand");
        return 0;
    }

    /* Check the flags for temporary copies */
    if (op_flags&(NPY_ITER_COPY|NPY_ITER_UPDATEIFCOPY)) {
        *op_itflags |= NPY_OP_ITFLAG_COPY;
        if (((*op_itflags)&NPY_OP_ITFLAG_WRITE) &&
                    !(op_flags&NPY_ITER_UPDATEIFCOPY)) {
            PyErr_SetString(PyExc_ValueError,
                    "If an iterator operand is writeable, must use "
                    "the flag UPDATEIFCOPY instead of "
                    "COPY");
            return 0;
        }
    }
    return 1;
}

/*
 * Returns 1 if the from -> to cast can be done, based on the casting
 * flags provided in op_flags, and 0 otherwise.
 *
 * TODO: Maybe this approach, with the NPY_CASTING enum, should replace
 *       the PyArray_CanCastTo code for NumPy 2.0?
 */
static int
npyiter_can_cast(PyArray_Descr *from, PyArray_Descr *to, NPY_CASTING casting)
{
    /* If unsafe casts are allowed */
    if (casting == NPY_UNSAFE_CASTING) {
        return 1;
    }
    /* Equivalent types can be cast with any value of 'casting'  */
    else if (PyArray_EquivTypenums(from->type_num, to->type_num)) {
        /* If the types are extended, convert to NBO and compare */
        if (PyTypeNum_ISEXTENDED(from->type_num)) {
            int ret;

            /* Only NPY_NO_CASTING prevents byte order conversion */
            if ((casting != NPY_NO_CASTING) &&
                                (!PyArray_ISNBO(from->byteorder) ||
                                 !PyArray_ISNBO(to->byteorder))) {
                PyArray_Descr *nbo_from, *nbo_to;

                nbo_from = PyArray_DescrNewByteorder(from, NPY_NATIVE);
                nbo_to = PyArray_DescrNewByteorder(to, NPY_NATIVE);
                if (nbo_from == NULL || nbo_to == NULL) {
                    Py_XDECREF(nbo_from);
                    Py_XDECREF(nbo_to);
                    PyErr_Clear();
                    return 0;
                }
                ret = PyArray_EquivTypes(nbo_from, nbo_to);
                Py_DECREF(nbo_from);
                Py_DECREF(nbo_to);
            }
            else {
                ret = PyArray_EquivTypes(from, to);
            }
            return ret;
        }
        return 1;
    }
    /* If safe or same-kind casts are allowed */
    else if (casting == NPY_SAFE_CASTING || casting == NPY_SAME_KIND_CASTING) {
        if (PyArray_CanCastTo(from, to)) {
            return 1;
        }
        else if(casting == NPY_SAME_KIND_CASTING) {
            /* TODO: Should also allow casting from "lower" to "higher
             *       kinds, but kind is a char so we need to remap to
             *       an ordered version (like NPY_SCALARKIND is ordered).
             */
            return from->kind == to->kind;
        }
        else {
            return 0;
        }
    }
    /* NPY_NO_CASTING or NPY_EQUIV_CASTING was specified */
    else {
        return 0;
    }
}

/*
 * Prepares a a constructor operand.  Assumes a reference to 'op'
 * is owned, and that 'op' may be replaced.  Fills in 'op_dtype',
 * 'op_type' and 'ndim'.
 *
 * Returns 1 on success, 0 on failure.
 */
static int
pyiter_prepare_operand(PyArrayObject **op, PyArray_Descr *op_request_dtype,
                       PyArray_Descr **op_dtype, int* op_type,
                       npy_intp* op_ndim,
                       npy_uint32 op_flags, char *op_itflags)
{
    /* NULL operands must be automatically allocated outputs */
    if (*op == NULL) {
        /* ALLOCATE should be enabled */
        if (!(op_flags&NPY_ITER_ALLOCATE)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator input was NULL, but automatic allocation as an "
                    "output wasn't requested");
            return 0;
        }
        /* Reading should be disabled */
        if ((*op_itflags)&NPY_OP_ITFLAG_READ) {
            PyErr_SetString(PyExc_ValueError,
                    "Automatic allocation was requested for an iterator "
                    "operand, but it wasn't flagged as write only");
            return 0;
        }
        /* If a requested dtype was provided, use it, otherwise NULL */
        Py_XINCREF(op_request_dtype);
        *op_dtype = op_request_dtype;
        *op_type = NPY_ITER_OP_NULL;
        *op_ndim = 0;
        /* No copying of NULL operands */
        *op_itflags &= ~NPY_OP_ITFLAG_COPY;
        return 1;
    }

    if (PyArray_Check(*op)) {
        if (((*op_itflags)&NPY_OP_ITFLAG_WRITE) &&
                    (!PyArray_CHKFLAGS(*op, NPY_WRITEABLE))) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator input was a non-writeable array, but was "
                    "flagged as writeable");
            return 0;

        }
        *op_type = NPY_ITER_OP_ARRAY;
        *op_ndim = PyArray_NDIM(*op);
        /* PyArray_DESCR does not give us a reference */
        *op_dtype = PyArray_DESCR(*op);
        if (*op_dtype == NULL) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator input array object has no dtype descr");
            return 0;
        }
        Py_INCREF(*op_dtype);
        /*
         * Make sure that if the data type has a Python reference or
         * other pointer, WRITEABLE_REFERENCES was specified.
         */
        if (((*op_itflags)&NPY_OP_ITFLAG_WRITE) &&
                        !(op_flags&NPY_ITER_WRITEABLE_REFERENCES)) {
            if (PyDataType_FLAGCHK(*op_dtype, NPY_ITEM_HASOBJECT) ||
                        PyDataType_FLAGCHK(*op_dtype, NPY_ITEM_IS_POINTER)) {
                PyErr_SetString(PyExc_ValueError,
                        "Tried to construct an iterator for a writeable "
                        "array of references/pointers without specifying the "
                        "WRITEABLE_REFERENCES flag.");
                return 0;
            }
        }
        /*
         * Checking whether casts are valid is done later, once the
         * final data types have been selected.  For now, just store the
         * requested type.
         */
        if (op_request_dtype != NULL) {
            /* Store the requested dtype */
            Py_DECREF(*op_dtype);
            Py_INCREF(op_request_dtype);
            *op_dtype = op_request_dtype;
        }

        /* Check if the operand is aligned and in the byte order requested */
        if (op_flags&NPY_ITER_NBO_ALIGNED) {
            /* Check byte order */
            if (!PyArray_ISNBO((*op_dtype)->byteorder)) {
                PyArray_Descr *nbo_dtype;
                
                /* Replace with a new descr which is in native byte order */
                nbo_dtype = PyArray_DescrNewByteorder(*op_dtype, NPY_NATIVE);
                Py_DECREF(*op_dtype);
                *op_dtype = nbo_dtype;

                /* Indicate that byte order or alignment needs fixing */
                *op_itflags |= NPY_OP_ITFLAG_COPYSWAP;
            }
            /* Check alignment */
            else if (!PyArray_ISALIGNED(*op)) {
                *op_itflags |= NPY_OP_ITFLAG_COPYSWAP;
            }
        }
    }
    else {
        PyErr_SetString(PyExc_ValueError,
                "Iterator inputs must be ndarrays");
        return 0;
    }

    return 1;
}

static int
npyiter_check_casting(npy_intp niter, PyArrayObject **op,
                    PyArray_Descr **op_dtype,
                    NPY_CASTING casting,
                    char *op_itflags)
{
    npy_intp iiter;

    for(iiter = 0; iiter < niter; ++iiter) {
        /* If the types aren't equivalent, a cast is necessary */
        if (op[iiter] != NULL && !PyArray_EquivTypes(PyArray_DESCR(op[iiter]),
                                                     op_dtype[iiter])) {
            /* Check read (op -> temp) casting */
            if ((op_itflags[iiter]&NPY_OP_ITFLAG_READ) &&
                        !npyiter_can_cast(PyArray_DESCR(op[iiter]),
                                          op_dtype[iiter],
                                          casting)) {
                PyErr_Format(PyExc_TypeError,
                        "Iterator operand %d dtype could not be cast "
                        "to the requested dtype, according to "
                        "the casting enabled", (int)iiter);
                return 0;
            }
            /* Check write (temp -> op) casting */
            if ((op_itflags[iiter]&NPY_OP_ITFLAG_WRITE) &&
                        !npyiter_can_cast(op_dtype[iiter],
                                          PyArray_DESCR(op[iiter]),
                                          casting)) {
                PyErr_Format(PyExc_TypeError,
                        "Iterator requested dtype could not be cast "
                        "to the operand %d dtype, according to "
                        "the casting enabled", (int)iiter);
                return 0;
            }

            /* Indicate that this operand needs casting */
            op_itflags[iiter] |= NPY_OP_ITFLAG_CAST;
        }
    }

    return 1;
}

/*
 * Fills in the AXISDATA for the 'niter' operands, broadcasting
 * the dimensionas as necessary.  Also fills
 * in the ITERSIZE data member.
 *
 * If op_axes is not NULL, it should point to an array of ndim-sized
 * arrays, one for each op.
 *
 * Returns 1 on success, 0 on failure.
 */
static int
npyiter_fill_axisdata(NpyIter *iter, PyArrayObject **op,
                      npy_intp *op_ndim, char **op_dataptr,
                      npy_uint32 *op_flags, npy_intp **op_axes)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp ondim;
    char *odataptr, *axisdata0, *axisdata;
    npy_intp sizeof_axisdata;

    axisdata0 = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    /* Process the first operand */
    if (op_axes == NULL || op_axes[0] == NULL) {
        /* Default broadcasting rules if op_axes is not specified */
        axisdata = axisdata0;
        ondim = op_ndim[0];
        odataptr = op_dataptr[0];
        /* Possible if op_axes are being used, but op_axes[0] is NULL */
        if (ondim > ndim) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator input has more dimensions than allowed "
                    "by the 'op_axes' specified");
            return 0;
        }
        for (idim = 0; idim < ondim; ++idim) {
            npy_intp shape;

            if (op[0] != NULL) {
                shape = PyArray_DIM(op[0], ondim-idim-1);
            }
            else {
                shape = 1;
            }

            NAD_SHAPE(axisdata) = shape;
            NAD_COORD(axisdata) = 0;
            if (shape == 1) {
                NAD_STRIDES(axisdata)[0] = 0;
            }
            else {
                NAD_STRIDES(axisdata)[0] = PyArray_STRIDE(op[0], ondim-idim-1);
            }
            NAD_PTRS(axisdata)[0] = odataptr;

            axisdata += sizeof_axisdata;
        }
        for (idim = ondim; idim < ndim; ++idim) {
            NAD_SHAPE(axisdata) = 1;
            NAD_COORD(axisdata) = 0;
            NAD_STRIDES(axisdata)[0] = 0;
            NAD_PTRS(axisdata)[0] = odataptr;

            axisdata += sizeof_axisdata;
        }
    }
    else {
        npy_intp *axes = op_axes[0];

        /* Use op_axes to choose the axes */
        axisdata = axisdata0;
        ondim = op_ndim[0];
        odataptr = op_dataptr[0];
        for (idim = 0; idim < ndim; ++idim) {
            npy_intp i = axes[ndim-idim-1];
            if (i < 0) {
                NAD_SHAPE(axisdata) = 1;
                NAD_COORD(axisdata) = 0;
                NAD_STRIDES(axisdata)[0] = 0;
                NAD_PTRS(axisdata)[0] = odataptr;
            }
            else if (i < ondim) {
                npy_intp shape;
                
                if (op[0] != NULL) {
                    shape = PyArray_DIM(op[0], i);
                }
                else {
                    shape = 1;
                }

                NAD_SHAPE(axisdata) = shape;
                NAD_COORD(axisdata) = 0;
                if (shape == 1) {
                    NAD_STRIDES(axisdata)[0] = 0;
                }
                else {
                    NAD_STRIDES(axisdata)[0] = PyArray_STRIDE(op[0], i);
                }
                NAD_PTRS(axisdata)[0] = odataptr;
            }
            else {
                PyErr_Format(PyExc_ValueError,
                        "Iterator input op_axes[0][%d] (==%d) is not a valid "
                        "axis of op[0], which has %d dimensions",
                        (int)(ndim-idim-1), (int)i, (int)ondim);
                return 0;
            }

            axisdata += sizeof_axisdata;
        }
    }

    /*
     * Process the rest of the operands, using the broadcasting rules
     * to combine them.
     */
    for (iiter = 1; iiter < niter; ++iiter) {
        if (op_axes == NULL || op_axes[iiter] == NULL) {
            axisdata = axisdata0;
            ondim = op_ndim[iiter];
            odataptr = op_dataptr[iiter];
            /* Possible if op_axes are being used, but op_axes[iiter] is NULL */
            if (ondim > ndim) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator input has more dimensions than allowed "
                        "by the 'op_axes' specified");
                return 0;
            }
            for (idim = 0; idim < ondim; ++idim) {
                npy_intp shape;
                
                if (op[iiter] != NULL) {
                    shape = PyArray_DIM(op[iiter], ondim-idim-1);
                }
                else {
                    shape = 1;
                }

                if (shape == 1) {
                    NAD_STRIDES(axisdata)[iiter] = 0;
                }
                else {
                    if (NAD_SHAPE(axisdata) == 1) {
                        NAD_SHAPE(axisdata) = shape;
                    }
                    else if (NAD_SHAPE(axisdata) != shape) {
                        PyErr_SetString(PyExc_ValueError,
                                "Iterator input objects cannot be broadcast "
                                "to a single shape");
                        return 0;
                    }
                    NAD_STRIDES(axisdata)[iiter] = PyArray_STRIDE(
                                                      op[iiter], ondim-idim-1);
                }
                NAD_PTRS(axisdata)[iiter] = odataptr;

                axisdata += sizeof_axisdata;
            }
            for (idim = ondim; idim < ndim; ++idim) {
                NAD_STRIDES(axisdata)[iiter] = 0;
                NAD_PTRS(axisdata)[iiter] = odataptr;

                axisdata += sizeof_axisdata;
            }
        }
        else {
            npy_intp *axes = op_axes[iiter];

            /* Use op_axes to choose the axes */
            axisdata = axisdata0;
            ondim = op_ndim[iiter];
            odataptr = op_dataptr[iiter];
            for (idim = 0; idim < ndim; ++idim) {
                npy_intp i = axes[ndim-idim-1];
                if (i < 0) {
                    NAD_STRIDES(axisdata)[iiter] = 0;
                    NAD_PTRS(axisdata)[iiter] = odataptr;
                }
                else if (i < ondim) {
                    npy_intp shape;
                    
                    if (op[iiter] != NULL) {
                        shape = PyArray_DIM(op[iiter], i);
                    }
                    else {
                        shape = 1;
                    }

                    if (shape == 1) {
                        NAD_STRIDES(axisdata)[iiter] = 0;
                    }
                    else {
                        if (NAD_SHAPE(axisdata) == 1) {
                            NAD_SHAPE(axisdata) = shape;
                        }
                        else if (NAD_SHAPE(axisdata) != shape) {
                            PyErr_SetString(PyExc_ValueError,
                                    "Iterator input objects cannot be "
                                    "broadcast to a single shape");
                            return 0;
                        }
                        NAD_STRIDES(axisdata)[iiter] =
                                                PyArray_STRIDE(op[iiter], i);
                    }
                    NAD_PTRS(axisdata)[iiter] = odataptr;
                }
                else {
                    PyErr_Format(PyExc_ValueError,
                            "Iterator input op_axes[%d][%d] (==%d) is not a "
                            "valid axis of op[%d], which has %d dimensions ",
                            (int)iiter, (int)(ndim-idim-1), (int)i,
                            (int)iiter, (int)ondim);
                    return 0;
                }

                axisdata += sizeof_axisdata;
            }
        }
    }

    /* Go through and check for operands with NPY_ITER_NO_BROADCAST set */
    for (iiter = 0; iiter < niter; ++iiter) {
        if (op_flags[iiter]&NPY_ITER_NO_BROADCAST) {
            npy_intp *axes;

            if (op_ndim[iiter] != ndim) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator input has the NO_BROADCAST flag set, but "
                        "has a different number of dimensions than the "
                        "iterator");
                return 0;
            }

            axes = op_axes ? op_axes[iiter] : NULL;
            axisdata = axisdata0;
            for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
                npy_intp i;
                if (axes) {
                    i = axes[ndim-idim-1];
                }
                else {
                    i = ndim-idim-1;
                }
                if (PyArray_DIM(op[iiter], i) != NAD_SHAPE(axisdata)) {
                    PyErr_SetString(PyExc_ValueError,
                            "Iterator input has the NO_BROADCAST flag set, "
                            "but has different dimensions than the final "
                            "broadcast shape of the iterator");
                    return 0;
                }
            }
        }
    }

    /* Now fill in the ITERSIZE member */
    NIT_ITERSIZE(iter) = 1;
    axisdata = axisdata0;
    for (idim = 0; idim < ndim; ++idim) {
        NIT_ITERSIZE(iter) *= NAD_SHAPE(axisdata);

        axisdata += sizeof_axisdata;
    }

    return 1;
}

/*
 * Replaces the AXISDATA for the iiter'th operand, broadcasting
 * the dimensions as necessary.  Assumes the replacement array is
 * exactly the same shape as the original array used when
 * npy_fill_axisdata was called.
 *
 * If op_axes is not NULL, it should point to an ndim-sized
 * array.
 */
static void
npyiter_replace_axisdata(NpyIter *iter, npy_intp iiter,
                      PyArrayObject *op,
                      npy_intp op_ndim, char *op_dataptr,
                      npy_intp *op_axes)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char *axisdata0, *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp *perm;

    perm = NIT_PERM(iter);
    axisdata0 = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    /*
     * Replace just the strides which were non-zero, and compute
     * the base data address.
     */
    axisdata = axisdata0;

    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        npy_intp i, p, shape;
        
        /* Apply the perm to get the original axis */
        p = perm[idim];
        if (p < 0) {
            i = ndim+p;
        }
        else {
            i = ndim-p-1;
        }

        /* Apply op_axes */
        if (op_axes != NULL) {
            i = op_axes[i];
        }
        else {
            i -= (ndim - op_ndim);
        }

        if (i >= 0 && i < op_ndim) {
            shape = PyArray_DIM(op, i);
            if (shape != 1) {
                npy_intp stride = PyArray_STRIDE(op, i);
                if (p < 0) {
                    /* If the perm entry is negative, flip the axis */
                    NAD_STRIDES(axisdata)[iiter] = -stride;
                    op_dataptr += stride*(shape-1);
                }
                else {
                    NAD_STRIDES(axisdata)[iiter] = stride;
                }
            }
        }
    }

    /* Now the base data pointer is calculated, set it everywhere its needed */
    NIT_RESETDATAPTR(iter)[iiter] = op_dataptr;
    axisdata = axisdata0;
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        NAD_PTRS(axisdata)[iiter] = op_dataptr;
    }
}

/*
 * Computes the iterator's index strides and initializes the index values
 * to zero.
 *
 * This must be called before the axes (i.e. the AXISDATA array) may
 * be reordered.
 */
static void
npyiter_compute_index_strides(NpyIter *iter, npy_uint32 flags)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp indexstride;
    char* axisdata;
    npy_intp sizeof_axisdata;

    /*
     * If there is only one element being iterated, we just have
     * to touch the first AXISDATA because nothing will ever be
     * incremented.
     */
    if (NIT_ITERSIZE(iter) == 1) {
        if (itflags&NPY_ITFLAG_HASINDEX) {
            axisdata = NIT_AXISDATA(iter);
            NAD_PTRS(axisdata)[niter] = 0;
        }
        return;
    }

    if (flags&NPY_ITER_C_ORDER_INDEX) {
        sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
        axisdata = NIT_AXISDATA(iter);
        indexstride = 1;
        for(idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
            npy_intp shape = NAD_SHAPE(axisdata);

            if (shape == 1) {
                NAD_STRIDES(axisdata)[niter] = 0;
            }
            else {
                NAD_STRIDES(axisdata)[niter] = indexstride;
            }
            NAD_PTRS(axisdata)[niter] = 0;
            indexstride *= shape;
        }
    }
    else if (flags&NPY_ITER_F_ORDER_INDEX) {
        sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
        axisdata = NIT_AXISDATA(iter) + (ndim-1)*sizeof_axisdata;
        indexstride = 1;
        for(idim = 0; idim < ndim; ++idim, axisdata -= sizeof_axisdata) {
            npy_intp shape = NAD_SHAPE(axisdata);

            if (shape == 1) {
                NAD_STRIDES(axisdata)[niter] = 0;
            }
            else {
                NAD_STRIDES(axisdata)[niter] = indexstride;
            }
            NAD_PTRS(axisdata)[niter] = 0;
            indexstride *= shape;
        }
    }
}

/*
 * If the order is NPY_KEEPORDER, lets the iterator find the best
 * iteration order, otherwise forces it.  Indicates in the itflags that
 * whether the iteration order was forced.
 */
static void
npyiter_apply_forced_iteration_order(NpyIter *iter, NPY_ORDER order)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    switch (order) {
    case NPY_CORDER:
        NIT_ITFLAGS(iter) |= NPY_ITFLAG_FORCEDORDER;
        break;
    case NPY_FORTRANORDER:
        NIT_ITFLAGS(iter) |= NPY_ITFLAG_FORCEDORDER;
        /* Only need to actually do something if there is more than 1 dim */
        if (ndim > 1) {
            npyiter_reverse_axis_ordering(iter);
        }
        break;
    case NPY_ANYORDER:
        NIT_ITFLAGS(iter) |= NPY_ITFLAG_FORCEDORDER;
        /* Only need to actually do something if there is more than 1 dim */
        if (ndim > 1) {
            PyArrayObject **op = NIT_OBJECTS(iter);
            int forder = 1;

            /* Check that all the array inputs are fortran order */
            for (iiter = 0; iiter < niter; ++iiter, ++op) {
                if (*op && !PyArray_CHKFLAGS(*op, NPY_F_CONTIGUOUS)) {
                   forder = 0;
                   break;
                }
            }

            if (forder) {
                npyiter_reverse_axis_ordering(iter);
            }
        }
        break;
    case NPY_KEEPORDER:
        /* Don't set the forced order flag here... */
        break;
    }
}


/*
 * This function negates any strides in the iterator
 * which are negative.  When iterating more than one
 * object, it only flips strides when they are all
 * negative or zero.
 */
static void
npyiter_flip_negative_strides(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp istrides, nstrides = NAD_NSTRIDES();
    char *axisdata, *axisdata0;
    npy_intp *ptrs0;
    npy_intp sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    int any_flipped = 0;

    axisdata0 = axisdata = NIT_AXISDATA(iter);
    ptrs0 = (npy_intp*)NAD_PTRS(axisdata0);
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        npy_intp *strides = NAD_STRIDES(axisdata);
        int any_negative = 0;

        /*
         * Check the signs of all the strides, excluding
         * the index stride at the end.
         */
        for (iiter = 0; iiter < niter; ++iiter) {
            if (strides[iiter] < 0) {
                any_negative = 1;
            }
            else if (strides[iiter] != 0) {
                break;
            }
        }
        /*
         * If at least on stride is negative and none are positive,
         * flip all the strides for this dimension.
         */
        if (any_negative && iiter == niter) {
            npy_intp shapem1 = NAD_SHAPE(axisdata) - 1;

            for (istrides = 0; istrides < nstrides; ++istrides) {
                npy_intp stride = strides[istrides];

                /* Adjust the base pointers to start at the end */
                ptrs0[istrides] += shapem1 * stride;
                /* Flip the stride */
                strides[istrides] = -stride;
            }
            /* Make the perm entry negative, so getcoords knows it's  flipped */
            NIT_PERM(iter)[idim] = -1-NIT_PERM(iter)[idim];

            any_flipped = 1;
        }
    }

    /*
     * If any strides were flipped, the base pointers were adjusted
     * in the first AXISDATA, and need to be copied to all the rest
     */
    if (any_flipped) {
        npy_intp *resetdataptr = (npy_intp*)NIT_RESETDATAPTR(iter);

        for (istrides = 0; istrides < nstrides; ++istrides) {
            resetdataptr[istrides] = ptrs0[istrides];
        }
        axisdata = axisdata0;
        for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
            npy_intp *ptrs = (npy_intp*)NAD_PTRS(axisdata);
            for (istrides = 0; istrides < nstrides; ++istrides) {
                ptrs[istrides] = ptrs0[istrides];
            }
        }
        /*
         * Indicate that some of the perm entries are negative,
         * and that it's not (strictly speaking) the identity perm.
         */
        NIT_ITFLAGS(iter) = (NIT_ITFLAGS(iter)|NPY_ITFLAG_NEGPERM) &
                            ~NPY_ITFLAG_IDENTPERM;
    }
}

static void
npyiter_reverse_axis_ordering(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp i, temp, size;
    npy_intp* first;
    npy_intp* last;

    /* Need at least two dimensions for reversing to change things */
    if (ndim < 2) {
        return;
    }

    size = NIT_SIZEOF_AXISDATA(itflags, ndim, niter)/NPY_SIZEOF_INTP;
    first = (npy_intp*)NIT_AXISDATA(iter);
    last = first + (ndim-1)*size;

    /* This loop reverses the order of the AXISDATA array */
    while (first < last) {
        for (i = 0; i < size; ++i) {
            temp = first[i];
            first[i] = last[i];
            last[i] = temp;
        }
        first += size;
        last -= size;
    }

    /* Store the perm we applied */
    first = NIT_PERM(iter);
    for(i = ndim-1; i >= 0; --i, ++first) {
        *first = i;
    }

    NIT_ITFLAGS(iter) &= ~NPY_ITFLAG_IDENTPERM;
}

static npy_intp intp_abs(npy_intp x)
{
    return (x < 0) ? -x : x;
}

static void 
npyiter_find_best_axis_ordering(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp i0, i1, ipos, j0, j1;
    npy_intp *perm;
    char *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    int permuted = 0;

    perm = NIT_PERM(iter);
    
    /*
     * Do a custom stable insertion sort.  Note that because
     * the AXISDATA has been reversed from C order, this
     * is sorting from smallest stride to biggest stride.
     */
    for (i0 = 1; i0 < ndim; ++i0) {
        npy_intp *strides0;

        /* 'ipos' is where perm[i0] will get inserted */
        ipos = i0;
        j0 = perm[i0];
        strides0 = NAD_STRIDES(axisdata + j0*sizeof_axisdata);
        for (i1 = i0-1; i1 >= 0; --i1) {
            int ambig = 1, shouldswap = 0;
            npy_intp *strides1;

            j1 = perm[i1];
            strides1 = NAD_STRIDES(axisdata + j1*sizeof_axisdata);

            for (iiter = 0; iiter < niter; ++iiter) {
                if (strides0[iiter] != 0 && strides1[iiter] != 0) {
                    if (intp_abs(strides1[iiter]) <=
                                            intp_abs(strides0[iiter])) {
                        /*
                         * Set swap even if it's not ambiguous already,
                         * because in the case of conflicts between
                         * different operands, C-order wins.
                         */
                        shouldswap = 0;
                    }
                    else {
                        /* Only set swap if it's still ambiguous */
                        if (ambig) {
                            shouldswap = 1;
                        }
                    }
                    
                    /*
                     * A comparison has been done, so it's
                     * no longer ambiguous
                     */
                    ambig = 0;
                }
            }
            /*
             * If the comparison was unambiguous, either shift
             * 'ipos' to 'i1' or stop looking for an insertion
             * point
             */
            if (!ambig) {
                if (shouldswap) {
                    ipos = i1;
                }
                else {
                    break;
                }
            }
        }

        /* Insert perm[i0] into the right place */
        if (ipos != i0) {
            for (i1 = i0; i1 > ipos; --i1) {
                perm[i1] = perm[i1-1];
            }
            perm[ipos] = j0;
            permuted = 1;
        }
    }

    /* Apply the computed permutation to the AXISDATA array */
    if (permuted == 1) {
        npy_intp i, size = sizeof_axisdata/NPY_SIZEOF_INTP;
        char *ad_i;

        /* Use the coord as a flag, set each to 1 */
        for (idim = 0; idim < ndim; ++idim) {
            NAD_COORD(axisdata + idim*sizeof_axisdata) = 1;
        }
        /* Apply the permutation by following the cycles */
        for (idim = 0; idim < ndim; ++idim) {
            ad_i = axisdata + idim*sizeof_axisdata;

            /* If this axis hasn't been touched yet, process it */
            if (NAD_COORD(ad_i) == 1) {
                npy_intp pidim = perm[idim], qidim, tmp;
                char *ad_p, *ad_q;

                if (pidim != idim) {
                    /* Follow the cycle, copying the data */
                    for (i = 0; i < size; ++i) {
                        qidim = idim;
                        pidim = perm[idim];
                        ad_q = ad_i;
                        tmp = *((npy_intp*)ad_q + i);
                        while (pidim != idim) {
                            ad_p = axisdata + pidim*sizeof_axisdata;
                            *((npy_intp*)ad_q + i) = *((npy_intp*)ad_p + i);

                            qidim = pidim;
                            ad_q = ad_p;
                            pidim = perm[pidim];
                        }
                        *((npy_intp*)ad_q + i) = tmp;
                    }
                    /* Follow the cycle again, marking it as done */
                    pidim = perm[idim];
                    while (pidim != idim) {
                        NAD_COORD(axisdata + pidim*sizeof_axisdata) = 0;
                        pidim = perm[pidim];
                    }
                }
                NAD_COORD(ad_i) = 0;
            }
        }
        /* Clear the identity perm flag */
        NIT_ITFLAGS(iter) &= ~NPY_ITFLAG_IDENTPERM;
    }
}

static void 
npyiter_coalesce_axes(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp istrides, nstrides = NAD_NSTRIDES();
    char *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    char *ad_compress;
    npy_intp new_ndim = 1;

    /* The HASCOORDS or IDENTPERM flags do not apply after coalescing */
    NIT_ITFLAGS(iter) &= ~(NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_HASCOORDS);

    axisdata = NIT_AXISDATA(iter);
    ad_compress = axisdata;

    for (idim = 0; idim < ndim-1; ++idim) {
        int can_coalesce = 1;
        npy_intp shape0 = NAD_SHAPE(ad_compress);
        npy_intp shape1 = NAD_SHAPE(axisdata + sizeof_axisdata);
        npy_intp *strides0 = NAD_STRIDES(ad_compress);
        npy_intp *strides1 = NAD_STRIDES(axisdata + sizeof_axisdata);

        /* Check that all the axes can be coalesced */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            if (!((shape0 == 1 && strides0[istrides] == 0) ||
                  (shape1 == 1 && strides1[istrides] == 0)) &&
                     (strides0[istrides]*shape0 != strides1[istrides])) {
                can_coalesce = 0;
                break;
            }
        }

        if (can_coalesce) {
            npy_intp *strides = NAD_STRIDES(ad_compress);

            axisdata += sizeof_axisdata;
            NAD_SHAPE(ad_compress) *= NAD_SHAPE(axisdata);
            for (istrides = 0; istrides < nstrides; ++istrides) {
                if (strides[istrides] == 0) {
                    strides[istrides] = NAD_STRIDES(axisdata)[istrides];
                }
            }
        }
        else {
            axisdata += sizeof_axisdata;
            ad_compress += sizeof_axisdata;
            if (ad_compress != axisdata) {
                memcpy(ad_compress, axisdata, sizeof_axisdata);
            }
            ++new_ndim;
        }
    }

    /*
     * If the number of axes shrunk, reset the perm and
     * compress the data into the new layout.
     */
    if (new_ndim < ndim) {
        npy_intp *perm = NIT_PERM(iter);

        /* Reset to an identity perm */
        for (idim = 0; idim < new_ndim; ++idim) {
            perm[idim] = idim;
        }
        npyiter_shrink_ndim(iter, new_ndim);
    }
}

static void
npyiter_shrink_ndim(NpyIter *iter, npy_intp new_ndim)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char *iterdata = (char*)iter;

    /*
     * The only place that will shift is perm[ndim], so only one
     * memmove is needed for the data after that.  Also, any
     * extra buffers stored after the main iterator structure shouldn't
     * be moved.
     */
    memmove(iterdata + NIT_DTYPES_OFFSET(itflags, new_ndim, niter),
            iterdata + NIT_DTYPES_OFFSET(itflags, ndim, niter),
            NIT_SIZEOF_ITERATOR(itflags, new_ndim, niter) -
            NIT_DTYPES_OFFSET(itflags, new_ndim, niter));
    
    NIT_NDIM(iter) = new_ndim;
}

/*
 * Allocates a temporary array which can be used to replace op
 * in the iteration.  Its dtype will be op_dtype.
 *
 * The result array has a memory ordering which matches the iterator,
 * which may or may not match that of op.  The parameter 'shape' may be
 * NULL, in which case it is filled in from the iterator's shape.
 *
 * This function must be called before any axes are coalesced.
 */
static PyArrayObject *
npyiter_new_temp_array(NpyIter *iter, PyTypeObject *subtype,
                npy_intp op_ndim, npy_intp *shape,
                PyArray_Descr *op_dtype, npy_intp *op_axes)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp *perm = NIT_PERM(iter);
    npy_intp new_shape[NPY_MAXDIMS], strides[NPY_MAXDIMS],
             stride = op_dtype->elsize;
    char reversestride[NPY_MAXDIMS], anyreverse = 0;
    char *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    PyArrayObject *ret;

    /* Initially no strides have been set */
    for (idim = 0; idim < op_ndim; ++idim) {
        strides[idim] = 0;
        reversestride[idim] = 0;
    }

    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        npy_intp i, p;
        
        /* Apply the perm to get the original axis */
        p = perm[idim];
        if (p < 0) {
            i = ndim+p;
        }
        else {
            i = ndim-p-1;
        }

        /* Apply op_axes */
        if (op_axes != NULL) {
            i = op_axes[i];
        }
        else {
            i -= (ndim - op_ndim);
        }

        if (i >= 0) {
            strides[i] = stride;
            stride *= NAD_SHAPE(axisdata);
            if (p < 0) {
                reversestride[i] = 1;
                anyreverse = 1;
            }
            if (shape == NULL) {
                new_shape[i] = NAD_SHAPE(axisdata);
            }
        }
    }

    /*
     * If custom axes were specified, some dimensions may not have been used.
     * Throw an error in this case.
     */
    for (idim = 0; idim < op_ndim; ++idim) {
        if (strides[idim] == 0) {
            if (shape) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator input requiring a temporary copy "
                        "had custom axes specified which don't cover "
                        "all the input axes");
            }
            else {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator automatically allocated output "
                        "had custom axes specified which don't cover "
                        "all the input axes");
            }
            return NULL;
        }
    }

    /* If shape was NULL, set it to the shape we calculated */
    if (shape == NULL) {
        shape = new_shape;
    }

    /* Allocate the temporary array */
    Py_INCREF(op_dtype);
    ret = (PyArrayObject *)PyArray_NewFromDescr(subtype, op_dtype, op_ndim,
                               shape, strides, NULL, 0, NULL);
    if (ret == NULL) {
        return NULL;
    }
    
    /* If there are any reversed axes, create a view that reverses them */
    if (anyreverse) {
        char *dataptr = PyArray_DATA(ret);
        PyArrayObject *newret;

        for (idim = 0; idim < op_ndim; ++idim) {
            if (reversestride[idim]) {
                dataptr += strides[idim]*(shape[idim]-1);
                strides[idim] = -strides[idim];
            }
        }
        Py_INCREF(op_dtype);
        newret = (PyArrayObject *)PyArray_NewFromDescr(subtype,
                              op_dtype, op_ndim,
                              shape, strides, dataptr,
                              NPY_WRITEABLE, NULL);
        if (newret == NULL) {
            Py_DECREF(ret);
            return NULL;
        }
        newret->base = (PyObject *)ret;
        ret = newret;
    }

    /* Make sure all the flags are good */
    PyArray_UpdateFlags(ret, NPY_UPDATE_ALL);

    /* Double-check that the subtype didn't mess with the dimensions */
    if (subtype != &PyArray_Type) {
        if (PyArray_NDIM(ret) != op_ndim ||
                    !PyArray_CompareLists(shape, PyArray_DIMS(ret), op_ndim)) {
            PyErr_SetString(PyExc_RuntimeError,
                    "Iterator automatic output has an array subtype "
                    "which changed the dimensions of the output");
            Py_DECREF(ret);
            return NULL;
        }
    }

    return ret;
}

/*
 * Calculates a dtype that all the types can be promoted to, using the
 * ufunc rules.  If only_inputs is 1, it leaves any operands that
 * are not read from out of the calculation.
 */
static PyArray_Descr *
npyiter_get_common_dtype(npy_intp niter, PyArrayObject **op, npy_intp *op_ndim,
                        char *op_itflags, PyArray_Descr **op_dtype,
                        int only_inputs)
{
    PyArray_Descr *scalar_dtype = NULL, *array_dtype = NULL;
    int scalar_kind = NPY_NOSCALAR;
    npy_intp iiter;

    /* First promote all the scalar dtypes */
    for (iiter = 0; iiter < niter; ++iiter) {
        if (op_ndim[iiter] == 0 && op_dtype[iiter] != NULL &&
                    (!only_inputs || (op_itflags[iiter]&NPY_OP_ITFLAG_READ))) {
            if (scalar_dtype == NULL) {
                scalar_dtype = op_dtype[iiter];
                Py_INCREF(scalar_dtype);
                /*
                 * Note: because dtype and op[iiter] may not have the same
                 *       type, this value could be (slightly) wrong in
                 *       very few cases.
                 */
                scalar_kind = PyArray_ScalarKind(scalar_dtype->type_num,
                                                            &op[iiter]);
            }
            else {
                PyArray_Descr *dtype = op_dtype[iiter];
                int kind, typenum;

                if (PyTypeNum_ISEXTENDED(scalar_dtype->type_num) ||
                                PyTypeNum_ISEXTENDED(dtype->type_num)) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_DECREF(scalar_dtype);
                    return NULL;
                }
                /*
                 * Note: because dtype and op[iiter] may not have the same
                 *       type, this value could be (slightly) wrong in
                 *       very few cases.
                 */
                kind = PyArray_ScalarKind(dtype->type_num, &op[iiter]);
                if (kind > scalar_kind) {
                    scalar_kind = kind;
                }
                typenum = npyiter_promote_types(scalar_dtype->type_num,
                                                  dtype->type_num);
                if (typenum == NPY_NOTYPE) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_DECREF(scalar_dtype);
                    return NULL;
                }

                dtype = PyArray_DescrFromType(typenum);
                Py_DECREF(scalar_dtype);
                scalar_dtype = dtype;
            }
        }
    }

    /* Then promote all the array dtypes */
    for (iiter = 0; iiter < niter; ++iiter) {
        if (op_ndim[iiter] > 0 && op_dtype[iiter] != NULL &&
                    (!only_inputs || (op_itflags[iiter]&NPY_OP_ITFLAG_READ))) {
            if (array_dtype == NULL) {
                array_dtype = op_dtype[iiter];
                Py_INCREF(array_dtype);
            }
            else {
                PyArray_Descr *dtype = op_dtype[iiter];
                int typenum;

                if (PyTypeNum_ISEXTENDED(array_dtype->type_num) ||
                                PyTypeNum_ISEXTENDED(dtype->type_num)) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_XDECREF(scalar_dtype);
                    return NULL;
                }
                typenum = npyiter_promote_types(array_dtype->type_num,
                                                  dtype->type_num);
                if (typenum == NPY_NOTYPE) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_XDECREF(scalar_dtype);
                    return NULL;
                }
                            
                dtype = PyArray_DescrFromType(typenum);
                Py_DECREF(array_dtype);
                array_dtype = dtype;
            }
        }
    }

    /* Now combine the scalar and array types */
    if (scalar_dtype != NULL && array_dtype != NULL) {
        PyArray_Descr *dtype;
        if (PyArray_CanCoerceScalar(scalar_dtype->type_num,
                                    array_dtype->type_num,
                                    scalar_kind)) {
            dtype = PyArray_DescrFromType(array_dtype->type_num);
            Py_DECREF(scalar_dtype);
            Py_DECREF(array_dtype);
            return dtype;
        }
        else {
            int typenum;

            typenum = npyiter_promote_types(scalar_dtype->type_num,
                                            array_dtype->type_num);
            Py_DECREF(scalar_dtype);
            Py_DECREF(array_dtype);
            return PyArray_DescrFromType(typenum);
        }
    }
    else if (array_dtype != NULL) {
        return array_dtype;
    }
    else if (scalar_dtype != NULL) {
        return scalar_dtype;
    }
    else {
        PyErr_SetString(PyExc_TypeError,
                "Iterator allocated output could not "
                "determine a data type based on the inputs");
        return NULL;
    }
}

/*
 * TODO This is a slow way to do it and depends on the type
 * number ordering.  Should be based on a table like scalar kinds.
 */
static int
npyiter_promote_types(int type1, int type2)
{
    int i;

    for(i = 0; i < NPY_NTYPES; ++i) {
        if (PyArray_CanCastSafely(type1, i) &&
                            PyArray_CanCastSafely(type2, i)) {
            return i;
        }
    }

    return NPY_NOTYPE;
}

static int
npyiter_allocate_buffers(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);
    char *bufferdata = NIT_BUFFERDATA(iter), *axisdata = NIT_AXISDATA(iter);
    PyArrayObject **op = NIT_OBJECTS(iter);
    PyArray_Descr **op_dtype = NIT_DTYPES(iter);
    npy_intp *strides = NAD_STRIDES(axisdata), op_stride;
    npy_intp buffersize = NBF_BUFFERSIZE(bufferdata);
    char *buffer;

    PyArray_StridedTransferFn stransfer = NULL;
    void *transferdata = NULL;

    for (iiter = 0; iiter < niter; ++iiter) {
        char flags = op_itflags[iiter];
        op_stride = strides[iiter];

        /*
         * If we have determined that a buffer may be needed,
         * allocate one.
         */
        if (!(flags&NPY_OP_ITFLAG_BUFNEVER)) {
            npy_intp itemsize = op_dtype[iiter]->elsize;
            buffer = PyArray_malloc(itemsize*buffersize);
            if (buffer == NULL) {
                PyErr_NoMemory();
                return 0;
            }
            NBF_BUFFERS(bufferdata)[iiter] = buffer;

            /* Also need to get an appropriate transfer functions */
            if (flags&NPY_OP_ITFLAG_READ) {
                if (PyArray_GetDTypeTransferFunction(
                                        (flags&NPY_OP_ITFLAG_ALIGNED) != 0,
                                        op_stride,
                                        op_dtype[iiter]->elsize,
                                        PyArray_DESCR(op[iiter]),
                                        op_dtype[iiter],
                                        &stransfer,
                                        &transferdata) != NPY_SUCCEED) {
                    return 0;
                }
                NBF_READTRANSFERFN(bufferdata)[iiter] = stransfer;
                NBF_READTRANSFERDATA(bufferdata)[iiter] = transferdata;
            }
            else {
                NBF_READTRANSFERFN(bufferdata)[iiter] = NULL;
            }
            if (flags&NPY_OP_ITFLAG_WRITE) {
                if (PyArray_GetDTypeTransferFunction(
                                        (flags&NPY_OP_ITFLAG_ALIGNED) != 0,
                                        op_dtype[iiter]->elsize,
                                        op_stride,
                                        op_dtype[iiter],
                                        PyArray_DESCR(op[iiter]),
                                        &stransfer,
                                        &transferdata) != NPY_SUCCEED) {
                    return 0;
                }
                NBF_WRITETRANSFERFN(bufferdata)[iiter] = stransfer;
                NBF_WRITETRANSFERDATA(bufferdata)[iiter] = transferdata;
            }
            else {
                NBF_WRITETRANSFERFN(bufferdata)[iiter] = NULL;
            }
        }
        else {
            NBF_READTRANSFERFN(bufferdata)[iiter] = NULL;
            NBF_WRITETRANSFERFN(bufferdata)[iiter] = NULL;
        }
    }

    return 1;
}

/*
 * This function jumps the iterator forward by 'count' steps.
 * It is used by the buffering code, and only affects the non-buffering
 * data.
 *
 * Returns 1 if it landed on a valid element, 0 if it went
 * past the end.
 */
static int
npyiter_jumpforward(NpyIter *iter, npy_intp count)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char *axisdata, **dataptr;
    npy_intp sizeof_axisdata;
    npy_intp delta, coord, shape;
    npy_intp istrides, nstrides;

    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    axisdata = NIT_AXISDATA(iter);

    /* Go forward through axisdata, incrementing the coordinates */
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        shape = NAD_SHAPE(axisdata);
        delta = count % shape;
        coord = NAD_COORD(axisdata) + delta;
        count -= delta;
        if (coord >= shape) {
            coord -= shape;
            count += shape;
        }
        NAD_COORD(axisdata) = coord;
        if (count == 0) {
            break;
        }
        count /= shape;
    }

    /* If it incremented past the end, set to the last coordinate */
    if (count > 0) {
        axisdata = NIT_AXISDATA(iter);
        for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
            NAD_COORD(axisdata) = NAD_SHAPE(axisdata)-1;
        }
        return 0;
    }

    /* Go backwards through axisdata, applying the new coordinates */
    dataptr = NIT_RESETDATAPTR(iter);
    nstrides = NAD_NSTRIDES();

    /*
     * Set the pointers, from the slowest-changing to the
     * fastest-changing.  The successive pointers accumulate
     * the offsets, starting from the original data pointers.
     */
    axisdata = NIT_AXISDATA(iter) + (ndim-1)*sizeof_axisdata;
    for (idim = 0; idim < ndim; ++idim, axisdata -= sizeof_axisdata) {
        npy_intp *strides;
        char **ptrs;

        coord = NAD_COORD(axisdata);
        strides = NAD_STRIDES(axisdata);
        ptrs = NAD_PTRS(axisdata);

        for (istrides = 0; istrides < nstrides; ++istrides) {
            ptrs[istrides] = dataptr[istrides] + coord*strides[istrides];
        }

        dataptr = ptrs;
    }

    return 1;
}

/*
 * This checks how much space is left before we reach the end of
 * the iterator, and whether the whole buffer can be done with one
 * stride.
 */
static npy_intp
npyiter_checkspaceleft(NpyIter *iter, npy_intp count,
                        int *out_is_onestride)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp coord, shape;
    npy_intp spaceleft = 1, factor = 1;

    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    axisdata = NIT_AXISDATA(iter);

    /* Go forward through axisdata, calculating the space left */
    for (idim = 0; idim < ndim && spaceleft < count;
                                    ++idim, axisdata += sizeof_axisdata) {
        shape = NAD_SHAPE(axisdata);
        coord = NAD_COORD(axisdata);
        spaceleft += (shape-coord-1) * factor;
        factor *= shape;
    }

    /* If we broke out after dimension 0, it works with one stride */
    if (idim == 1) {
        *out_is_onestride = 1;
    }
    else {
        *out_is_onestride = 0;
    }

    /* Return either count or how much space is left */
    if (spaceleft < count) {
        return spaceleft;
    }
    else {
        return count;
    }
}

/*
 * This gets called after the the buffers have been exhausted, and
 * their data needs to be written back to the arrays.  The coordinates
 * must be positioned for the beginning of the buffer.
 */
static void
npyiter_copy_from_buffers(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);
    char *bufferdata = NIT_BUFFERDATA(iter),
         *axisdata = NIT_AXISDATA(iter);

    PyArray_Descr **dtypes = NIT_DTYPES(iter);
    npy_intp transfersize = NBF_SIZE(bufferdata);
    npy_intp *strides = NBF_STRIDES(bufferdata),
             *ad_strides = NAD_STRIDES(axisdata);
    char **ptrs = NBF_PTRS(bufferdata), **ad_ptrs = NAD_PTRS(axisdata);
    char **buffers = NBF_BUFFERS(bufferdata);

    PyArray_StridedTransferFn stransfer = NULL;
    void *transferdata = NULL;
    
    npy_intp axisdata_incr = NIT_SIZEOF_AXISDATA(itflags, ndim, niter) /
                                NPY_SIZEOF_INTP;

    /* If we're past the end, nothing to copy */
    if (NBF_SIZE(bufferdata) == 0) {
        return;
    }

    for (iiter = 0; iiter < niter; ++iiter) {
        stransfer = NBF_WRITETRANSFERFN(bufferdata)[iiter];
        transferdata = NBF_WRITETRANSFERDATA(bufferdata)[iiter];
        if ((stransfer != NULL) && (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE)) {
            /* Copy back only if the pointer was pointing to the buffer */
            npy_intp delta = (ptrs[iiter] - buffers[iiter]);
            if (0 <= delta && delta <= transfersize*dtypes[iiter]->elsize) {
                PyArray_TransferStridedToNDim(ndim,
                        ad_ptrs[iiter], &ad_strides[iiter], axisdata_incr,
                        buffers[iiter], strides[iiter],
                        &NAD_COORD(axisdata), axisdata_incr,
                        &NAD_SHAPE(axisdata), axisdata_incr,
                        transfersize, dtypes[iiter]->elsize,
                        stransfer,
                        transferdata);
            }
        }
    }
}

/*
 * This gets called after the iterator has been positioned to coordinates
 * for the start of a buffer.  It decides which operands need a buffer,
 * and copies the data into the buffers.
 */
static void
npyiter_copy_to_buffers(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);
    char *bufferdata = NIT_BUFFERDATA(iter),
         *axisdata = NIT_AXISDATA(iter);

    PyArray_Descr **dtypes = NIT_DTYPES(iter);
    npy_intp *strides = NBF_STRIDES(bufferdata),
             *ad_strides = NAD_STRIDES(axisdata);
    char **ptrs = NBF_PTRS(bufferdata), **ad_ptrs = NAD_PTRS(axisdata);
    char **buffers = NBF_BUFFERS(bufferdata);
    npy_intp buffersize = NBF_BUFFERSIZE(bufferdata), transfersize;
    int is_onestride = 0, any_buffered = 0;

    PyArray_StridedTransferFn stransfer = NULL;
    void *transferdata = NULL;

    npy_intp axisdata_incr = NIT_SIZEOF_AXISDATA(itflags, ndim, niter) /
                                NPY_SIZEOF_INTP;

    transfersize = npyiter_checkspaceleft(iter, buffersize, &is_onestride);
    NBF_SIZE(bufferdata) = transfersize;
    NBF_POS(bufferdata) = 0;

    for (iiter = 0; iiter < niter; ++iiter) {
        /*
         * If the buffer is write-only, these two are NULL, and the buffer
         * pointers will be set up but the read copy won't be done
         */
        stransfer = NBF_READTRANSFERFN(bufferdata)[iiter];
        transferdata = NBF_READTRANSFERDATA(bufferdata)[iiter];
        switch (op_itflags[iiter]&
                        (NPY_OP_ITFLAG_BUFNEVER|
                         NPY_OP_ITFLAG_COPYSWAP|
                         NPY_OP_ITFLAG_CAST)) {
            /* never need to buffer this operand */
            case NPY_OP_ITFLAG_BUFNEVER:
                ptrs[iiter] = ad_ptrs[iiter];
                /*
                 * Should not adjust the stride - ad_strides[iiter]
                 * could be zero, but strides[iiter] was initialized
                 * to the first non-trivial stride.
                 */
                stransfer = NULL;
                break;
            /* just a copy */
            case 0:
                /*
                 * No copyswap or cast was requested, so all we're
                 * doing is copying the data to fill the buffer and
                 * produce a single stride.  If the underlying data
                 * already does that, no need to copy it.
                 */
                if (is_onestride) {
                    ptrs[iiter] = ad_ptrs[iiter];
                    strides[iiter] = ad_strides[iiter];
                    stransfer = NULL;
                }
                else {
                    /* In this case, the buffer is being used */
                    ptrs[iiter] = buffers[iiter];
                    strides[iiter] = dtypes[iiter]->elsize;
                }
                break;
            default:
                /* In this case, the buffer is being used */
                ptrs[iiter] = buffers[iiter];
                strides[iiter] = dtypes[iiter]->elsize;
                break;
        }

        if (stransfer != NULL) {
            /*printf("transfer %p -> %p\n", ad_ptrs[iiter], ptrs[iiter]);*/
            any_buffered = 1;
            PyArray_TransferNDimToStrided(ndim,
                    ptrs[iiter], strides[iiter],
                    ad_ptrs[iiter], &ad_strides[iiter], axisdata_incr,
                    &NAD_COORD(axisdata), axisdata_incr,
                    &NAD_SHAPE(axisdata), axisdata_incr,
                    transfersize, dtypes[iiter]->elsize,
                    stransfer,
                    transferdata);
        }

    }

    /*
     * If buffering wasn't needed, we can grow the inner
     * loop to as large as possible.
     */
    if (!any_buffered && (itflags&NPY_ITFLAG_GROWINNER)) {
        npy_intp maxsize = NAD_SHAPE(axisdata)-NAD_COORD(axisdata);
        if (maxsize > transfersize) {
            NBF_SIZE(bufferdata) = maxsize;
        }
    }
}

/* For debugging */
NPY_NO_EXPORT void
NpyIter_DebugPrint(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *axisdata;
    npy_intp sizeof_axisdata;

    printf("\n------ BEGIN ITERATOR DUMP ------\n");
    printf("Iterator Address: %p\n", iter);
    printf("Flags: ");
    if (itflags&NPY_ITFLAG_IDENTPERM)
        printf("IDENTPERM ");
    if (itflags&NPY_ITFLAG_NEGPERM)
        printf("NEGPERM ");
    if (itflags&NPY_ITFLAG_HASINDEX)
        printf("HASINDEX ");
    if (itflags&NPY_ITFLAG_HASCOORDS)
        printf("HASCOORDS ");
    if (itflags&NPY_ITFLAG_HASOFFSETS)
        printf("HASOFFSETS ");
    if (itflags&NPY_ITFLAG_FORCEDORDER)
        printf("FORCEDORDER ");
    if (itflags&NPY_ITFLAG_NOINNER)
        printf("NOINNER ");
    if (itflags&NPY_ITFLAG_BUFFER)
        printf("BUFFER ");
    if (itflags&NPY_ITFLAG_GROWINNER)
        printf("GROWINNER ");
    printf("\n");
    printf("NDim: %d\n", (int)ndim);
    printf("NIter: %d\n", (int)niter);
    printf("IterSize: %d\n", (int)NIT_ITERSIZE(iter));
    printf("Iterator SizeOf: %d\n",
                            (int)NIT_SIZEOF_ITERATOR(itflags, ndim, niter));
    printf("AxisData SizeOf: %d\n",
                            (int)NIT_SIZEOF_AXISDATA(itflags, ndim, niter));
    printf("\n");

    printf("Perm: ");
    for (idim = 0; idim < ndim; ++idim) {
        printf("%d ", (int)NIT_PERM(iter)[idim]);
    }
    printf("\n");
    printf("DTypes: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_DTYPES(iter)[iiter]);
    }
    printf("\n");
    printf("DTypes: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        if (NIT_DTYPES(iter)[iiter] != NULL)
            PyObject_Print((PyObject*)NIT_DTYPES(iter)[iiter], stdout, 0);
        else
            printf("(nil) ");
        printf(" "); 
    }
    printf("\n");
    printf("InitDataPtrs: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_RESETDATAPTR(iter)[iiter]);
    }
    printf("\n");
    if (itflags&NPY_ITFLAG_HASINDEX) {
        printf("InitIndex: %d\n",
                        (int)(npy_intp)NIT_RESETDATAPTR(iter)[niter]);
    }
    printf("Objects: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_OBJECTS(iter)[iiter]);
    }
    printf("\n");
    printf("OpItFlags:\n");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("  Flags[%d]: ", (int)iiter);
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_READ)
            printf("READ ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_WRITE)
            printf("WRITE ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_COPY)
            printf("COPY ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_CAST)
            printf("CAST ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_COPYSWAP)
            printf("COPYSWAP ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_BUFNEVER)
            printf("BUFNEVER ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_ALIGNED)
            printf("ALIGNED ");
        printf("\n");
    }
    printf("\n");

    if (itflags&NPY_ITFLAG_BUFFER) {
        char *bufferdata = NIT_BUFFERDATA(iter);
        printf("BufferData:\n");
        printf("  BufferSize: %d\n", (int)NBF_BUFFERSIZE(bufferdata));
        printf("  Size: %d\n", (int)NBF_SIZE(bufferdata));
        printf("  Pos: %d\n", (int)NBF_POS(bufferdata));
        printf("  Strides: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%d ", (int)NBF_STRIDES(bufferdata)[iiter]);
        printf("\n");
        printf("  Ptrs: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_PTRS(bufferdata)[iiter]);
        printf("\n");
        printf("  ReadTransferFn: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_READTRANSFERFN(bufferdata)[iiter]);
        printf("\n");
        printf("  ReadTransferData: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_READTRANSFERDATA(bufferdata)[iiter]);
        printf("\n");
        printf("  WriteTransferFn: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_WRITETRANSFERFN(bufferdata)[iiter]);
        printf("\n");
        printf("  WriteTransferData: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_WRITETRANSFERDATA(bufferdata)[iiter]);
        printf("\n");
        printf("  Buffers: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_BUFFERS(bufferdata)[iiter]);
        printf("\n");
        printf("\n");
    }

    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        printf("AxisData[%d]:\n", (int)idim);
        printf("  Shape: %d\n", (int)NAD_SHAPE(axisdata));
        printf("  Coord: %d\n", (int)NAD_COORD(axisdata));
        printf("  Strides: ");
        for (iiter = 0; iiter < niter; ++iiter) {
            printf("%d ", (int)NAD_STRIDES(axisdata)[iiter]);
        }
        printf("\n");
        if (itflags&NPY_ITFLAG_HASINDEX) {
            printf("  Index Stride: %d\n", (int)NAD_STRIDES(axisdata)[niter]);
        }
        printf("  Ptrs: ");
        for (iiter = 0; iiter < niter; ++iiter) {
            printf("%p ", NAD_PTRS(axisdata)[iiter]);
        }
        printf("\n");
        if (itflags&NPY_ITFLAG_HASINDEX) {
            printf("  Index Value: %d\n",
                               (int)((npy_intp*)NAD_PTRS(axisdata))[niter]);
        }
    }

    printf("------- END ITERATOR DUMP -------\n");
}

