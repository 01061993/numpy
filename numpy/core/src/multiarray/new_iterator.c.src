#define PY_SSIZE_T_CLEAN
#include "Python.h"
#include "structmember.h"

#define _MULTIARRAYMODULE
#include <numpy/ndarrayobject.h>

#include "new_iterator.h"


/* Internal iterator flags */

/* The perm is the identity */
#define NPY_ITER_FLAGS_IDENTPERM   0x001
/* The perm has >= 1 negative entries (indicating flipped axes) */
#define NPY_ITER_FLAGS_NEGPERM     0x002
/* The iterator is tracking an index */
#define NPY_ITER_FLAGS_HASINDEX    0x004
/* The iterator is tracking coordinates */
#define NPY_ITER_FLAGS_HASCOORDS   0x008
/* The iteration order was forced on construction */
#define NPY_ITER_FLAGS_FORCEDORDER 0x010

/*
 * The data layout of the iterator is fully specified by
 * a triple (itflags, ndim, niter).  These three variables
 * are expected to exist in all functions calling these macros,
 * either as true variables initialized to the correct values
 * from the iterator, or as constants in the case of specialized
 * functions such as the various iternext functions.
 *
 * If this layout changes, the function 'npyiter_shrink_ndim'
 * needs to be changed as well.
 */

/* Size of all the data before the AXISDATA starts */
#define NIT_SIZEOF_BASEDATA(itflags, ndim, niter) ( \
        /* uint32 itflags AND uint16 ndim AND uint16 niter */ \
        8 + \
        /* npy_intp itersize */ \
        NPY_SIZEOF_INTP + \
        /* npy_intp perm[ndim] */ \
        (NPY_SIZEOF_INTP)*(ndim) + \
        /* PyArray_Descr* dtypes[niter] */ \
        /* npy_intp itemsizes[niter] */ \
        2*(NPY_SIZEOF_INTP)*(niter) \
        )

/* Size of one AXISDATA struct within the iterator */
#define NIT_SIZEOF_AXISDATA(itflags, ndim, niter) (( \
        /* intp shape */ \
        1 + \
        /* intp coord */ \
        1 + \
        /* intp stride[niter] AND char* ptr[niter] */ \
        2*(niter) + \
        /* intp indexstride AND intp index (when index is provided) */ \
        ((itflags&NPY_ITER_FLAGS_HASINDEX) ? 2 : 0) \
        )*NPY_SIZEOF_INTP ) \

/* Size of the whole iterator */
#define NIT_SIZEOF_ITERATOR(itflags, ndim, niter) ( \
        NIT_SIZEOF_BASEDATA(itflags, ndim, niter) + \
        NIT_SIZEOF_AXISDATA(itflags, ndim, niter)*(ndim) + \
        NPY_SIZEOF_INTP*(ndim) + \
        NPY_SIZEOF_INTP*(niter))

/* Byte offsets of the iterator members */
#define NIT_ITERSIZE_OFFSET() \
        (8)
#define NIT_PERM_OFFSET() \
        (NIT_ITERSIZE_OFFSET() + \
         (NPY_SIZEOF_INTP))
#define NIT_DTYPES_OFFSET(itflags, ndim, niter) \
        (NIT_PERM_OFFSET() + \
         (NPY_SIZEOF_INTP)*(ndim))
#define NIT_ITEMSIZES_OFFSET(itflags, ndim, niter) \
        (NIT_DTYPES_OFFSET(itflags, ndim, niter) + \
         (NPY_SIZEOF_INTP)*(niter))
#define NIT_OBJECTS_OFFSET(itflags, ndim, niter) \
        (NIT_ITEMSIZES_OFFSET(itflags, ndim, niter) + \
         (NPY_SIZEOF_INTP)*(niter))
#define NIT_AXISDATA_OFFSET(itflags, ndim, niter) \
        (NIT_OBJECTS_OFFSET(itflags, ndim, niter) + \
         (NPY_SIZEOF_INTP)*(niter))

/* Internal-only ITERATOR DATA MEMBER ACCESS */
#define NIT_ITFLAGS(iter) \
        (*((npy_uint32*)(iter)))
#define NIT_NDIM(iter) \
        (*((npy_uint16*)(iter) + 2))
#define NIT_NITER(iter) \
        (*((npy_uint16*)(iter) + 3))
#define NIT_ITERSIZE(iter) (*((npy_intp*)( \
        (char*)(iter) + NIT_ITERSIZE_OFFSET())))
#define NIT_PERM(iter)  ((npy_intp*)( \
        (char*)(iter) + NIT_PERM_OFFSET()))
#define NIT_DTYPES(iter) ((PyArray_Descr**)( \
        (char*)(iter) + NIT_DTYPES_OFFSET(itflags, ndim, niter)))
#define NIT_ITEMSIZES(iter) ((npy_intp*)( \
        (char*)(iter) + NIT_ITEMSIZES_OFFSET(itflags, ndim, niter)))
#define NIT_OBJECTS(iter) ((PyObject**)( \
        (char*)(iter) + NIT_OBJECTS_OFFSET(itflags, ndim, niter)))
#define NIT_AXISDATA(iter) \
        ((char*)(iter) + NIT_AXISDATA_OFFSET(itflags, ndim, niter))

/* Internal-only AXISDATA MEMBER ACCESS. */
#define NAD_SHAPE(axisdata) (*((npy_intp*)(axisdata)))
#define NAD_COORD(axisdata) (*((npy_intp*)(axisdata) + 1))
#define NAD_STRIDES(axisdata) ((npy_intp*)(axisdata) + 2)
#define NAD_PTRS(axisdata) \
        ((char**)(axisdata) + 2 + NAD_NSTRIDES())
#define NAD_NSTRIDES() \
        ((niter) + ((itflags&NPY_ITER_FLAGS_HASINDEX) ? 1 : 0))

/* Internal helper functions */
static void
npyiter_flip_negative_strides(PyArray_NpyIter *iter);
static void 
npyiter_find_best_ordering(PyArray_NpyIter *iter);
static void 
npyiter_coalesce_axes(PyArray_NpyIter *iter);
static void
npyiter_shrink_ndim(PyArray_NpyIter *iter, npy_intp new_ndim);
static void
print_iterator(PyArray_NpyIter *iter);


PyArray_NpyIter*
NpyIter_New(PyObject* op, npy_uint32 flags, PyArray_Descr* dtype,
                  int min_depth, int max_depth)
{
    npy_uint32 itflags = NPY_ITER_FLAGS_IDENTPERM;
    npy_intp ndim;
    npy_intp niter = 1;

    npy_intp idim, indexstride, sizeof_axisdata;
    PyArray_Descr* opdtype;
    PyArray_NpyIter* iter = 0;
    char* axisdata = 0;
    int zerodim = 0;

    /* Currently only work with arrays */
    if (!PyArray_Check(op)) {
        PyErr_SetString(PyExc_ValueError,
                "Can only create an iterator for an array");
        return NULL;
    }

    ndim = PyArray_NDIM(op);
    if ((min_depth >= 0 && ndim < min_depth) ||
            (max_depth >= 0 && ndim > max_depth)) {
        PyErr_Format(PyExc_ValueError,
                "Requested an iterator with %d <= ndim <= %d, "
                "but input object ndim is %d",
                min_depth, max_depth, (int)ndim);
        return NULL;
    }

    /* Get the data type of the array */
    opdtype = PyArray_DESCR(op);
    if (opdtype == NULL) {
        PyErr_SetString(PyExc_ValueError,
                "Input object has no dtype descr");
        return NULL;
    }
    if (dtype != NULL && !PyArray_EquivTypes(opdtype, dtype)) {
        PyErr_SetString(PyExc_ValueError,
                "Don't support automatic dtype conversions yet");
        return NULL;
    }

    /* Process the input flags */
    if (flags&(NPY_ITER_C_ORDER_INDEX | NPY_ITER_F_ORDER_INDEX)) {
        if ((flags&(NPY_ITER_C_ORDER_INDEX | NPY_ITER_F_ORDER_INDEX)) ==
                    (NPY_ITER_C_ORDER_INDEX | NPY_ITER_F_ORDER_INDEX)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flags NPY_ITER_C_ORDER_INDEX and "
                    "NPY_ITER_F_ORDER_INDEX cannot both be specified");
            return NULL;
        }
        itflags |= NPY_ITER_FLAGS_HASINDEX;
    }
    if (flags&NPY_ITER_COORDS) {
        if (ndim == 0) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flag NPY_ITER_COORDS cannot be used "
                    "with a zero-dimensional input");
            return NULL;
        }

        /*
         * This flag primarily disables dimension manipulations that
         * would produce a different set of coordinates.
         */
        itflags |= NPY_ITER_FLAGS_HASCOORDS;
    }

    /*
     * In the zero dimensional case, we need to up it to one
     * dimension to get a data pointer in the AXISDATA structure.
     */
    if (ndim == 0) {
        zerodim = 1;
        ndim = 1;
    }

    /* Allocate memory for the iterator */
    iter = (PyArray_NpyIter*)malloc(NIT_SIZEOF_ITERATOR(itflags, ndim, niter));

    /* Fill in the base data */
    NIT_ITFLAGS(iter) = itflags;
    NIT_NDIM(iter) = ndim;
    NIT_NITER(iter) = niter;
    NIT_ITERSIZE(iter) = 1;
    NIT_DTYPES(iter)[0] = opdtype;
    Py_INCREF(opdtype);
    NIT_ITEMSIZES(iter)[0] = PyArray_ITEMSIZE(op);

    /* Fill in the source object array */
    NIT_OBJECTS(iter)[0] = op;
    Py_INCREF(op);

    /* In the zero dimensional case, fill things in manually */
    if (zerodim) {
        axisdata = NIT_AXISDATA(iter);
        NAD_SHAPE(axisdata) = 1;
        NAD_COORD(axisdata) = 0;
        NAD_STRIDES(axisdata)[0] = 0;
        NAD_PTRS(axisdata)[0] = PyArray_DATA(op);
        if (itflags&NPY_ITER_FLAGS_HASINDEX) {
            NAD_STRIDES(axisdata)[1] = 0;
            NAD_PTRS(axisdata)[1] = 0;
        }
        print_iterator(iter);
        return iter;
    }

    /* Fill in the axis data (in reverse of C order)*/
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    axisdata = NIT_AXISDATA(iter);
    for(idim = ndim-1; idim >= 0; --idim, axisdata += sizeof_axisdata) {
        npy_intp shape = PyArray_DIM(op, idim);

        NIT_ITERSIZE(iter) *= shape;
        NAD_SHAPE(axisdata) = shape;
        NAD_COORD(axisdata) = 0;
        /* Make the stride 0 for dimensions with a shape of 1 */
        if (shape == 1) {
            NAD_STRIDES(axisdata)[0] = 0;
        } else {
            NAD_STRIDES(axisdata)[0] = PyArray_STRIDE(op, idim);
        }
        NAD_PTRS(axisdata)[0] = PyArray_DATA(op);
    }

    /* If an index was requested, compute the strides for it */
    if (flags&NPY_ITER_C_ORDER_INDEX) {
        indexstride = 1;
        axisdata = NIT_AXISDATA(iter);
        for(idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
            NAD_STRIDES(axisdata)[1] = indexstride;
            NAD_PTRS(axisdata)[1] = 0;
            indexstride *= NAD_SHAPE(axisdata);
        }
    } else if (flags&NPY_ITER_F_ORDER_INDEX) {
        indexstride = 1;
        axisdata = NIT_AXISDATA(iter) + (ndim-1)*sizeof_axisdata;
        for(idim = 0; idim < ndim; ++idim, axisdata -= sizeof_axisdata) {
            NAD_STRIDES(axisdata)[1] = indexstride;
            NAD_PTRS(axisdata)[1] = 0;
            indexstride *= NAD_SHAPE(axisdata);
        }
    }

    /*
     * If an iteration order is being forced, apply it, otherwise
     * compute the most cache-friendly order.
     */
    if (flags&(NPY_ITER_FORCE_F_ORDER|NPY_ITER_FORCE_ANY_CONTIGUOUS)) {
        if ((flags&NPY_ITER_FORCE_F_ORDER) ||
                ((flags&NPY_ITER_FORCE_ANY_CONTIGUOUS) && 
                 PyArray_CHKFLAGS(op, NPY_F_CONTIGUOUS))) {
            npy_intp i, temp, size = sizeof_axisdata/NPY_SIZEOF_INTP;
            npy_intp* first = (npy_intp*)NIT_AXISDATA(iter);
            npy_intp* last = first + (ndim-1)*size;
            
            /* This loop reverses the order of the AXISDATA array */
            while (first < last) {
                for(i = 0; i < size; ++i) {
                    temp = first[i];
                    first[i] = last[i];
                    last[i] = temp;
                }
                first += size;
                last -= size;
            }

            /* Store the perm we applied */
            first = NIT_PERM(iter);
            for(i = ndim-1; i >= 0; --i, ++first) {
                *first = i;
            }
            itflags &= ~NPY_ITER_FLAGS_IDENTPERM;
        }
        itflags |= NPY_ITER_FLAGS_FORCEDORDER;
        NIT_ITFLAGS(iter) = itflags;
    } else if (flags&NPY_ITER_FORCE_C_ORDER) {
        itflags |= NPY_ITER_FLAGS_FORCEDORDER;
        NIT_ITFLAGS(iter) = itflags;
    } else {
        /*
         * If the ordering is not forced, reorder the axes
         * and flip negative strides to find the best one.
         */
        npyiter_find_best_ordering(iter);
        npyiter_flip_negative_strides(iter);
        itflags = NIT_ITFLAGS(iter);
    }

    /* Store the identity perm if the ident flag is set */
    if (itflags&NPY_ITER_FLAGS_IDENTPERM) {
        npy_intp *perm = NIT_PERM(iter);

        for(idim = 0; idim < ndim; ++idim, ++perm) {
            *perm = idim;
        }
    }

    /*
     * As the last thing to do, if coords aren't required,
     * it may be possible to coalesce some axes together.
     */
    if (!(itflags&NPY_ITER_FLAGS_HASCOORDS)) {
        npyiter_coalesce_axes(iter);
    }

    print_iterator(iter);
    return iter;
}

int NpyIter_Deallocate(PyArray_NpyIter *iter)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp i;
    PyArray_Descr **dtypes = NIT_DTYPES(iter);
    PyObject **arrays = NIT_OBJECTS(iter);

    /* Deallocate all the dtypes and objects that were iterated */
    for(i = 0; i < niter; ++i) {
        Py_XDECREF(dtypes[i]);
        Py_XDECREF(arrays[i]);
    }

    /* Deallocate the iterator memory */
    free(iter);

    return NPY_SUCCEED;
}


/* SPECIALIZED iternext functions */

/**begin repeat
 * #const_itflags = 0, NPY_ITER_FLAGS_HASINDEX#
 * #tag_itflags = 0, IND#
 */
/**begin repeat1
 * #const_ndims = 1, 2, 100#
 * #tag_ndims = 1, 2, ANY#
 */
/**begin repeat2
 * #const_niter = 1, 2, 100#
 * #tag_niter = 1, 2, ANY#
 */

/* Specialized iternext (@const_itflags@,@const_ndims@,@const_niter@) */
NPY_NO_EXPORT int
npyiter_iternext_itflags@tag_itflags@_dims@tag_ndims@_iters@tag_niter@(
                                                      PyArray_NpyIter *iter)
{
    const npy_uint32 itflags = @const_itflags@;
#if @const_ndims@ < 100
    const npy_intp ndim = @const_ndims@;
#else
    npy_intp idim, ndim = NIT_NDIM(iter);
#endif
#if @const_ndims@ < 100
    const npy_intp niter = @const_niter@;
#else
    npy_intp niter = NIT_NITER(iter);
#endif

    npy_intp istrides, nstrides, sizeof_axisdata;
#if @const_ndims@ > 0
    char* axisdata0;
#endif
#if @const_ndims@ > 1
    char* axisdata1;
#endif
#if @const_ndims@ > 2
    char* axisdata2;
#endif

    nstrides = NAD_NSTRIDES();
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    axisdata0 = NIT_AXISDATA(iter);
    /* Increment coordinate 0 */
    NAD_COORD(axisdata0)++;
    /* Increment pointer 0 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata0)[istrides] += NAD_STRIDES(axisdata0)[istrides];
    }

#if @const_ndims@ == 1
    /* Finished when the coordinate equals the shape */
    return NAD_COORD(axisdata0) < NAD_SHAPE(axisdata0);
# else

    if (NAD_COORD(axisdata0) < NAD_SHAPE(axisdata0)) {
        return 1;
    }

    axisdata1 = axisdata0 + sizeof_axisdata;
    /* Increment coordinate 1 */
    NAD_COORD(axisdata1)++;
    /* Increment pointer 1 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata1)[istrides] += NAD_STRIDES(axisdata1)[istrides];
    }

    if (NAD_COORD(axisdata1) < NAD_SHAPE(axisdata1)) {
        /* Reset the 1st coordinate to 0 */
        NAD_COORD(axisdata0) = 0;
        /* Reset the 1st pointer to the value of the 2nd */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata0)[istrides] = NAD_PTRS(axisdata1)[istrides];
        }
        return 1;
    }

# if @const_ndims@ == 2
    return 0;
# else
    
    axisdata2 = axisdata1 + sizeof_axisdata;
    /* Increment coordinate 2 */
    NAD_COORD(axisdata2)++;
    /* Increment pointer 2 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata2)[istrides] += NAD_STRIDES(axisdata2)[istrides];
    }

    if (NAD_COORD(axisdata2) < NAD_SHAPE(axisdata2)) {
        /* Reset the 1st and 2nd coordinates to 0 */
        NAD_COORD(axisdata0) = 0;
        NAD_COORD(axisdata1) = 0;
        /* Reset the 1st and second pointers to the value of the 3nd */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata0)[istrides] = NAD_PTRS(axisdata2)[istrides];
            NAD_PTRS(axisdata1)[istrides] = NAD_PTRS(axisdata2)[istrides];
        }
        return 1;
    }

    for (idim = 3; idim < ndim; ++idim) {
        axisdata2 += sizeof_axisdata;
        /* Increment the coordinate */
        NAD_COORD(axisdata2)++;
        /* Increment the pointer */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata2)[istrides] += NAD_STRIDES(axisdata2)[istrides];
        }


        if (NAD_COORD(axisdata2) < NAD_SHAPE(axisdata2)) {
            /* Reset the coordinates and pointers of all previous axisdatas */
            axisdata1 = axisdata2;
            do {
                axisdata1 -= sizeof_axisdata;
                /* Reset the coordinate to 0 */
                NAD_COORD(axisdata1) = 0;
                /* Reset the pointer to the updated value */
                for (istrides = 0; istrides < nstrides; ++istrides) {
                    NAD_PTRS(axisdata1)[istrides] =
                                        NAD_PTRS(axisdata2)[istrides];
                }
            } while (axisdata1 != axisdata0);

            return 1;
        }
    }

    return 0;

# endif /* ndims != 2 */
    
#endif /* ndims != 1 */
}

/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/* Specialization of iternext for when the iteration size is 1 */
NPY_NO_EXPORT int
npyiter_iternext_sizeone(PyArray_NpyIter *iter)
{
    return 0;
}

/* Returns a specialized iternext function */
NpyIter_IterNext_Fn NpyIter_GetIterNext(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    /*
     * When there is just one element being iterated,
     * the iternext function is very simple
     */
    if (NIT_ITERSIZE(iter) == 1) {
        return &npyiter_iternext_sizeone;
    }

    /*
     * Ignore all the flags that don't affect the iterator memory
     * layout or the iternext function.  Currently only HASINDEX
     * affects them.
     */
    itflags &= NPY_ITER_FLAGS_HASINDEX;

    /* Switch statements let the compiler optimize this most effectively */
    switch (itflags) {
/**begin repeat
 * #const_itflags = 0, NPY_ITER_FLAGS_HASINDEX#
 * #tag_itflags = 0, IND#
 */
        case @const_itflags@:
            switch (ndim) {
/**begin repeat1
 * #const_ndims = 1, 2#
 * #tag_ndims = 1, 2#
 */
                case @const_ndims@:
                    switch (niter) {
/**begin repeat2
 * #const_niter = 1, 2#
 * #tag_niter = 1, 2#
 */
                        case @const_niter@:
                            return &npyiter_iternext_itflags@tag_itflags@_dims@tag_ndims@_iters@tag_niter@;
/**end repeat2**/
                        /* Not specialized on niter */
                        default:
                            return &npyiter_iternext_itflags@tag_itflags@_dims@tag_ndims@_itersANY;
                    }
/**end repeat1**/
                /* Not specialized on ndim */
                default:
                    switch (niter) {
/**begin repeat1
 * #const_niter = 1, 2#
 * #tag_niter = 1, 2#
 */
                        case @const_niter@:
                            return &npyiter_iternext_itflags@tag_itflags@_dimsANY_iters@tag_niter@;
/**end repeat1**/
                        /* Not specialized on niter */
                        default:
                            return &npyiter_iternext_itflags@tag_itflags@_dimsANY_itersANY;
                    }
            }
/**end repeat**/
    }
    /* The switch above should have caught all the possibilities. */
    PyErr_Format(PyExc_ValueError,
            "Internal iterator error - unexpected itflags/ndim/niter "
            " combination (%04x/%d/%d)", (int)itflags, (int)ndim, (int)niter);
    return NULL;
}


/* SPECIALIZED getcoord functions */

/**begin repeat
 * #const_itflags = 0,
 *    NPY_ITER_FLAGS_HASINDEX,
 *    NPY_ITER_FLAGS_IDENTPERM,
 *    NPY_ITER_FLAGS_HASINDEX|NPY_ITER_FLAGS_IDENTPERM,
 *    NPY_ITER_FLAGS_NEGPERM,
 *    NPY_ITER_FLAGS_HASINDEX|NPY_ITER_FLAGS_NEGPERM#
 * #tag_itflags = 0, IND, IDP, INDuIDP, NEGP, INDuNEGP#
 */
NPY_NO_EXPORT void
npyiter_getcoord_itflags@tag_itflags@(PyArray_NpyIter *iter, npy_intp *outcoord)
{
    const npy_uint32 itflags = @const_itflags@;
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp idim, sizeof_axisdata;
    char* axisdata;
#if !((@const_itflags@)&NPY_ITER_FLAGS_IDENTPERM)
    npy_intp* perm = NIT_PERM(iter);
#endif

    outcoord += ndim-1;
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
#if ((@const_itflags@)&NPY_ITER_FLAGS_IDENTPERM)
    for(idim = 0; idim < ndim; ++idim, --outcoord,
                                    axisdata += sizeof_axisdata) {
        *outcoord = NAD_COORD(axisdata);
    }
#elif !((@const_itflags@)&NPY_ITER_FLAGS_NEGPERM)
    for(idim = 0; idim < ndim; ++idim, --outcoord) {
        *outcoord = NAD_COORD(axisdata + sizeof_axisdata*perm[idim]);
    }
#else
    for(idim = 0; idim < ndim; ++idim, --outcoord) {
        npy_intp p = perm[idim];
        if (p < 0) {
            /* If the perm entry is negative, reverse the coordinate */
            char *ad = axisdata + sizeof_axisdata*(-p-1);
            *outcoord = NAD_SHAPE(ad) - NAD_COORD(ad) - 1;

        } else {
            *outcoord = NAD_COORD(axisdata + sizeof_axisdata*p);
        }
    }
#endif /* not ident perm */
}
/**end repeat**/

/* Returns a specialized getcoord function */
NpyIter_GetCoords_Fn NpyIter_GetGetCoords(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);

    if (!(itflags&NPY_ITER_FLAGS_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot get a GetCoords function for an iterator "
                "that doesn't track coordinates.");
        return NULL;
    }

    /*
     * Only these flags affect the iterator memory layout or
     * the getcoords behavior. IDENTPERM and NEGPERM are mutually
     * exclusive, so that reduces the number of cases slightly.
     */
    itflags &= (NPY_ITER_FLAGS_HASINDEX |
                NPY_ITER_FLAGS_IDENTPERM |
                NPY_ITER_FLAGS_NEGPERM);
    
    switch (itflags) {
/**begin repeat
 * #const_itflags = 0,
 *    NPY_ITER_FLAGS_HASINDEX,
 *    NPY_ITER_FLAGS_IDENTPERM,
 *    NPY_ITER_FLAGS_HASINDEX|NPY_ITER_FLAGS_IDENTPERM,
 *    NPY_ITER_FLAGS_NEGPERM,
 *    NPY_ITER_FLAGS_HASINDEX|NPY_ITER_FLAGS_NEGPERM#
 * #tag_itflags = 0, IND, IDP, INDuIDP, NEGP, INDuNEGP#
 */
        case @const_itflags@:
            return npyiter_getcoord_itflags@tag_itflags@;
/**end repeat**/
    }
    return NULL; /* TODO: return generic version */

}

npy_intp NpyIter_GetNDim(PyArray_NpyIter *iter)
{
    return NIT_NDIM(iter);
}

char **NpyIter_GetDataPtrArray(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char* axisdata = NIT_AXISDATA(iter);

    return NAD_PTRS(axisdata);
}

npy_intp *NpyIter_GetIndexPtr(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char* axisdata = NIT_AXISDATA(iter);

    if (itflags&NPY_ITER_FLAGS_HASINDEX) {
        /* The index is just after the data pointers */
        return (npy_intp*)NAD_PTRS(axisdata) + niter;
    } else {
        return NULL;
    }
}


npy_intp *NpyIter_GetItemSizeArray(PyArray_NpyIter *iter)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);
    
    return NIT_ITEMSIZES(iter);
}

/*
 * This function negates any strides in the iterator
 * which are negative.  When iterating more than one
 * object, it only flips strides when they are all
 * negative or zero.
 */
static void
npyiter_flip_negative_strides(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp istrides, nstrides = NAD_NSTRIDES();
    char *axisdata, *axisdata0;
    npy_intp *ptrs0;
    npy_intp sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    int any_flipped = 0;

    axisdata0 = axisdata = NIT_AXISDATA(iter);
    ptrs0 = (npy_intp*)NAD_PTRS(axisdata0);
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        npy_intp *strides = NAD_STRIDES(axisdata);
        int any_negative = 0;

        /*
         * Check the signs of all the strides, excluding
         * the index stride at the end.
         */
        for (iiter = 0; iiter < niter; ++iiter) {
            if (strides[iiter] < 0) {
                any_negative = 1;
            } else if (strides[iiter] != 0) {
                break;
            }
        }
        /*
         * If at least on stride is negative and none are positive,
         * flip all the strides for this dimension.
         */
        if (any_negative && iiter == niter) {
            npy_intp shapem1 = NAD_SHAPE(axisdata) - 1;

            for (istrides = 0; istrides < nstrides; ++istrides) {
                npy_intp stride = strides[istrides];

                /* Adjust the base pointers to start at the end */
                ptrs0[istrides] += shapem1 * stride;
                /* Flip the stride */
                strides[istrides] = -stride;
            }
            /* Make the perm entry negative, so getcoords knows it's  flipped */
            NIT_PERM(iter)[idim] = -1-NIT_PERM(iter)[idim];

            any_flipped = 1;
        }
    }

    /*
     * If any strides were flipped, the base pointers were adjusted
     * in the first AXISDATA, and need to be copied to all the rest
     */
    if (any_flipped) {
        axisdata = axisdata0;
        for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
            npy_intp *ptrs = (npy_intp*)NAD_PTRS(axisdata);
            for (istrides = 0; istrides < nstrides; ++istrides) {
                ptrs[istrides] = ptrs0[istrides];
            }
        }
        /*
         * Indicate that some of the perm entries are negative,
         * and that it's not (strictly speaking) the identity perm.
         */
        NIT_ITFLAGS(iter) = (NIT_ITFLAGS(iter)|NPY_ITER_FLAGS_NEGPERM) &
                            ~NPY_ITER_FLAGS_IDENTPERM;
    }
}

static npy_intp intp_abs(npy_intp x)
{
    return (x < 0) ? -x : x;
}

static void 
npyiter_find_best_ordering(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp i0, i1, ipos, j0, j1;
    npy_intp *perm = NIT_PERM(iter);
    char *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    int permuted = 0;
    
    /* Initialize the perm to the identity */
    for(idim = 0; idim < ndim; ++idim) {
        perm[idim] = idim;
    }

    /*
     * Do a custom stable insertion sort.  Note that because
     * the AXISDATA has been reversed from C order, this
     * is sorting from smallest stride to biggest stride.
     */
    for (i0 = 1; i0 < ndim; ++i0) {
        npy_intp *strides0;

        /* 'ipos' is where perm[i0] will get inserted */
        ipos = i0;
        j0 = perm[i0];
        strides0 = NAD_STRIDES(axisdata + j0*sizeof_axisdata);
        for (i1 = i0-1; i1 >= 0; --i1) {
            int ambig = 1, shouldswap = 0;
            npy_intp *strides1;

            j1 = perm[i1];
            strides1 = NAD_STRIDES(axisdata + j1*sizeof_axisdata);

            for (iiter = 0; iiter < niter; ++iiter) {
                if (strides0[iiter] != 0 && strides1[iiter] != 0) {
                    if (intp_abs(strides1[iiter]) <=
                                            intp_abs(strides0[iiter])) {
                        /*
                         * Set swap even if it's not ambiguous already,
                         * because in the case of conflicts between
                         * different operands, C-order wins.
                         */
                        shouldswap = 0;
                    } else {
                        /* Only set swap if it's still ambiguous */
                        if (ambig) {
                            shouldswap = 1;
                        }
                    }
                    
                    /*
                     * A comparison has been done, so it's
                     * no longer ambiguous
                     */
                    ambig = 0;
                }
            }
            /*
             * If the comparison was unambiguous, either shift
             * 'ipos' to 'i1' or stop looking for an insertion
             * point
             */
            if (!ambig) {
                if (shouldswap) {
                    ipos = i1;
                } else {
                    break;
                }
            }
        }

        /* Insert perm[i0] into the right place */
        if (ipos != i0) {
            for (i1 = i0; i1 > ipos; --i1) {
                perm[i1] = perm[i1-1];
            }
            perm[ipos] = j0;
            permuted = 1;
        }
    }

    /* Apply the computed permutation to the AXISDATA array */
    if (permuted == 1) {
        npy_intp i, size = sizeof_axisdata/NPY_SIZEOF_INTP;
        char *ad_i;

        /* Use the coord as a flag, set each to 1 */
        for (idim = 0; idim < ndim; ++idim) {
            NAD_COORD(axisdata + idim*sizeof_axisdata) = 1;
        }
        /* Apply the permutation by following the cycles */
        for (idim = 0; idim < ndim; ++idim) {
            ad_i = axisdata + idim*sizeof_axisdata;

            /* If this axis hasn't been touched yet, process it */
            if (NAD_COORD(ad_i) == 1) {
                npy_intp pidim = perm[idim], qidim, tmp;
                char *ad_p, *ad_q;

                if (pidim != idim) {
                    /* Follow the cycle, copying the data */
                    for (i = 0; i < size; ++i) {
                        qidim = idim;
                        pidim = perm[idim];
                        ad_q = ad_i;
                        tmp = *((npy_intp*)ad_q + i);
                        while (pidim != idim) {
                            ad_p = axisdata + pidim*sizeof_axisdata;
                            *((npy_intp*)ad_q + i) = *((npy_intp*)ad_p + i);

                            qidim = pidim;
                            ad_q = ad_p;
                            pidim = perm[pidim];
                        }
                        *((npy_intp*)ad_q + i) = tmp;
                    }
                    /* Follow the cycle again, marking it as done */
                    pidim = perm[idim];
                    while (pidim != idim) {
                        NAD_COORD(axisdata + pidim*sizeof_axisdata) = 0;
                        pidim = perm[pidim];
                    }
                }
                NAD_COORD(ad_i) = 0;
            }
        }
        /* Clear the identity perm flag */
        NIT_ITFLAGS(iter) &= ~NPY_ITER_FLAGS_IDENTPERM;
    }
}

static void 
npyiter_coalesce_axes(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    int any_coalesced = 0;
    npy_intp istrides, nstrides = NAD_NSTRIDES();
    char *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);

    /* The HASCOORDS or IDENTPERM flags do not apply after coalescing */
    NIT_ITFLAGS(iter) &= ~(NPY_ITER_FLAGS_IDENTPERM|NPY_ITER_FLAGS_HASCOORDS);

    /*
     * This uses the COORD entry in AXISDATA as the can_coalesce
     * array.  Because this is creation time, these are already
     * set to all zeros.
     */

    for (idim = 0; idim < ndim-1; ++idim, axisdata += sizeof_axisdata) {
        int can_coalesce = 1;
        npy_intp shape0 = NAD_SHAPE(axisdata);
        npy_intp *strides0 = NAD_STRIDES(axisdata);
        npy_intp *strides1 = NAD_STRIDES(axisdata + sizeof_axisdata);

        /*
         * If the strides for all the inputs and index can be coalesced,
         * mark this axis.
         */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            if (strides0[istrides]*shape0 != strides1[istrides]) {
                can_coalesce = 0;
                break;
            }
        }
        if (can_coalesce) {
            NAD_COORD(axisdata) = 1;
            any_coalesced = 1;
        }
    }

    /*
     * If we found any axes to coalesce, go through and coalesce
     * the AXISDATA array.
     */
    if (any_coalesced) {
        char *ad_compress;
        npy_intp new_ndim = 1;

        axisdata = NIT_AXISDATA(iter);
        ad_compress = axisdata;

        for (idim = 0; idim < ndim-1; ++idim) {
            if (NAD_COORD(axisdata)) {
                axisdata += sizeof_axisdata;
                NAD_SHAPE(ad_compress) *= NAD_SHAPE(axisdata);
            } else {
                NAD_COORD(ad_compress) = 0;
                axisdata += sizeof_axisdata;
                ad_compress += sizeof_axisdata;
                if (ad_compress != axisdata) {
                    memcpy(ad_compress, axisdata, sizeof_axisdata);
                }
                ++new_ndim;
            }
        }

        /* Might as well reset it to an identity perm, not that it matters */
        for (idim = 0; idim < new_ndim; ++idim) {
            NIT_PERM(iter)[idim] = idim;
        }
        
        /*
         * Because changing ndim affects the memory layout, we need
         * to compress the data into the new layout.
         */
        npyiter_shrink_ndim(iter, new_ndim);
    }
}

static void
npyiter_shrink_ndim(PyArray_NpyIter *iter, npy_intp new_ndim)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char *iterdata = (char*)iter;

    printf("shrinking %d to %d\n", (int)ndim, (int)new_ndim);

    print_iterator(iter);

    /*
     * The only place that will shift is perm[ndim], so only one
     * memmove is needed for the data after that.
     */
    memmove(iterdata + NIT_DTYPES_OFFSET(itflags, new_ndim, niter),
            iterdata + NIT_DTYPES_OFFSET(itflags, ndim, niter),
            NIT_SIZEOF_ITERATOR(itflags, new_ndim, niter) -
            NIT_DTYPES_OFFSET(itflags, new_ndim, niter));
    
    NIT_NDIM(iter) = new_ndim;

    print_iterator(iter);
}

/* For debugging */
static void
print_iterator(PyArray_NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *axisdata;
    npy_intp sizeof_axisdata;

    printf("\n------ BEGIN ITERATOR DUMP ------\n");
    printf(" Flags: ");
    if (itflags&NPY_ITER_FLAGS_IDENTPERM)
        printf("IDENTPERM ");
    if (itflags&NPY_ITER_FLAGS_NEGPERM)
        printf("NEGPERM ");
    if (itflags&NPY_ITER_FLAGS_HASINDEX)
        printf("HASINDEX ");
    if (itflags&NPY_ITER_FLAGS_HASCOORDS)
        printf("HASCOORDS ");
    if (itflags&NPY_ITER_FLAGS_FORCEDORDER)
        printf("FORCEDORDER ");
    printf("\n");
    printf("NDim: %d\n", (int)ndim);
    printf("NIter: %d\n", (int)niter);
    printf("IterSize: %d\n", (int)NIT_ITERSIZE(iter));
    printf("Iterator SizeOf: %d\n",
                            (int)NIT_SIZEOF_ITERATOR(itflags, ndim, niter));
    printf("AxisData SizeOf: %d\n",
                            (int)NIT_SIZEOF_AXISDATA(itflags, ndim, niter));
    printf("\n");

    printf("Perm: ");
    for (idim = 0; idim < ndim; ++idim) {
        printf("%d ", (int)NIT_PERM(iter)[idim]);
    }
    printf("\n");
    printf("DTypes: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_DTYPES(iter)[iiter]);
    }
    printf("\n");
    printf("ItemSizes: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%d ", (int)NIT_ITEMSIZES(iter)[iiter]);
    }
    printf("\n");
    printf("Objects: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_OBJECTS(iter)[iiter]);
    }
    printf("\n");
    printf("\n");

    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_SIZEOF_AXISDATA(itflags, ndim, niter);
    for (idim = 0; idim < ndim; ++idim, axisdata += sizeof_axisdata) {
        printf("AxisData[%d]:\n", (int)idim);
        printf("  Shape: %d\n", (int)NAD_SHAPE(axisdata));
        printf("  Coord: %d\n", (int)NAD_COORD(axisdata));
        printf("  Strides: ");
        for (iiter = 0; iiter < niter; ++iiter) {
            printf("%d ", (int)NAD_STRIDES(axisdata)[iiter]);
        }
        printf("\n");
        if (itflags&NPY_ITER_FLAGS_HASINDEX) {
            printf("  Index Stride: %d\n", (int)NAD_STRIDES(axisdata)[niter]);
        }
        printf("  Ptrs: ");
        for (iiter = 0; iiter < niter; ++iiter) {
            printf("%p ", NAD_PTRS(axisdata)[iiter]);
        }
        printf("\n");
        if (itflags&NPY_ITER_FLAGS_HASINDEX) {
            printf("  Index Value: %d\n",
                                    (int)((npy_intp*)NAD_PTRS(axisdata))[niter]);
        }
    }

    printf("------- END ITERATOR DUMP -------\n");
}
