#define PY_SSIZE_T_CLEAN
#include "Python.h"
#include "structmember.h"

#define _MULTIARRAYMODULE
#include <numpy/ndarrayobject.h>

#include "lowlevel_strided_loops.h"

/* Rounds up a number of bytes to be divisible by sizeof intp */
#if NPY_SIZEOF_INTP == 4
#define NPY_INTP_ALIGNED(size) ((size + 0x3)&(-0x4))
#else
#define NPY_INTP_ALIGNED(size) ((size + 0x7)&(-0x8))
#endif

/* Internal iterator flags */

/* The perm is the identity */
#define NPY_ITFLAG_IDENTPERM    0x001
/* The perm has negative entries (indicating flipped axes) */
#define NPY_ITFLAG_NEGPERM      0x002
/* The iterator is tracking an index */
#define NPY_ITFLAG_HASINDEX     0x004
/* The iterator is tracking coordinates */
#define NPY_ITFLAG_HASCOORDS    0x008
/* The iteration order was forced on construction */
#define NPY_ITFLAG_FORCEDORDER  0x010
/* The inner loop is handled outside the iterator */
#define NPY_ITFLAG_NOINNER      0x020
/* The iterator is ranged */
#define NPY_ITFLAG_RANGE        0x040
/* The iterator is buffered */
#define NPY_ITFLAG_BUFFER       0x080
/* The iterator should grow the buffered inner loop when possible */
#define NPY_ITFLAG_GROWINNER    0x100
/* There is just one iteration, can specialize iternext for that */
#define NPY_ITFLAG_ONEITERATION 0x200
/* Delay buffer allocation until first Reset* call */
#define NPY_ITFLAG_DELAYBUF     0x400

/* Internal iterator per-operand iterator flags */

/* The operand will be written to */
#define NPY_OP_ITFLAG_WRITE        0x01
/* The operand will be read from */
#define NPY_OP_ITFLAG_READ         0x02
/* The operand may have a temporary copy made */
#define NPY_OP_ITFLAG_COPY         0x04
/* The operand needs type conversion/byte swapping/alignment */
#define NPY_OP_ITFLAG_CAST         0x08
/* The operand never needs buffering */
#define NPY_OP_ITFLAG_BUFNEVER     0x10
/* The operand is aligned */
#define NPY_OP_ITFLAG_ALIGNED      0x20

/*
 * The data layout of the iterator is fully specified by
 * a triple (itflags, ndim, niter).  These three variables
 * are expected to exist in all functions calling these macros,
 * either as true variables initialized to the correct values
 * from the iterator, or as constants in the case of specialized
 * functions such as the various iternext functions.
 *
 * If this layout changes, the function 'npyiter_shrink_ndim'
 * needs to be changed as well.
 */

struct NpyIter_InternalOnly {
    /* Initial fixed position data */
    npy_uint32 itflags;
    npy_uint16 ndim, niter;
    npy_intp itersize, iterstart, iterend;
    /* iterindex is only used if RANGED or BUFFERED was set */
    npy_intp iterindex;
    /* The rest is variable */
    char iter_flexdata;
};

typedef struct NpyIter_AD NpyIter_AxisData;
typedef struct NpyIter_BD NpyIter_BufferData;

/* Byte sizes of the iterator members */
#define NIT_PERM_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(ndim))
#define NIT_DTYPES_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(niter))
#define NIT_RESETDATAPTR_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(niter+1))
#define NIT_BASEOFFSETS_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(niter+1))
#define NIT_OBJECTS_SIZEOF(itflags, ndim, niter) \
        ((NPY_SIZEOF_INTP)*(niter))
#define NIT_OPITFLAGS_SIZEOF(itflags, ndim, niter) \
        (NPY_INTP_ALIGNED(niter))
#define NIT_BUFFERDATA_SIZEOF(itflags, ndim, niter) \
        ((itflags&NPY_ITFLAG_BUFFER) ? ((NPY_SIZEOF_INTP)*(3 + 7*niter)) : 0)

/* Byte offsets of the iterator members starting from iter->iter_flexdata */
#define NIT_PERM_OFFSET() \
        (0)
#define NIT_DTYPES_OFFSET(itflags, ndim, niter) \
        (NIT_PERM_OFFSET() + \
         NIT_PERM_SIZEOF(itflags, ndim, niter))
#define NIT_RESETDATAPTR_OFFSET(itflags, ndim, niter) \
        (NIT_DTYPES_OFFSET(itflags, ndim, niter) + \
         NIT_DTYPES_SIZEOF(itflags, ndim, niter))
#define NIT_BASEOFFSETS_OFFSET(itflags, ndim, niter) \
        (NIT_RESETDATAPTR_OFFSET(itflags, ndim, niter) + \
         NIT_RESETDATAPTR_SIZEOF(itflags, ndim, niter))
#define NIT_OBJECTS_OFFSET(itflags, ndim, niter) \
        (NIT_BASEOFFSETS_OFFSET(itflags, ndim, niter) + \
         NIT_BASEOFFSETS_SIZEOF(itflags, ndim, niter))
#define NIT_OPITFLAGS_OFFSET(itflags, ndim, niter) \
        (NIT_OBJECTS_OFFSET(itflags, ndim, niter) + \
         NIT_OBJECTS_SIZEOF(itflags, ndim, niter))
#define NIT_BUFFERDATA_OFFSET(itflags, ndim, niter) \
        (NIT_OPITFLAGS_OFFSET(itflags, ndim, niter) + \
         NIT_OPITFLAGS_SIZEOF(itflags, ndim, niter))
#define NIT_AXISDATA_OFFSET(itflags, ndim, niter) \
        (NIT_BUFFERDATA_OFFSET(itflags, ndim, niter) + \
         NIT_BUFFERDATA_SIZEOF(itflags, ndim, niter))

/* Internal-only ITERATOR DATA MEMBER ACCESS */
#define NIT_ITFLAGS(iter) \
        ((iter)->itflags)
#define NIT_NDIM(iter) \
        ((iter)->ndim)
#define NIT_NITER(iter) \
        ((iter)->niter)
#define NIT_ITERSIZE(iter) \
        (iter->itersize)
#define NIT_ITERSTART(iter) \
        (iter->iterstart)
#define NIT_ITEREND(iter) \
        (iter->iterend)
#define NIT_ITERINDEX(iter) \
        (iter->iterindex)
#define NIT_PERM(iter)  ((npy_intp*)( \
        &(iter)->iter_flexdata + NIT_PERM_OFFSET()))
#define NIT_DTYPES(iter) ((PyArray_Descr **)( \
        &(iter)->iter_flexdata + NIT_DTYPES_OFFSET(itflags, ndim, niter)))
#define NIT_RESETDATAPTR(iter) ((char **)( \
        &(iter)->iter_flexdata + NIT_RESETDATAPTR_OFFSET(itflags, ndim, niter)))
#define NIT_BASEOFFSETS(iter) ((npy_intp *)( \
        &(iter)->iter_flexdata + NIT_BASEOFFSETS_OFFSET(itflags, ndim, niter)))
#define NIT_OBJECTS(iter) ((PyArrayObject **)( \
        &(iter)->iter_flexdata + NIT_OBJECTS_OFFSET(itflags, ndim, niter)))
#define NIT_OPITFLAGS(iter) ( \
        &(iter)->iter_flexdata + NIT_OPITFLAGS_OFFSET(itflags, ndim, niter))
#define NIT_BUFFERDATA(iter) ((NpyIter_BufferData *)( \
        &(iter)->iter_flexdata + NIT_BUFFERDATA_OFFSET(itflags, ndim, niter)))
#define NIT_AXISDATA(iter) ((NpyIter_AxisData *)( \
        &(iter)->iter_flexdata + NIT_AXISDATA_OFFSET(itflags, ndim, niter)))

/* Internal-only BUFFERDATA MEMBER ACCESS */
struct NpyIter_BD {
    npy_intp buffersize, size, bufiterend;
    npy_intp bd_flexdata;
};
#define NBF_BUFFERSIZE(bufferdata) ((bufferdata)->buffersize)
#define NBF_SIZE(bufferdata) ((bufferdata)->size)
#define NBF_BUFITEREND(bufferdata) ((bufferdata)->bufiterend)
#define NBF_STRIDES(bufferdata) ( \
        &(bufferdata)->bd_flexdata + 0)
#define NBF_PTRS(bufferdata) ((char **) \
        (&(bufferdata)->bd_flexdata + 1*(niter)))
#define NBF_READTRANSFERFN(bufferdata) ((PyArray_StridedTransferFn *) \
        (&(bufferdata)->bd_flexdata + 2*(niter)))
#define NBF_READTRANSFERDATA(bufferdata) ((void **) \
        (&(bufferdata)->bd_flexdata + 3*(niter)))
#define NBF_WRITETRANSFERFN(bufferdata) ((PyArray_StridedTransferFn *) \
        (&(bufferdata)->bd_flexdata + 4*(niter)))
#define NBF_WRITETRANSFERDATA(bufferdata) ((void **) \
        (&(bufferdata)->bd_flexdata + 5*(niter)))
#define NBF_BUFFERS(bufferdata) ((char **) \
        (&(bufferdata)->bd_flexdata + 6*(niter)))

/* Internal-only AXISDATA MEMBER ACCESS. */
struct NpyIter_AD {
    npy_intp shape, coord;
    npy_intp ad_flexdata;
};
#define NAD_SHAPE(axisdata) ((axisdata)->shape)
#define NAD_COORD(axisdata) ((axisdata)->coord)
#define NAD_STRIDES(axisdata) ( \
        &(axisdata)->ad_flexdata + 0)
#define NAD_PTRS(axisdata) ((char **) \
        &(axisdata)->ad_flexdata + 1*(niter+1))

#define NAD_NSTRIDES() \
        ((niter) + ((itflags&NPY_ITFLAG_HASINDEX) ? 1 : 0))

/* Size of one AXISDATA struct within the iterator */
#define NIT_AXISDATA_SIZEOF(itflags, ndim, niter) (( \
        /* intp shape */ \
        1 + \
        /* intp coord */ \
        1 + \
        /* intp stride[niter+1] AND char* ptr[niter+1] */ \
        2*((niter)+1) \
        )*NPY_SIZEOF_INTP )

/*
 * Macro to advance an AXISDATA pointer by a specified count.
 * Requires that sizeof_axisdata be previously initialized
 * to NIT_AXISDATA_SIZEOF(itflags, ndim, niter).
 */
#define NIT_ADVANCE_AXISDATA(axisdata, count) \
        (*((char **)(&axisdata))) += (count)*sizeof_axisdata
#define NIT_INDEX_AXISDATA(axisdata, index) ((NpyIter_AxisData *) \
        (((char *)(axisdata)) + (index)*sizeof_axisdata))

/* Size of the whole iterator */
#define NIT_SIZEOF_ITERATOR(itflags, ndim, niter) ( \
        sizeof(struct NpyIter_InternalOnly) + \
        NIT_AXISDATA_OFFSET(itflags, ndim, niter) + \
        NIT_AXISDATA_SIZEOF(itflags, ndim, niter)*(ndim))

/* Internal helper functions */
static int
npyiter_check_global_flags(npy_uint32 flags, npy_uint32* itflags);
static int
npyiter_check_op_axes(npy_intp niter, npy_intp oa_ndim, npy_intp **op_axes);
static npy_intp
npyiter_calculate_ndim(npy_intp niter, PyArrayObject **op_in,
                       npy_intp oa_ndim);
static int
npyiter_check_per_op_flags(npy_uint32 flags, char *op_itflags);
static int
npyiter_prepare_one_operand(PyArrayObject **op,
                        char **op_dataptr,
                        PyArray_Descr *op_request_dtype,
                        PyArray_Descr** op_dtype,
                        npy_uint32 op_flags, char *op_itflags);
static int
npyiter_prepare_operands(npy_intp niter, PyArrayObject **op_in,
                    PyArrayObject **op,
                    char **op_dataptr,
                    PyArray_Descr **op_request_dtypes,
                    PyArray_Descr **op_dtype,
                    npy_uint32 *op_flags, char *op_itflags);
static int
npyiter_check_casting(npy_intp niter, PyArrayObject **op,
                    PyArray_Descr **op_dtype,
                    NPY_CASTING casting,
                    char *op_itflags);
static int
npyiter_fill_axisdata(NpyIter *iter, char **op_dataptr,
                      npy_uint32 *op_flags, npy_intp **op_axes);
static void
npyiter_replace_axisdata(NpyIter *iter, npy_intp iiter,
                      PyArrayObject *op,
                      npy_intp op_ndim, char *op_dataptr,
                      npy_intp *op_axes);
static void
npyiter_compute_index_strides(NpyIter *iter, npy_uint32 flags);
static void
npyiter_apply_forced_iteration_order(NpyIter *iter, NPY_ORDER order);

static void
npyiter_flip_negative_strides(NpyIter *iter);
static void
npyiter_reverse_axis_ordering(NpyIter *iter);
static void 
npyiter_find_best_axis_ordering(NpyIter *iter);
static void 
npyiter_coalesce_axes(NpyIter *iter);
static void
npyiter_shrink_ndim(NpyIter *iter, npy_intp new_ndim);

static PyArray_Descr *
npyiter_get_common_dtype(npy_intp niter, PyArrayObject **op,
                        char *op_itflags, PyArray_Descr **op_dtype,
                        int only_inputs, int output_scalars);
static int
npyiter_promote_types(int type1, int type2);
static int
npyiter_can_cast(PyArray_Descr *from, PyArray_Descr *to, NPY_CASTING casting);

static PyArrayObject *
npyiter_new_temp_array(NpyIter *iter, PyTypeObject *subtype,
                npy_intp op_ndim, npy_intp *shape,
                PyArray_Descr *op_dtype, npy_intp *op_axes);
static int
npyiter_allocate_arrays(NpyIter *iter,
                        PyArray_Descr **op_dtype, PyTypeObject *subtype,
                        npy_uint32 *op_flags, char *op_itflags,
                        npy_intp **op_axes, int output_scalars);
static void
npyiter_get_priority_subtype(npy_intp niter, PyArrayObject **op,
                            char *op_itflags,
                            double *subtype_priority, PyTypeObject **subtype);

static int
npyiter_allocate_buffers(NpyIter *iter);
static void npyiter_goto_iterindex(NpyIter *iter, npy_intp iterindex);
static void
npyiter_copy_from_buffers(NpyIter *iter);
static void
npyiter_copy_to_buffers(NpyIter *iter);

/*NUMPY_API
 * Allocate a new iterator for multiple array objects
 */
NPY_NO_EXPORT NpyIter *
NpyIter_MultiNew(npy_intp niter, PyArrayObject **op_in, npy_uint32 flags,
                 NPY_ORDER order, NPY_CASTING casting,
                 npy_uint32 *op_flags,
                 PyArray_Descr **op_request_dtypes,
                 npy_intp oa_ndim, npy_intp **op_axes, npy_intp buffersize)
{
    npy_uint32 itflags = NPY_ITFLAG_IDENTPERM;
    npy_intp idim, ndim;
    npy_intp iiter;

    /* The iterator being constructed */
    NpyIter *iter;

    /* Per-operand values */
    PyArrayObject **op;
    PyArray_Descr **op_dtype;
    char *op_itflags;
    char **op_dataptr;

    npy_intp *perm;
    NpyIter_BufferData *bufferdata = NULL;
    int any_allocate = 0, any_missing_dtypes = 0,
            output_scalars = 0, need_subtype = 0;

    /* The subtype for automatically allocated outputs */
    double subtype_priority = NPY_PRIORITY;
    PyTypeObject *subtype = &PyArray_Type;

    if (niter > NPY_MAXARGS) {
        PyErr_Format(PyExc_ValueError,
            "Cannot construct an iterator with more than %d operands "
            "(%d were requested)", (int)NPY_MAXARGS, (int)niter);
        return NULL;
    }

    /* Error check 'oa_ndim' and 'op_axes', which must be used together */
    if (!npyiter_check_op_axes(niter, oa_ndim, op_axes)) {
        return NULL;
    }

    /* Check the global iterator flags */
    if (!npyiter_check_global_flags(flags, &itflags)) {
        return NULL;
    }

    /* Calculate how many dimensions the iterator should have */
    ndim = npyiter_calculate_ndim(niter, op_in, oa_ndim);

    /* If 'ndim' is zero, any outputs should be scalars */
    if (ndim == 0) {
        output_scalars = 1;
        ndim = 1;
    }

    /* Allocate memory for the iterator */
    iter = (NpyIter*)
                PyArray_malloc(NIT_SIZEOF_ITERATOR(itflags, ndim, niter));

    /* Fill in the basic data */
    NIT_ITFLAGS(iter) = itflags;
    NIT_NDIM(iter) = ndim;
    NIT_NITER(iter) = niter;
    NIT_ITERINDEX(iter) = 0;
    memset(NIT_BASEOFFSETS(iter), 0, (niter+1)*NPY_SIZEOF_INTP);

    op = NIT_OBJECTS(iter);
    op_dtype = NIT_DTYPES(iter);
    op_itflags = NIT_OPITFLAGS(iter);
    op_dataptr = NIT_RESETDATAPTR(iter);

    /* Prepare all the operands */
    if (!npyiter_prepare_operands(niter, op_in, op, op_dataptr,
                        op_request_dtypes, op_dtype,
                        op_flags, op_itflags)) {
        PyArray_free(iter);
        return NULL;
    }
    /* Set resetindex to zero as well (it's just after the resetdataptr) */
    op_dataptr[niter] = 0;

    /*
     * Initialize buffer data (must set the buffers and transferdata
     * to NULL before we might deallocate the iterator).
     */
    if (itflags&NPY_ITFLAG_BUFFER) {
        bufferdata = NIT_BUFFERDATA(iter);
        NBF_SIZE(bufferdata) = 0;
        memset(NBF_BUFFERS(bufferdata), 0, niter*NPY_SIZEOF_INTP);
        memset(NBF_READTRANSFERDATA(bufferdata), 0, niter*NPY_SIZEOF_INTP);
        memset(NBF_WRITETRANSFERDATA(bufferdata), 0, niter*NPY_SIZEOF_INTP);
    }

    /* Fill in the AXISDATA arrays and set the ITERSIZE field */
    if (!npyiter_fill_axisdata(iter, op_dataptr, op_flags, op_axes)) {
        NpyIter_Deallocate(iter);
        return NULL;
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        /*
         * If buffering is enabled and no buffersize was given, use a default
         * chosen to be big enough to get some amortization benefits, but
         * small enough to be cache-friendly.
         */
        if (buffersize <= 0) {
            buffersize = 1 << 12;
        }
        /* No point in a buffer bigger than the iteration size */
        if (buffersize > NIT_ITERSIZE(iter)) {
            buffersize = NIT_ITERSIZE(iter);
        }
        NBF_BUFFERSIZE(bufferdata) = buffersize;
    }

    /*
     * If an index was requested, compute the strides for it.
     * Note that we must do this before changing the order of the
     * axes
     */
    npyiter_compute_index_strides(iter, flags);

    /* Initialize the perm to the identity */
    perm = NIT_PERM(iter);
    for(idim = 0; idim < ndim; ++idim) {
        perm[idim] = idim;
    }

    /*
     * If an iteration order is being forced, apply it.
     */
    npyiter_apply_forced_iteration_order(iter, order);
    itflags = NIT_ITFLAGS(iter);

    /* Set some flags for allocated outputs */
    for (iiter = 0; iiter < niter; ++iiter) {
        if (op[iiter] == NULL) {
            /* Flag this so later we can avoid flipping axes */
            any_allocate = 1;
            /* If a subtype may be used, indicate so */
            if (!(op_flags[iiter]&NPY_ITER_NO_SUBTYPE)) {
                need_subtype = 1;
            }
            /*
             * If the data type wasn't provided, will need to
             * calculate it.
             */
            if (op_dtype[iiter] == NULL) {
                any_missing_dtypes = 1;
            }
        }
    }

    /*
     * If the ordering was not forced, reorder the axes
     * and flip negative strides to find the best one.
     */
    if (!(itflags&NPY_ITFLAG_FORCEDORDER)) {
        if (ndim > 1) {
            npyiter_find_best_axis_ordering(iter);
        }
        /*
         * If there's an output being allocated, we must not negate
         * any strides.
         */
        if (!any_allocate) {
            npyiter_flip_negative_strides(iter);
        }
        itflags = NIT_ITFLAGS(iter);
    }

    if (need_subtype) {
        npyiter_get_priority_subtype(niter, op, op_itflags,
                                     &subtype_priority, &subtype);
    }

    /*
     * If an automatically allocated output didn't have a specified
     * dtype, we need to figure it out now, before allocating the outputs.
     */
    if (any_missing_dtypes || (flags&NPY_ITER_COMMON_DTYPE)) {
        PyArray_Descr *dtype;
        int only_inputs = !(flags&NPY_ITER_COMMON_DTYPE);

        dtype = npyiter_get_common_dtype(niter, op,
                                    op_itflags, op_dtype,
                                    only_inputs,
                                    output_scalars);
        if (dtype == NULL) {
            NpyIter_Deallocate(iter);
            return NULL;
        }
        if (flags&NPY_ITER_COMMON_DTYPE) {
            /* Replace all the data types */
            for (iiter = 0; iiter < niter; ++iiter) {
                Py_XDECREF(op_dtype[iiter]);
                Py_INCREF(dtype);
                op_dtype[iiter] = dtype;
                NIT_DTYPES(iter)[iiter] = dtype;
            }
        }
        else {
            /* Replace the NULL data types */
            for (iiter = 0; iiter < niter; ++iiter) {
                if (op_dtype[iiter] == NULL) {
                    Py_INCREF(dtype);
                    op_dtype[iiter] = dtype;
                    NIT_DTYPES(iter)[iiter] = dtype;
                }
            }
        }
        Py_DECREF(dtype);
    }

    /*
     * All of the data types have been settled, so it's time
     * to check that data type conversions are following the
     * casting rules.
     */
    if (!npyiter_check_casting(niter, op, op_dtype, casting, op_itflags)) {
        NpyIter_Deallocate(iter);
        return NULL;
    }

    /*
     * At this point, the iteration order has been finalized. so
     * any allocation of ops that were NULL, or any temporary
     * copying due to casting/byte order/alignment can be
     * done now using a memory layout matching the iterator.
     */
    if (!npyiter_allocate_arrays(iter, op_dtype, subtype, op_flags,
                            op_itflags, op_axes, output_scalars)) {
        NpyIter_Deallocate(iter);
        return NULL;
    }

    /*
     * Finally, if coords weren't requested,
     * it may be possible to coalesce some axes together.
     */
    if (ndim > 1 && !(itflags&NPY_ITFLAG_HASCOORDS)) {
        npyiter_coalesce_axes(iter);
        itflags = NIT_ITFLAGS(iter);
        ndim = NIT_NDIM(iter);
    }

    /*
     * Now that the axes are finished, check whether we can apply
     * the single iteration optimization to the iternext function.
     */
    if (!(itflags&NPY_ITFLAG_BUFFER)) {
        NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
        if (itflags&NPY_ITFLAG_NOINNER) {
            if (NIT_ITERSIZE(iter) == NAD_SHAPE(axisdata)) {
                NIT_ITFLAGS(iter) |= NPY_ITFLAG_ONEITERATION;
            }
        }
        else if (NIT_ITERSIZE(iter) == 1) {
            NIT_ITFLAGS(iter) |= NPY_ITFLAG_ONEITERATION;
        }
    }

    /* If buffering is set without delayed allocation */
    if (itflags&NPY_ITFLAG_BUFFER) {
        if (itflags&NPY_ITFLAG_DELAYBUF) {
            /* Make the data pointers NULL */
            memset(NBF_PTRS(bufferdata), 0, niter*NPY_SIZEOF_INTP);
        }
        else {
            /* Allocate the buffers */
            if (!npyiter_allocate_buffers(iter)) {
                NpyIter_Deallocate(iter);
                return NULL;
            }

            /* Prepare the next buffers and set iterend/size */
            npyiter_copy_to_buffers(iter);
        }
    }

    return iter;
}

/*NUMPY_API
 * Allocate a new iterator for one array object
 */
NPY_NO_EXPORT NpyIter *
NpyIter_New(PyArrayObject *op, npy_uint32 flags,
                  NPY_ORDER order, NPY_CASTING casting,
                  PyArray_Descr* dtype,
                  npy_intp a_ndim, npy_intp *axes, npy_intp buffersize)
{
    /* Split the flags into separate global and op flags */
    npy_uint32 op_flags = flags&NPY_ITER_PER_OP_FLAGS;
    flags &= NPY_ITER_GLOBAL_FLAGS;

    if (a_ndim > 0) {
        return NpyIter_MultiNew(1, &op, flags, order, casting,
                                &op_flags, &dtype,
                                a_ndim, &axes, buffersize);
    }
    else {
        return NpyIter_MultiNew(1, &op, flags, order, casting,
                                &op_flags, &dtype,
                                0, NULL, buffersize);
    }
}

/*NUMPY_API
 * Makes a copy of the iterator
 */
NPY_NO_EXPORT NpyIter *
NpyIter_Copy(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);
    int out_of_memory = 0;

    npy_intp size;
    NpyIter *newiter;
    PyArrayObject **objects;
    PyArray_Descr **dtypes;

    /* Allocate memory for the new iterator */
    size = NIT_SIZEOF_ITERATOR(itflags, ndim, niter);
    newiter = (NpyIter*)PyArray_malloc(size);

    /* Copy the raw values to the new iterator */
    memcpy(newiter, iter, size);

    /* Take ownership of references to the operands and dtypes */
    objects = NIT_OBJECTS(newiter);
    dtypes = NIT_DTYPES(newiter);
    for (iiter = 0; iiter < niter; ++iiter) {
        Py_INCREF(objects[iiter]);
        Py_INCREF(dtypes[iiter]);
    }

    /* Allocate buffers and make copies of the transfer data if necessary */
    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *bufferdata;
        npy_intp buffersize, itemsize;
        char **buffers;
        void **readtransferdata, **writetransferdata;

        bufferdata = NIT_BUFFERDATA(newiter);
        buffers = NBF_BUFFERS(bufferdata);
        readtransferdata = NBF_READTRANSFERDATA(bufferdata);
        writetransferdata = NBF_WRITETRANSFERDATA(bufferdata);
        buffersize = NBF_BUFFERSIZE(bufferdata);

        for (iiter = 0; iiter < niter; ++iiter) {
            if (buffers[iiter] != NULL) {
                if (out_of_memory) {
                    buffers[iiter] = NULL;
                }
                else {
                    itemsize = dtypes[iiter]->elsize;
                    buffers[iiter] = PyArray_malloc(itemsize*buffersize);
                    if (buffers[iiter] == NULL) {
                        out_of_memory = 1;
                    }
                }
            }

            if (readtransferdata[iiter] != NULL) {
                if (out_of_memory) {
                    readtransferdata[iiter] = NULL;
                }
                else {
                    readtransferdata[iiter] =
                      PyArray_CopyStridedTransferData(readtransferdata[iiter]);
                    if (readtransferdata[iiter] == NULL) {
                        out_of_memory = 1;
                    }
                }
            }

            if (writetransferdata[iiter] != NULL) {
                if (out_of_memory) {
                    writetransferdata[iiter] = NULL;
                }
                else {
                    writetransferdata[iiter] =
                      PyArray_CopyStridedTransferData(writetransferdata[iiter]);
                    if (writetransferdata[iiter] == NULL) {
                        out_of_memory = 1;
                    }
                }
            }
        }

        /* Initialize the buffers to the current iterindex */
        if (!out_of_memory && NBF_SIZE(bufferdata) > 0) {
            npyiter_goto_iterindex(newiter, NIT_ITERINDEX(newiter));

            /* Prepare the next buffers and set iterend/size */
            npyiter_copy_to_buffers(newiter);
        }
    }

    if (out_of_memory) {
        NpyIter_Deallocate(newiter);
        PyErr_NoMemory();
        return NULL;
    }

    return newiter;
}

/*NUMPY_API
 * Deallocate an iterator
 */
NPY_NO_EXPORT int
NpyIter_Deallocate(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    PyArray_Descr **dtype = NIT_DTYPES(iter);
    PyArrayObject **object = NIT_OBJECTS(iter);

    /* Deallocate any buffers and buffering data */
    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
        char **buffers;
        void **transferdata;

        /* buffers */
        buffers = NBF_BUFFERS(bufferdata);
        for(iiter = 0; iiter < niter; ++iiter, ++buffers) {
            if (*buffers) {
                PyArray_free(*buffers);
            }
        }
        /* read bufferdata */
        transferdata = NBF_READTRANSFERDATA(bufferdata);
        for(iiter = 0; iiter < niter; ++iiter, ++transferdata) {
            if (*transferdata) {
                PyArray_FreeStridedTransferData(*transferdata);
            }
        }
        /* write bufferdata */
        transferdata = NBF_WRITETRANSFERDATA(bufferdata);
        for(iiter = 0; iiter < niter; ++iiter, ++transferdata) {
            if (*transferdata) {
                PyArray_FreeStridedTransferData(*transferdata);
            }
        }
    }

    /* Deallocate all the dtypes and objects that were iterated */
    for(iiter = 0; iiter < niter; ++iiter, ++dtype, ++object) {
        Py_XDECREF(*dtype);
        Py_XDECREF(*object);
    }

    /* Deallocate the iterator memory */
    PyArray_free(iter);

    return NPY_SUCCEED;
}

/*NUMPY_API
 * Removes coords support from an iterator
 */
NPY_NO_EXPORT int
NpyIter_RemoveCoords(NpyIter *iter)
{
    npy_uint32 itflags;

    /* Make sure the iterator is reset */
    if (NpyIter_Reset(iter) != NPY_SUCCEED) {
        return NPY_FAIL;
    }

    itflags = NIT_ITFLAGS(iter);
    if (itflags&NPY_ITFLAG_HASCOORDS) {
        NIT_ITFLAGS(iter) = itflags & ~NPY_ITFLAG_HASCOORDS;
        npyiter_coalesce_axes(iter);
    }

    return NPY_SUCCEED;
}

/*NUMPY_API
 * Removes the inner loop handling (so HasInnerLoop returns false)
 */
NPY_NO_EXPORT int
NpyIter_RemoveInnerLoop(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    /* Check conditions under which this can be done */
    if (itflags&(NPY_ITFLAG_HASINDEX|NPY_ITFLAG_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Iterator flag NO_INNER_ITERATION cannot be used "
                "if coords or an index is being tracked");
        return NPY_FAIL;
    }
    if ((itflags&(NPY_ITFLAG_BUFFER|NPY_ITFLAG_RANGE|NPY_ITFLAG_NOINNER))
                        == (NPY_ITFLAG_RANGE|NPY_ITFLAG_NOINNER)) {
        PyErr_SetString(PyExc_ValueError,
                "Iterator flag NO_INNER_ITERATION cannot be used "
                "with ranged iteration unless buffering is also enabled");
        return NPY_FAIL;
    }
    /* Set the flag */
    if (!(itflags&NPY_ITFLAG_NOINNER)) {
        itflags |= NPY_ITFLAG_NOINNER;
        NIT_ITFLAGS(iter) = itflags;
        
        /*
         * Check whether we can apply the single iteration
         * optimization to the iternext function.
         */
        if (!(itflags&NPY_ITFLAG_BUFFER)) {
            NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
            if (NIT_ITERSIZE(iter) == NAD_SHAPE(axisdata)) {
                NIT_ITFLAGS(iter) |= NPY_ITFLAG_ONEITERATION;
            }
        }
    }

    /* Reset the iterator */
    return NpyIter_Reset(iter);
}

/*NUMPY_API
 * Resets the iterator to its initial state
 */
NPY_NO_EXPORT int
NpyIter_Reset(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *bufferdata;

        /* If buffer allocation was delayed, do it now */
        if (itflags&NPY_ITFLAG_DELAYBUF) {
            if (!npyiter_allocate_buffers(iter)) {
                return NPY_FAIL;
            }
            NIT_ITFLAGS(iter) &= ~NPY_ITFLAG_DELAYBUF;
        }
        else {
            /*
             * If the iterindex is already right, no need to
             * do anything
             */
            bufferdata = NIT_BUFFERDATA(iter);
            if (NIT_ITERINDEX(iter) == NIT_ITERSTART(iter) &&
                    NBF_BUFITEREND(bufferdata) <= NIT_ITEREND(iter) &&
                    NBF_SIZE(bufferdata) > 0) {
                return NPY_SUCCEED;
            }

            /* Copy any data from the buffers back to the arrays */
            npyiter_copy_from_buffers(iter);
        }
    }
    
    npyiter_goto_iterindex(iter, NIT_ITERSTART(iter));

    if (itflags&NPY_ITFLAG_BUFFER) {
        /* Prepare the next buffers and set iterend/size */
        npyiter_copy_to_buffers(iter);
    }

    return NPY_SUCCEED;
}

/*NUMPY_API
 * Resets the iterator to its initial state, with new base data pointers
 */
NPY_NO_EXPORT int
NpyIter_ResetBasePointers(NpyIter *iter, char **baseptrs)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char **resetdataptr = NIT_RESETDATAPTR(iter);
    npy_intp *baseoffsets = NIT_BASEOFFSETS(iter);

    if (itflags&NPY_ITFLAG_BUFFER) {
        /* If buffer allocation was delayed, do it now */
        if (itflags&NPY_ITFLAG_DELAYBUF) {
            if (!npyiter_allocate_buffers(iter)) {
                return NPY_FAIL;
            }
            NIT_ITFLAGS(iter) &= ~NPY_ITFLAG_DELAYBUF;
        }
        else {
            /* Copy any data from the buffers back to the arrays */
            npyiter_copy_from_buffers(iter);
        }
    }

    /* The new data pointers for resetting */
    for (iiter = 0; iiter < niter; ++iiter) {
        resetdataptr[iiter] = baseptrs[iiter] + baseoffsets[iiter];
    }
    
    npyiter_goto_iterindex(iter, NIT_ITERSTART(iter));

    if (itflags&NPY_ITFLAG_BUFFER) {
        /* Prepare the next buffers and set iterend/size */
        npyiter_copy_to_buffers(iter);
    }

    return NPY_SUCCEED;
}

/*NUMPY_API
 * Resets the iterator to a new iterator index range
 */
NPY_NO_EXPORT int
NpyIter_ResetToIterIndexRange(NpyIter *iter,
                              npy_intp istart, npy_intp iend)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    /*npy_intp ndim = NIT_NDIM(iter);*/
    /*npy_intp niter = NIT_NITER(iter);*/

    if (!(itflags&NPY_ITFLAG_RANGE)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call ResetToIterIndexRange on an iterator without "
                "requesting ranged iteration support in the constructor");
        return NPY_FAIL;
    }

    if (istart < 0 || iend > NIT_ITERSIZE(iter)) {
        PyErr_Format(PyExc_ValueError,
                "Out-of-bounds range [%d, %d) passed to "
                "ResetToIterIndexRange", (int)istart, (int)iend);
        return NPY_FAIL;
    }
    else if (iend < istart) {
        PyErr_Format(PyExc_ValueError,
                "Invalid range [%d, %d) passed to ResetToIterIndexRange",
                (int)istart, (int)iend);
        return NPY_FAIL;
    }

    NIT_ITERSTART(iter) = istart;
    NIT_ITEREND(iter) = iend;

    return NpyIter_Reset(iter);
}

/*NUMPY_API
 * Sets the iterator to the specified coordinates, which must have the
 * correct number of entries for 'ndim'.  It is only valid
 * when NPY_ITER_COORDS was passed to the constructor.  This operation
 * fails if the coordinates are out of bounds.
 *
 * Returns NPY_SUCCEED on success, NPY_FAIL on failure.
 */
NPY_NO_EXPORT int
NpyIter_GotoCoords(NpyIter *iter, npy_intp *coords)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp iterindex, factor;
    NpyIter_AxisData *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp *perm;

    if (!(itflags&NPY_ITFLAG_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoCoords on an iterator without "
                "requesting coordinates in the constructor");
        return NPY_FAIL;
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoCoords on an iterator which "
                "is buffered");
        return NPY_FAIL;
    }

    if (itflags&NPY_ITFLAG_NOINNER) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoCoords on an iterator which "
                "has the flag NO_INNER_ITERATION");
        return NPY_FAIL;
    }

    perm = NIT_PERM(iter);
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);

    /* Compute the iterindex corresponding to the coordinates */
    iterindex = 0;
    factor = 1;
    for (idim = 0; idim < ndim; ++idim) {
        npy_intp p = perm[idim], i, shape;

        shape = NAD_SHAPE(axisdata);
        if (p < 0) {
            /* If the perm entry is negative, reverse the coordinate */
            i = shape - coords[ndim+p] - 1;
        }
        else {
            i = coords[ndim-p-1];
        }

        /* Bounds-check this coordinate */
        if (i >= 0 && i < shape) {
            iterindex += factor * i;
            factor *= shape;
        }
        else {
            PyErr_SetString(PyExc_IndexError,
                    "Iterator GotoCoords called with out-of-bounds "
                    "coordinates.");
            return NPY_FAIL;
        }
        
        NIT_ADVANCE_AXISDATA(axisdata, 1);
    }

    if (iterindex < NIT_ITERSTART(iter) || iterindex >= NIT_ITEREND(iter)) {
        PyErr_SetString(PyExc_IndexError,
                "Iterator GotoCoords called with coordinates outside the "
                "iteration range.");
        return NPY_FAIL;
    }

    npyiter_goto_iterindex(iter, iterindex);

    return NPY_SUCCEED;
}

/*NUMPY_API
 * If the iterator is tracking an index, sets the iterator
 * to the specified index.
 *
 * Returns NPY_SUCCEED on success, NPY_FAIL on failure.
 */
NPY_NO_EXPORT int
NpyIter_GotoIndex(NpyIter *iter, npy_intp index)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp iterindex, factor;
    NpyIter_AxisData *axisdata;
    npy_intp sizeof_axisdata;

    if (!(itflags&NPY_ITFLAG_HASINDEX)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoIndex on an iterator without "
                "requesting an index in the constructor");
        return NPY_FAIL;
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoIndex on an iterator which "
                "is buffered");
        return NPY_FAIL;
    }

    if (itflags&NPY_ITFLAG_NOINNER) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoIndex on an iterator which "
                "has the flag NO_INNER_ITERATION");
        return NPY_FAIL;
    }

    if (index < 0 || index >= NIT_ITERSIZE(iter)) {
        PyErr_SetString(PyExc_IndexError,
                "Iterator GotoIndex called with an out-of-bounds "
                "index.");
        return NPY_FAIL;
    }

    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);

    /* Compute the iterindex corresponding to the index */
    iterindex = 0;
    factor = 1;
    for (idim = 0; idim < ndim; ++idim) {
        npy_intp i, shape, iterstride;

        iterstride = NAD_STRIDES(axisdata)[niter];
        shape = NAD_SHAPE(axisdata);

        /* Extract the coordinate from the index */
        if (iterstride == 0) {
            i = 0;
        }
        else if (iterstride < 0) {
            i = shape - (index/(-iterstride))%shape - 1;
        }
        else {
            i = (index/iterstride)%shape;
        }

        /* Add its contribution to iterindex */
        iterindex += factor * i;
        factor *= shape;

        NIT_ADVANCE_AXISDATA(axisdata, 1);
    }


    if (iterindex < NIT_ITERSTART(iter) || iterindex >= NIT_ITEREND(iter)) {
        PyErr_SetString(PyExc_IndexError,
                "Iterator GotoIndex called with an index outside the "
                "iteration range.");
        return NPY_FAIL;
    }

    npyiter_goto_iterindex(iter, iterindex);

    return NPY_SUCCEED;
}

/*NUMPY_API
 * Sets the iterator position to the specified iterindex,
 * which matches the iteration order of the iterator.
 *
 * Returns NPY_SUCCEED on success, NPY_FAIL on failure.
 */
NPY_NO_EXPORT int
NpyIter_GotoIterIndex(NpyIter *iter, npy_intp iterindex)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    if (itflags&NPY_ITFLAG_NOINNER) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot call GotoIterIndex on an iterator which "
                "has the flag NO_INNER_ITERATION");
        return NPY_FAIL;
    }

    if (iterindex < NIT_ITERSTART(iter) || iterindex >= NIT_ITEREND(iter)) {
        PyErr_SetString(PyExc_IndexError,
                "Iterator GotoIterIndex called with an iterindex outside the "
                "iteration range.");
        return NPY_FAIL;
    }

    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
        npy_intp bufiterend, size;

        size = NBF_SIZE(bufferdata);
        bufiterend = NBF_BUFITEREND(bufferdata);
        /* Check if the new iterindex is already within the buffer */
        if (iterindex < bufiterend && iterindex >= bufiterend - size) {
            npy_intp *strides, delta;
            char **ptrs;

            strides = NBF_STRIDES(bufferdata);
            ptrs = NBF_PTRS(bufferdata);
            delta = iterindex - NIT_ITERINDEX(iter);

            for (iiter = 0; iiter < niter; ++iiter) {
                ptrs[iiter] += delta * strides[iiter];
            }

            NIT_ITERINDEX(iter) = iterindex;
        }
        /* Start the buffer at the provided iterindex */
        else {
            /* Write back to the arrays */
            npyiter_copy_from_buffers(iter);

            npyiter_goto_iterindex(iter, iterindex);

            /* Prepare the next buffers and set iterend/size */
            npyiter_copy_to_buffers(iter);
        }
    }
    else {
        npyiter_goto_iterindex(iter, iterindex);
    }

    return NPY_SUCCEED;
}

/*NUMPY_API
 * Gets the current iteration index
 */
NPY_NO_EXPORT npy_intp
NpyIter_GetIterIndex(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    /* iterindex is only used if NPY_ITER_RANGED or NPY_ITER_BUFFERED was set */
    if (itflags&(NPY_ITFLAG_RANGE|NPY_ITFLAG_BUFFER)) {
        return NIT_ITERINDEX(iter);
    }
    else {
        npy_intp iterindex;
        NpyIter_AxisData *axisdata;
        npy_intp sizeof_axisdata;

        iterindex = 0;
        sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
        axisdata = NIT_INDEX_AXISDATA(NIT_AXISDATA(iter), ndim-1);

        for (idim = ndim-2; idim >= 0; --idim) {
            iterindex += NAD_COORD(axisdata);
            NIT_ADVANCE_AXISDATA(axisdata, -1);
            iterindex *= NAD_SHAPE(axisdata);
        }
        iterindex += NAD_COORD(axisdata);

        return iterindex;
    }
}

/* SPECIALIZED iternext functions that handle the non-buffering part */

/**begin repeat
 * #const_itflags = 0,
 *                  NPY_ITFLAG_HASINDEX,
 *                  NPY_ITFLAG_NOINNER,
 *                  NPY_ITFLAG_RANGE,
 *                  NPY_ITFLAG_RANGE|NPY_ITFLAG_HASINDEX#
 * #tag_itflags = 0, IND, NOINN, RNG, RNGuIND#
 */
/**begin repeat1
 * #const_ndim = 1, 2, NPY_MAXDIMS#
 * #tag_ndim = 1, 2, ANY#
 */
/**begin repeat2
 * #const_niter = 1, 2, NPY_MAXDIMS#
 * #tag_niter = 1, 2, ANY#
 */

/* Specialized iternext (@const_itflags@,@tag_ndim@,@tag_niter@) */
static int
npyiter_iternext_itflags@tag_itflags@_dims@tag_ndim@_iters@tag_niter@(
                                                      NpyIter *iter)
{
    const npy_uint32 itflags = @const_itflags@;
#if @const_ndim@ < NPY_MAXDIMS
    const npy_intp ndim = @const_ndim@;
#else
    npy_intp idim, ndim = NIT_NDIM(iter);
#endif
#if @const_niter@ < NPY_MAXDIMS
    const npy_intp niter = @const_niter@;
#else
    npy_intp niter = NIT_NITER(iter);
#endif

    npy_intp istrides, nstrides, sizeof_axisdata;
#if @const_ndim@ > 0
    NpyIter_AxisData *axisdata0;
#endif
#if @const_ndim@ > 1
    NpyIter_AxisData *axisdata1;
#endif
#if @const_ndim@ > 2
    NpyIter_AxisData *axisdata2;
#endif

#if (@const_itflags@&NPY_ITFLAG_RANGE)
    /* When ranged iteration is enabled, use the iterindex */
    if (++NIT_ITERINDEX(iter) >= NIT_ITEREND(iter)) {
        return 0;
    }
#endif

    nstrides = NAD_NSTRIDES();
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);

    axisdata0 = NIT_AXISDATA(iter);
#  if !(@const_itflags@&NPY_ITFLAG_NOINNER)
    /* Increment coordinate 0 */
    NAD_COORD(axisdata0)++;
    /* Increment pointer 0 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata0)[istrides] += NAD_STRIDES(axisdata0)[istrides];
    }
#  endif

#if @const_ndim@ == 1

#  if !(@const_itflags@&NPY_ITFLAG_NOINNER)
    /* Finished when the coordinate equals the shape */
    return NAD_COORD(axisdata0) < NAD_SHAPE(axisdata0);
#  else
    /* Get rid of unused variable warning */
    istrides = 0;

    return 0;
#  endif

#else

#  if !(@const_itflags@&NPY_ITFLAG_NOINNER)
    if (NAD_COORD(axisdata0) < NAD_SHAPE(axisdata0)) {
        return 1;
    }
#  endif

    axisdata1 = NIT_INDEX_AXISDATA(axisdata0, 1);
    /* Increment coordinate 1 */
    NAD_COORD(axisdata1)++;
    /* Increment pointer 1 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata1)[istrides] += NAD_STRIDES(axisdata1)[istrides];
    }

    if (NAD_COORD(axisdata1) < NAD_SHAPE(axisdata1)) {
        /* Reset the 1st coordinate to 0 */
        NAD_COORD(axisdata0) = 0;
        /* Reset the 1st pointer to the value of the 2nd */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata0)[istrides] = NAD_PTRS(axisdata1)[istrides];
        }
        return 1;
    }

# if @const_ndim@ == 2
    return 0;
# else
    
    axisdata2 = NIT_INDEX_AXISDATA(axisdata1, 1);
    /* Increment coordinate 2 */
    NAD_COORD(axisdata2)++;
    /* Increment pointer 2 */
    for (istrides = 0; istrides < nstrides; ++istrides) {
        NAD_PTRS(axisdata2)[istrides] += NAD_STRIDES(axisdata2)[istrides];
    }

    if (NAD_COORD(axisdata2) < NAD_SHAPE(axisdata2)) {
        /* Reset the 1st and 2nd coordinates to 0 */
        NAD_COORD(axisdata0) = 0;
        NAD_COORD(axisdata1) = 0;
        /* Reset the 1st and 2nd pointers to the value of the 3nd */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata0)[istrides] = NAD_PTRS(axisdata2)[istrides];
            NAD_PTRS(axisdata1)[istrides] = NAD_PTRS(axisdata2)[istrides];
        }
        return 1;
    }

    for (idim = 3; idim < ndim; ++idim) {
        NIT_ADVANCE_AXISDATA(axisdata2, 1);
        /* Increment the coordinate */
        NAD_COORD(axisdata2)++;
        /* Increment the pointer */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            NAD_PTRS(axisdata2)[istrides] += NAD_STRIDES(axisdata2)[istrides];
        }


        if (NAD_COORD(axisdata2) < NAD_SHAPE(axisdata2)) {
            /* Reset the coordinates and pointers of all previous axisdatas */
            axisdata1 = axisdata2;
            do {
                NIT_ADVANCE_AXISDATA(axisdata1, -1);
                /* Reset the coordinate to 0 */
                NAD_COORD(axisdata1) = 0;
                /* Reset the pointer to the updated value */
                for (istrides = 0; istrides < nstrides; ++istrides) {
                    NAD_PTRS(axisdata1)[istrides] =
                                        NAD_PTRS(axisdata2)[istrides];
                }
            } while (axisdata1 != axisdata0);

            return 1;
        }
    }

    return 0;

# endif /* ndim != 2 */
    
#endif /* ndim != 1 */
}

/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/* iternext function that handle the buffering part */
static int
npyiter_buffered_iternext(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);

    /*
     * If the iterator handles the inner loop, need to increment all
     * the coordinates and pointers
     */
    if (!(itflags&NPY_ITFLAG_NOINNER)) {
        /* Increment within the buffer */
        if (++NIT_ITERINDEX(iter) < NBF_BUFITEREND(bufferdata)) {
            npy_intp iiter, *strides;
            char **ptrs;

            strides = NBF_STRIDES(bufferdata);
            ptrs = NBF_PTRS(bufferdata);
            for (iiter = 0; iiter < niter; ++iiter) {
                ptrs[iiter] += strides[iiter];
            }
            return 1;
        }
    }
    else {
        NIT_ITERINDEX(iter) += NBF_SIZE(bufferdata);
    }

    /* Write back to the arrays */
    npyiter_copy_from_buffers(iter);

    /* Check if we're past the end */
    if (NIT_ITERINDEX(iter) >= NIT_ITEREND(iter)) {
        NBF_SIZE(bufferdata) = 0;
        return 0;
    }
    /* Increment to the next buffer */
    else {
        npyiter_goto_iterindex(iter, NIT_ITERINDEX(iter));
    }

    /* Prepare the next buffers and set iterend/size */
    npyiter_copy_to_buffers(iter);

    return 1;
}

/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/* Specialization of iternext for when the iteration size is 1 */
static int
npyiter_iternext_sizeone(NpyIter *iter)
{
    return 0;
}

/*NUMPY_API
 * Compute the specialized iteration function for an iterator
 */
NPY_NO_EXPORT NpyIter_IterNext_Fn
NpyIter_GetIterNext(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    /*
     * When there is just one iteration and buffering is disabled
     * the iternext function is very simple.
     */
    if (itflags&NPY_ITFLAG_ONEITERATION) {
        return &npyiter_iternext_sizeone;
    }

    /*
     * If buffering is enabled, don't specialize further.
     */
    if (itflags&NPY_ITFLAG_BUFFER) {
        return &npyiter_buffered_iternext;
    }

    /*
     * Ignore all the flags that don't affect the iterator memory
     * layout or the iternext function.  Currently only HASINDEX,
     * NOINNER, and RANGE affect them here.
     */
    itflags &= (NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NOINNER|NPY_ITFLAG_RANGE);

    /* Switch statements let the compiler optimize this most effectively */
    switch (itflags) {
    /*
     * The combinations HASINDEX|NOINNER and RANGE|NOINNER are excluded
     * by the New functions
     */
/**begin repeat
 * #const_itflags = 0,
 *                  NPY_ITFLAG_HASINDEX,
 *                  NPY_ITFLAG_NOINNER,
 *                  NPY_ITFLAG_RANGE,
 *                  NPY_ITFLAG_RANGE|NPY_ITFLAG_HASINDEX#
 * #tag_itflags = 0, IND, NOINN, RNG, RNGuIND#
 */
        case @const_itflags@:
            switch (ndim) {
/**begin repeat1
 * #const_ndim = 1, 2#
 * #tag_ndim = 1, 2#
 */
                case @const_ndim@:
                    switch (niter) {
/**begin repeat2
 * #const_niter = 1, 2#
 * #tag_niter = 1, 2#
 */
                        case @const_niter@:
                            return &npyiter_iternext_itflags@tag_itflags@_dims@tag_ndim@_iters@tag_niter@;
/**end repeat2**/
                        /* Not specialized on niter */
                        default:
                            return &npyiter_iternext_itflags@tag_itflags@_dims@tag_ndim@_itersANY;
                    }
/**end repeat1**/
                /* Not specialized on ndim */
                default:
                    switch (niter) {
/**begin repeat1
 * #const_niter = 1, 2#
 * #tag_niter = 1, 2#
 */
                        case @const_niter@:
                            return &npyiter_iternext_itflags@tag_itflags@_dimsANY_iters@tag_niter@;
/**end repeat1**/
                        /* Not specialized on niter */
                        default:
                            return &npyiter_iternext_itflags@tag_itflags@_dimsANY_itersANY;
                    }
            }
/**end repeat**/
    }
    /* The switch above should have caught all the possibilities. */
    PyErr_Format(PyExc_ValueError,
            "GetIterNext internal iterator error - unexpected "
            "itflags/ndim/niter combination (%04x/%d/%d)",
            (int)itflags, (int)ndim, (int)niter);
    return NULL;
}


/* SPECIALIZED getcoord functions */

/**begin repeat
 * #const_itflags = 0,
 *    NPY_ITFLAG_HASINDEX,
 *    NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER#
 * #tag_itflags = 0, IND, IDP, INDuIDP, NEGP, INDuNEGP,
 *                BUF, INDuBUF, IDPuBUF, INDuIDPuBUF, NEGPuBUF, INDuNEGPuBUF#
 */
static void
npyiter_getcoord_itflags@tag_itflags@(NpyIter *iter, npy_intp *outcoord)
{
    const npy_uint32 itflags = @const_itflags@;
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp idim, sizeof_axisdata;
    NpyIter_AxisData *axisdata;
#if !((@const_itflags@)&NPY_ITFLAG_IDENTPERM)
    npy_intp* perm = NIT_PERM(iter);
#endif

    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
#if ((@const_itflags@)&NPY_ITFLAG_IDENTPERM)
    outcoord += ndim-1;
    for(idim = 0; idim < ndim; ++idim, --outcoord,
                                    NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        *outcoord = NAD_COORD(axisdata);
    }
#elif !((@const_itflags@)&NPY_ITFLAG_NEGPERM)
    for(idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        npy_intp p = perm[idim];
        outcoord[ndim-p-1] = NAD_COORD(axisdata);
    }
#else
    for(idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        npy_intp p = perm[idim];
        if (p < 0) {
            /* If the perm entry is negative, reverse the coordinate */
            outcoord[ndim+p] = NAD_SHAPE(axisdata) - NAD_COORD(axisdata) - 1;
        }
        else {
            outcoord[ndim-p-1] = NAD_COORD(axisdata);
        }
    }
#endif /* not ident perm */
}
/**end repeat**/

/*NUMPY_API
 * Compute a specialized getcoords function for the iterator
 */
NPY_NO_EXPORT NpyIter_GetCoords_Fn
NpyIter_GetGetCoords(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    /* These flags must be correct */
    if ((itflags&(NPY_ITFLAG_HASCOORDS|NPY_ITFLAG_DELAYBUF)) !=
            NPY_ITFLAG_HASCOORDS) {
        if (!(itflags&NPY_ITFLAG_HASCOORDS)) {
            PyErr_SetString(PyExc_ValueError,
                    "Cannot retrieve a GetCoords function for an iterator "
                    "that doesn't track coordinates.");
            return NULL;
        }
        else {
            PyErr_SetString(PyExc_ValueError,
                    "Cannot retrieve a GetCoords function for an iterator "
                    "that used DELAY_BUFALLOC before a Reset call");
            return NULL;
        }
    }

    /*
     * Only these flags affect the iterator memory layout or
     * the getcoords behavior. IDENTPERM and NEGPERM are mutually
     * exclusive, so that reduces the number of cases slightly.
     */
    itflags &= (NPY_ITFLAG_HASINDEX |
                NPY_ITFLAG_IDENTPERM |
                NPY_ITFLAG_NEGPERM |
                NPY_ITFLAG_BUFFER);
    
    switch (itflags) {
/**begin repeat
 * #const_itflags = 0,
 *    NPY_ITFLAG_HASINDEX,
 *    NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM,
 *    NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM,
 *    NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER,
 *    NPY_ITFLAG_HASINDEX|NPY_ITFLAG_NEGPERM|NPY_ITFLAG_BUFFER#
 * #tag_itflags = 0, IND, IDP, INDuIDP, NEGP, INDuNEGP,
 *                BUF, INDuBUF, IDPuBUF, INDuIDPuBUF, NEGPuBUF, INDuNEGPuBUF#
 */
        case @const_itflags@:
            return npyiter_getcoord_itflags@tag_itflags@;
/**end repeat**/
    }
    /* The switch above should have caught all the possibilities. */
    PyErr_Format(PyExc_ValueError,
            "GetGetCoords internal iterator error - unexpected "
            "itflags/ndim/niter combination (%04x/%d/%d)",
            (int)itflags, (int)ndim, (int)niter);
    return NULL;

}

/*NUMPY_API
 * Whether the buffer allocation is being delayed
 */
NPY_NO_EXPORT int
NpyIter_HasDelayedBufAlloc(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_DELAYBUF) != 0;
}

/*NUMPY_API
 * Whether the iterator handles the inner loop
 */
NPY_NO_EXPORT int
NpyIter_HasInnerLoop(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_NOINNER) == 0;
}

/*NUMPY_API
 * Whether the iterator is tracking coordinates
 */
NPY_NO_EXPORT int
NpyIter_HasCoords(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_HASCOORDS) != 0;
}

/*NUMPY_API
 * Whether the iterator is tracking an index
 */
NPY_NO_EXPORT int
NpyIter_HasIndex(NpyIter *iter)
{
    return (NIT_ITFLAGS(iter)&NPY_ITFLAG_HASINDEX) != 0;
}

/*NUMPY_API
 * Gets the number of dimensions being iterated
 */
NPY_NO_EXPORT npy_intp
NpyIter_GetNDim(NpyIter *iter)
{
    return NIT_NDIM(iter);
}

/*NUMPY_API
 * Gets the number of operands being iterated
 */
NPY_NO_EXPORT npy_intp
NpyIter_GetNIter(NpyIter *iter)
{
    return NIT_NITER(iter);
}

/*NUMPY_API
 * Gets the number of elements being iterated
 */
NPY_NO_EXPORT npy_intp
NpyIter_GetIterSize(NpyIter *iter)
{
    return NIT_ITERSIZE(iter);
}

/*NUMPY_API
 * Gets the range of iteration indices being iterated
 */
NPY_NO_EXPORT void
NpyIter_GetIterIndexRange(NpyIter *iter,
                          npy_intp *istart, npy_intp *iend)
{
    *istart = NIT_ITERSTART(iter);
    *iend = NIT_ITEREND(iter);
}

/*NUMPY_API
 * Gets the broadcast shape (if coords are enabled)
 */
NPY_NO_EXPORT int
NpyIter_GetShape(NpyIter *iter, npy_intp *outshape)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp idim, sizeof_axisdata;
    NpyIter_AxisData *axisdata;
    npy_intp* perm;

    if (!(itflags&NPY_ITFLAG_HASCOORDS)) {
        PyErr_SetString(PyExc_ValueError,
                "Cannot get the shape of an iterator "
                "without coordinates requested in the constructor");
        return NPY_FAIL;
    }

    perm = NIT_PERM(iter);
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
    for(idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        npy_intp p = perm[idim];
        if (p < 0) {
            outshape[ndim+p] = NAD_SHAPE(axisdata);
        }
        else {
            outshape[ndim-p-1] = NAD_SHAPE(axisdata);
        }
    }

    return NPY_SUCCEED;
}

/*NUMPY_API
 * Get the array of data pointers (1 per object being iterated)
 */
NPY_NO_EXPORT char **
NpyIter_GetDataPtrArray(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
        return NBF_PTRS(bufferdata);
    }
    else {
        NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
        return NAD_PTRS(axisdata);
    }
}

/*NUMPY_API
 * Get the array of data type pointers (1 per object being iterated)
 */
NPY_NO_EXPORT PyArray_Descr **
NpyIter_GetDescrArray(NpyIter *iter)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    /*npy_intp niter = NIT_NITER(iter);*/

    return NIT_DTYPES(iter);
}

/*NUMPY_API
 * Get the array of objects being iterated
 */
NPY_NO_EXPORT PyArrayObject **
NpyIter_GetObjectArray(NpyIter *iter)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    return NIT_OBJECTS(iter);
}

/*NUMPY_API
 * Returns a view to the i-th object with the iterator's internal axes
 */
NPY_NO_EXPORT PyArrayObject *
NpyIter_GetIterView(NpyIter *iter, npy_intp i)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp shape[NPY_MAXDIMS], strides[NPY_MAXDIMS];
    PyArrayObject *obj, *view;
    PyArray_Descr *dtype;
    char *dataptr;
    NpyIter_AxisData *axisdata;
    npy_intp sizeof_axisdata;
    int writeable;

    if (i < 0 || i >= niter) {
        PyErr_SetString(PyExc_IndexError,
                "index provided for an iterator view was out of bounds");
        return NULL;
    }

    /* Don't provide views if buffering is enabled */
    if (itflags&NPY_ITFLAG_BUFFER) {
        PyErr_SetString(PyExc_ValueError,
                "cannot provide an iterator view when buffering is enabled");
        return NULL;
    }

    obj = NIT_OBJECTS(iter)[i];
    dtype = PyArray_DESCR(obj);
    writeable = NIT_OPITFLAGS(iter)[i]&NPY_OP_ITFLAG_WRITE;
    dataptr = NIT_RESETDATAPTR(iter)[i];
    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);

    /* Retrieve the shape and strides from the axisdata */
    for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        shape[ndim-idim-1] = NAD_SHAPE(axisdata);
        strides[ndim-idim-1] = NAD_STRIDES(axisdata)[i];
    }
    
    Py_INCREF(dtype);
    view = (PyArrayObject *)PyArray_NewFromDescr(&PyArray_Type, dtype, ndim,
                                shape, strides, dataptr,
                                writeable ? NPY_WRITEABLE : 0,
                                NULL);
    if (view == NULL) {
        return NULL;
    }
    /* Tell the view who owns the data */
    Py_INCREF(obj);
    view->base = (PyObject *)obj;
    /* Make sure all the flags are good */
    PyArray_UpdateFlags(view, NPY_UPDATE_ALL);

    return view;
}

/*NUMPY_API
 * Get a pointer to the index, if it is being tracked
 */
NPY_NO_EXPORT npy_intp *
NpyIter_GetIndexPtr(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);

    if (itflags&NPY_ITFLAG_HASINDEX) {
        /* The index is just after the data pointers */
        return (npy_intp*)NAD_PTRS(axisdata) + niter;
    }
    else {
        return NULL;
    }
}

/*NUMPY_API
 * Gets an array of read flags (1 per object being iterated)
 */
NPY_NO_EXPORT void
NpyIter_GetReadFlags(NpyIter *iter, char *outreadflags)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);

    for (iiter = 0; iiter < niter; ++iiter) {
        outreadflags[iiter] = (op_itflags[iiter]&NPY_OP_ITFLAG_READ) != 0;
    }
}

/*NUMPY_API
 * Gets an array of write flags (1 per object being iterated)
 */
NPY_NO_EXPORT void
NpyIter_GetWriteFlags(NpyIter *iter, char *outwriteflags)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);

    for (iiter = 0; iiter < niter; ++iiter) {
        outwriteflags[iiter] = (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE) != 0;
    }
}


/*NUMPY_API
 * Get the array of strides for the inner loop (when HasInnerLoop is false)
 */
NPY_NO_EXPORT npy_intp *
NpyIter_GetInnerStrideArray(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *data = NIT_BUFFERDATA(iter);
        return NBF_STRIDES(data);
    }
    else {
        NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
        return NAD_STRIDES(axisdata);
    }
}

/*NUMPY_API
 * Get a pointer to the size of the inner loop  (when HasInnerLoop is false)
 */
NPY_NO_EXPORT npy_intp *
NpyIter_GetInnerLoopSizePtr(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *data = NIT_BUFFERDATA(iter);
        return &NBF_SIZE(data);
    }
    else {
        NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
        return &NAD_SHAPE(axisdata);
    }
}

/* Checks 'flags' for (C|F)_ORDER_INDEX, COORDS, and NO_INNER_ITERATION,
 * setting the appropriate internal flags in 'itflags'.
 *
 * Returns 1 on success, 0 on error.
 */
static int
npyiter_check_global_flags(npy_uint32 flags, npy_uint32* itflags)
{
    if ((flags&NPY_ITER_PER_OP_FLAGS) != 0) {
        PyErr_SetString(PyExc_ValueError,
                    "A per-operand flag was passed as a global flag "
                    "to the iterator constructor");
        return 0;
    }

    /* Check for an index */
    if (flags&(NPY_ITER_C_INDEX | NPY_ITER_F_INDEX)) {
        if ((flags&(NPY_ITER_C_INDEX | NPY_ITER_F_INDEX)) ==
                    (NPY_ITER_C_INDEX | NPY_ITER_F_INDEX)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flags C_INDEX and "
                    "F_INDEX cannot both be specified");
            return 0;
        }
        (*itflags) |= NPY_ITFLAG_HASINDEX;
    }
    /* Check if coordinates were requested */
    if (flags&NPY_ITER_COORDS) {
        /*
         * This flag primarily disables dimension manipulations that
         * would produce a different set of coordinates.
         */
        (*itflags) |= NPY_ITFLAG_HASCOORDS;
    }
    /* Check if the caller wants to handle inner iteration */
    if (flags&NPY_ITER_NO_INNER_ITERATION) {
        if ((*itflags)&(NPY_ITFLAG_HASINDEX|NPY_ITFLAG_HASCOORDS)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flag NO_INNER_ITERATION cannot be used "
                    "if coords or an index is being tracked");
            return 0;
        }
        (*itflags) |= NPY_ITFLAG_NOINNER;
    }
    /* Ranged */
    if (flags&NPY_ITER_RANGED) {
        (*itflags) |= NPY_ITFLAG_RANGE;
        if ((flags&NPY_ITER_NO_INNER_ITERATION) &&
                                    !(flags&NPY_ITER_BUFFERED)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator flag RANGED cannot be used with "
                    "the flag NO_INNER_ITERATION unless "
                    "BUFFERED is also enabled");
            return 0;
        }
    }
    /* Buffering */
    if (flags&NPY_ITER_BUFFERED) {
        (*itflags) |= NPY_ITFLAG_BUFFER;
        if (flags&NPY_ITER_GROWINNER) {
            (*itflags) |= NPY_ITFLAG_GROWINNER;
        }
        if (flags&NPY_ITER_DELAY_BUFALLOC) {
            (*itflags) |= NPY_ITFLAG_DELAYBUF;
        }
    }

    return 1;
}

static npy_intp
npyiter_calculate_ndim(npy_intp niter, PyArrayObject **op_in,
                       npy_intp oa_ndim)
{
    /* If 'op_axes' is being used, force 'ndim' */
    if (oa_ndim > 0 ) {
        return oa_ndim;
    }
    /* Otherwise it's the maximum 'ndim' from the operands */
    else {
        npy_intp ndim = 0, iiter;

        for (iiter = 0; iiter < niter; ++iiter) {
            if (op_in[iiter] != NULL) {
                npy_intp ondim = PyArray_NDIM(op_in[iiter]);
                if (ondim > ndim) {
                    ndim = ondim;
                }
            }

        }

        return ndim;
    }
}

static int
npyiter_check_op_axes(npy_intp niter, npy_intp oa_ndim, npy_intp **op_axes)
{
    char axes_dupcheck[NPY_MAXDIMS];
    npy_intp iiter, idim;

    if (oa_ndim == 0 && op_axes != NULL) {
        PyErr_Format(PyExc_ValueError,
                "If 'op_axes' is not NULL in the iterator constructor, "
                "'oa_ndim' must be greater than zero");
        return 0;
    }
    else if (oa_ndim > 0) {
        if (oa_ndim > NPY_MAXDIMS) {
            PyErr_Format(PyExc_ValueError,
                "Cannot construct an iterator with more than %d dimensions "
                "(%d were requested for op_axes)",
                (int)NPY_MAXDIMS, (int)oa_ndim);
            return 0;
        }
        else if (op_axes == NULL) {
            PyErr_Format(PyExc_ValueError,
                    "If 'oa_ndim' is greater than zero in the iterator "
                    "constructor, then op_axes cannot be NULL");
            return 0;
        }

        /* Check that there are no duplicates in op_axes */
        for (iiter = 0; iiter < niter; ++iiter) {
            npy_intp *axes = op_axes[iiter];
            if (axes != NULL) {
                memset(axes_dupcheck, 0, NPY_MAXDIMS);
                for (idim = 0; idim < oa_ndim; ++idim) {
                    npy_intp i = axes[idim];
                    if (i >= 0) {
                        if (i >= NPY_MAXDIMS) {
                            PyErr_Format(PyExc_ValueError,
                                    "The 'op_axes' provided to the iterator "
                                    "constructor for operand %d "
                                    "contained invalid "
                                    "values %d", (int)iiter, (int)i);
                            return 0;
                        } else if(axes_dupcheck[i] == 1) {
                            PyErr_Format(PyExc_ValueError,
                                    "The 'op_axes' provided to the iterator "
                                    "constructor for operand %d "
                                    "contained duplicate "
                                    "value %d", (int)iiter, (int)i);
                            return 0;
                        }
                        else {
                            axes_dupcheck[i] = 1;
                        }
                    }
                }
            }
        }
    }

    return 1;
}

/*
 * Checks the per-operand input flags, and fills in op_itflags.
 *
 * Returns 1 on success, 0 on failure.
 */
static int
npyiter_check_per_op_flags(npy_uint32 op_flags, char *op_itflags)
{
    if ((op_flags&NPY_ITER_GLOBAL_FLAGS) != 0) {
        PyErr_SetString(PyExc_ValueError,
                    "A global iterator flag was passed as a per-operand flag "
                    "to the iterator constructor");
        return 0;
    }

    /* Check the read/write flags */
    if (op_flags&NPY_ITER_READONLY) {
        /* The read/write flags are mutually exclusive */
        if (op_flags&(NPY_ITER_READWRITE|NPY_ITER_WRITEONLY)) {
            PyErr_SetString(PyExc_ValueError,
                    "Only one of the iterator flags READWRITE, "
                    "READONLY, and WRITEONLY may be "
                    "specified for an operand");
            return 0;
        }

        *op_itflags = NPY_OP_ITFLAG_READ;
    }
    else if (op_flags&NPY_ITER_READWRITE) {
        /* The read/write flags are mutually exclusive */
        if (op_flags&NPY_ITER_WRITEONLY) {
            PyErr_SetString(PyExc_ValueError,
                    "Only one of the iterator flags READWRITE, "
                    "READONLY, and WRITEONLY may be "
                    "specified for an operand");
            return 0;
        }

        *op_itflags = NPY_OP_ITFLAG_READ|NPY_OP_ITFLAG_WRITE;
    }
    else if(op_flags&NPY_ITER_WRITEONLY) {
        *op_itflags = NPY_OP_ITFLAG_WRITE;
    }
    else {
        PyErr_SetString(PyExc_ValueError,
                "None of the iterator flags READWRITE, "
                "READONLY, or WRITEONLY were "
                "specified for an operand");
        return 0;
    }

    /* Check the flags for temporary copies */
    if (op_flags&(NPY_ITER_COPY|NPY_ITER_UPDATEIFCOPY)) {
        *op_itflags |= NPY_OP_ITFLAG_COPY;
        if (((*op_itflags)&NPY_OP_ITFLAG_WRITE) &&
                    !(op_flags&NPY_ITER_UPDATEIFCOPY)) {
            PyErr_SetString(PyExc_ValueError,
                    "If an iterator operand is writeable, must use "
                    "the flag UPDATEIFCOPY instead of "
                    "COPY");
            return 0;
        }
    }
    return 1;
}

/*
 * Prepares a a constructor operand.  Assumes a reference to 'op'
 * is owned, and that 'op' may be replaced.  Fills in 'op_dtype'
 * and 'ndim'.
 *
 * Returns 1 on success, 0 on failure.
 */
static int
npyiter_prepare_one_operand(PyArrayObject **op,
                        char **op_dataptr,
                        PyArray_Descr *op_request_dtype,
                        PyArray_Descr **op_dtype,
                        npy_uint32 op_flags, char *op_itflags)
{
    /* NULL operands must be automatically allocated outputs */
    if (*op == NULL) {
        /* ALLOCATE should be enabled */
        if (!(op_flags&NPY_ITER_ALLOCATE)) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator input was NULL, but automatic allocation as an "
                    "output wasn't requested");
            return 0;
        }
        /* Reading should be disabled */
        if ((*op_itflags)&NPY_OP_ITFLAG_READ) {
            PyErr_SetString(PyExc_ValueError,
                    "Automatic allocation was requested for an iterator "
                    "operand, but it wasn't flagged as write only");
            return 0;
        }
        *op_dataptr = NULL;
        /* If a requested dtype was provided, use it, otherwise NULL */
        Py_XINCREF(op_request_dtype);
        *op_dtype = op_request_dtype;
        /* No copying of NULL operands */
        *op_itflags &= ~NPY_OP_ITFLAG_COPY;
        return 1;
    }

    if (PyArray_Check(*op)) {
        if (((*op_itflags)&NPY_OP_ITFLAG_WRITE) &&
                    (!PyArray_CHKFLAGS(*op, NPY_WRITEABLE))) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator operand was a non-writeable array, but was "
                    "flagged as writeable");
            return 0;
        }
        if (PyArray_SIZE(*op) == 0) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator does not support iteration of zero-sized "
                    "operands");
            return 0;
        }
        *op_dataptr = PyArray_BYTES(*op);
        /* PyArray_DESCR does not give us a reference */
        *op_dtype = PyArray_DESCR(*op);
        if (*op_dtype == NULL) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator input array object has no dtype descr");
            return 0;
        }
        Py_INCREF(*op_dtype);
        /*
         * Make sure that if the data type has a Python reference or
         * other pointer, WRITEABLE_REFERENCES was specified.
         */
        if (((*op_itflags)&NPY_OP_ITFLAG_WRITE) &&
                        !(op_flags&NPY_ITER_WRITEABLE_REFERENCES)) {
            if (PyDataType_FLAGCHK(*op_dtype, NPY_ITEM_HASOBJECT) ||
                        PyDataType_FLAGCHK(*op_dtype, NPY_ITEM_IS_POINTER)) {
                PyErr_SetString(PyExc_ValueError,
                        "Tried to construct an iterator for a writeable "
                        "array of references/pointers without specifying the "
                        "WRITEABLE_REFERENCES flag.");
                return 0;
            }
        }
        /*
         * Checking whether casts are valid is done later, once the
         * final data types have been selected.  For now, just store the
         * requested type.
         */
        if (op_request_dtype != NULL) {
            /* Store the requested dtype */
            Py_DECREF(*op_dtype);
            Py_INCREF(op_request_dtype);
            *op_dtype = op_request_dtype;
        }

        /* Check if the operand is aligned and in the byte order requested */
        if (op_flags&NPY_ITER_NBO_ALIGNED) {
            /* Check byte order */
            if (!PyArray_ISNBO((*op_dtype)->byteorder)) {
                PyArray_Descr *nbo_dtype;
                
                /* Replace with a new descr which is in native byte order */
                nbo_dtype = PyArray_DescrNewByteorder(*op_dtype, NPY_NATIVE);
                Py_DECREF(*op_dtype);
                *op_dtype = nbo_dtype;

                /* Indicate that byte order or alignment needs fixing */
                *op_itflags |= NPY_OP_ITFLAG_CAST;
            }
            /* Check alignment */
            else if (!PyArray_ISALIGNED(*op)) {
                *op_itflags |= NPY_OP_ITFLAG_CAST;
            }
        }
    }
    else {
        PyErr_SetString(PyExc_ValueError,
                "Iterator inputs must be ndarrays");
        return 0;
    }

    return 1;
}

/*
 * Process all the operands, copying new references so further processing
 * can replace the arrays if copying is necessary.
 */
static int
npyiter_prepare_operands(npy_intp niter, PyArrayObject **op_in,
                    PyArrayObject **op,
                    char **op_dataptr,
                    PyArray_Descr **op_request_dtypes,
                    PyArray_Descr **op_dtype,
                    npy_uint32 *op_flags, char *op_itflags)
{
    npy_intp iiter, i;

    for (iiter = 0; iiter < niter; ++iiter) {
        op[iiter] = op_in[iiter];
        Py_XINCREF(op[iiter]);
        op_dtype[iiter] = NULL;

        /* Check the readonly/writeonly flags, and fill in op_itflags */
        if (!npyiter_check_per_op_flags(op_flags[iiter], &op_itflags[iiter])) {
            for (i = 0; i <= iiter; ++i) {
                Py_XDECREF(op[i]);
                Py_XDECREF(op_dtype[i]);
            }
            return 0;
        }

        /*
         * Prepare the operand.  This produces an op_dtype[iiter] reference
         * on success.
         */
        if (!npyiter_prepare_one_operand(&op[iiter],
                        &op_dataptr[iiter],
                        op_request_dtypes ? op_request_dtypes[iiter] : NULL,
                        &op_dtype[iiter],
                        op_flags[iiter], &op_itflags[iiter])) {
            for (i = 0; i <= iiter; ++i) {
                Py_XDECREF(op[i]);
                Py_XDECREF(op_dtype[i]);
            }
            return 0;
        }
    }


    /* If all the operands were NULL, it's an error */
    if (op[0] == NULL) {
        int all_null = 1;
        for (iiter = 1; iiter < niter; ++iiter) {
            if (op[iiter] != NULL) {
                all_null = 0;
                break;
            }
        }
        if (all_null) {
            npy_intp i;

            for (i = 0; i < niter; ++i) {
                Py_XDECREF(op[i]);
                Py_XDECREF(op_dtype[i]);
            }
            PyErr_SetString(PyExc_ValueError,
                    "At least one iterator input must be non-NULL");
            return 0;
        }
    }

    return 1;
}

/*
 * Returns 1 if the from -> to cast can be done, based on the casting
 * flags provided in op_flags, and 0 otherwise.
 *
 * TODO: Maybe this approach, with the NPY_CASTING enum, should replace
 *       the PyArray_CanCastTo code for NumPy 2.0?
 */
static int
npyiter_can_cast(PyArray_Descr *from, PyArray_Descr *to, NPY_CASTING casting)
{
    /* If unsafe casts are allowed */
    if (casting == NPY_UNSAFE_CASTING) {
        return 1;
    }
    /* Equivalent types can be cast with any value of 'casting'  */
    else if (PyArray_EquivTypenums(from->type_num, to->type_num)) {
        /* If the types are extended, convert to NBO and compare */
        if (PyTypeNum_ISEXTENDED(from->type_num)) {
            int ret;

            /* Only NPY_NO_CASTING prevents byte order conversion */
            if ((casting != NPY_NO_CASTING) &&
                                (!PyArray_ISNBO(from->byteorder) ||
                                 !PyArray_ISNBO(to->byteorder))) {
                PyArray_Descr *nbo_from, *nbo_to;

                nbo_from = PyArray_DescrNewByteorder(from, NPY_NATIVE);
                nbo_to = PyArray_DescrNewByteorder(to, NPY_NATIVE);
                if (nbo_from == NULL || nbo_to == NULL) {
                    Py_XDECREF(nbo_from);
                    Py_XDECREF(nbo_to);
                    PyErr_Clear();
                    return 0;
                }
                ret = PyArray_EquivTypes(nbo_from, nbo_to);
                Py_DECREF(nbo_from);
                Py_DECREF(nbo_to);
            }
            else {
                ret = PyArray_EquivTypes(from, to);
            }
            return ret;
        }

        if (casting == NPY_NO_CASTING) {
            return PyArray_ISNBO(from->byteorder) ==
                   PyArray_ISNBO(to->byteorder);
        }
        else {
            return 1;
        }
    }
    /* If safe or same-kind casts are allowed */
    else if (casting == NPY_SAFE_CASTING || casting == NPY_SAME_KIND_CASTING) {
        if (PyArray_CanCastTo(from, to)) {
            return 1;
        }
        else if(casting == NPY_SAME_KIND_CASTING) {
            /* TODO: Should also allow casting from "lower" to "higher
             *       kinds, but kind is a char so we need to remap to
             *       an ordered version (like NPY_SCALARKIND is ordered).
             */
            return from->kind == to->kind;
        }
        else {
            return 0;
        }
    }
    /* NPY_NO_CASTING or NPY_EQUIV_CASTING was specified */
    else {
        return 0;
    }
}

static const char *
npyiter_casting_to_string(NPY_CASTING casting)
{
    switch (casting) {
        case NPY_NO_CASTING:
            return "'no'";
        case NPY_EQUIV_CASTING:
            return "'equiv'";
        case NPY_SAFE_CASTING:
            return "'safe'";
        case NPY_SAME_KIND_CASTING:
            return "'same_kind'";
        case NPY_UNSAFE_CASTING:
            return "'unsafe'";
        default:
            return "<unknown>";
    }
}

static int
npyiter_check_casting(npy_intp niter, PyArrayObject **op,
                    PyArray_Descr **op_dtype,
                    NPY_CASTING casting,
                    char *op_itflags)
{
    npy_intp iiter;

    for(iiter = 0; iiter < niter; ++iiter) {
        /* If the types aren't equivalent, a cast is necessary */
        if (op[iiter] != NULL && !PyArray_EquivTypes(PyArray_DESCR(op[iiter]),
                                                     op_dtype[iiter])) {
            /* Check read (op -> temp) casting */
            if ((op_itflags[iiter]&NPY_OP_ITFLAG_READ) &&
                        !npyiter_can_cast(PyArray_DESCR(op[iiter]),
                                          op_dtype[iiter],
                                          casting)) {
                PyErr_Format(PyExc_TypeError,
                        "Iterator operand %d dtype could not be cast "
                        "to the requested dtype, according to "
                        "the casting rule given, %s", (int)iiter,
                        npyiter_casting_to_string(casting));
                return 0;
            }
            /* Check write (temp -> op) casting */
            if ((op_itflags[iiter]&NPY_OP_ITFLAG_WRITE) &&
                        !npyiter_can_cast(op_dtype[iiter],
                                          PyArray_DESCR(op[iiter]),
                                          casting)) {
                PyErr_Format(PyExc_TypeError,
                        "Iterator requested dtype could not be cast "
                        "to the operand %d dtype, according to "
                        "the casting rule given, %s", (int)iiter,
                        npyiter_casting_to_string(casting));
                return 0;
            }

            /* Indicate that this operand needs casting */
            op_itflags[iiter] |= NPY_OP_ITFLAG_CAST;
        }
    }

    return 1;
}

/*
 * Fills in the AXISDATA for the 'niter' operands, broadcasting
 * the dimensionas as necessary.  Also fills
 * in the ITERSIZE data member.
 *
 * If op_axes is not NULL, it should point to an array of ndim-sized
 * arrays, one for each op.
 *
 * Returns 1 on success, 0 on failure.
 */
static int
npyiter_fill_axisdata(NpyIter *iter, char **op_dataptr,
                      npy_uint32 *op_flags, npy_intp **op_axes)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp ondim;
    char *odataptr;
    NpyIter_AxisData *axisdata0, *axisdata;
    npy_intp sizeof_axisdata;
    PyArrayObject **op = NIT_OBJECTS(iter);

    axisdata0 = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);

    /* Process the first operand */
    if (op_axes == NULL || op_axes[0] == NULL) {
        /* Default broadcasting rules if op_axes is not specified */
        axisdata = axisdata0;
        ondim = (op[0] == NULL) ? 0 : PyArray_NDIM(op[0]);
        odataptr = op_dataptr[0];
        /* Possible if op_axes are being used, but op_axes[0] is NULL */
        if (ondim > ndim) {
            PyErr_SetString(PyExc_ValueError,
                    "Iterator input has more dimensions than allowed "
                    "by the 'op_axes' specified");
            return 0;
        }
        for (idim = 0; idim < ondim; ++idim) {
            npy_intp shape;

            /* op[0] != NULL, because we set ondim to 0 in that case */
            shape = PyArray_DIM(op[0], ondim-idim-1);

            NAD_SHAPE(axisdata) = shape;
            NAD_COORD(axisdata) = 0;
            if (shape == 1) {
                NAD_STRIDES(axisdata)[0] = 0;
            }
            else {
                NAD_STRIDES(axisdata)[0] = PyArray_STRIDE(op[0], ondim-idim-1);
            }
            NAD_PTRS(axisdata)[0] = odataptr;

            NIT_ADVANCE_AXISDATA(axisdata, 1);
        }
        for (idim = ondim; idim < ndim; ++idim) {
            NAD_SHAPE(axisdata) = 1;
            NAD_COORD(axisdata) = 0;
            NAD_STRIDES(axisdata)[0] = 0;
            NAD_PTRS(axisdata)[0] = odataptr;

            NIT_ADVANCE_AXISDATA(axisdata, 1);
        }
    }
    else {
        npy_intp *axes = op_axes[0];

        /* Use op_axes to choose the axes */
        axisdata = axisdata0;
        ondim = (op[0] == NULL) ? ndim : PyArray_NDIM(op[0]);
        odataptr = op_dataptr[0];
        for (idim = 0; idim < ndim; ++idim) {
            npy_intp i = axes[ndim-idim-1];
            if (i < 0) {
                NAD_SHAPE(axisdata) = 1;
                NAD_COORD(axisdata) = 0;
                NAD_STRIDES(axisdata)[0] = 0;
                NAD_PTRS(axisdata)[0] = odataptr;
            }
            else if (i < ondim) {
                npy_intp shape;
                
                if (op[0] != NULL) {
                    shape = PyArray_DIM(op[0], i);
                }
                else {
                    shape = 1;
                }

                NAD_SHAPE(axisdata) = shape;
                NAD_COORD(axisdata) = 0;
                if (shape == 1) {
                    NAD_STRIDES(axisdata)[0] = 0;
                }
                else {
                    NAD_STRIDES(axisdata)[0] = PyArray_STRIDE(op[0], i);
                }
                NAD_PTRS(axisdata)[0] = odataptr;
            }
            else {
                PyErr_Format(PyExc_ValueError,
                        "Iterator input op_axes[0][%d] (==%d) is not a valid "
                        "axis of op[0], which has %d dimensions",
                        (int)(ndim-idim-1), (int)i, (int)ondim);
                return 0;
            }

            NIT_ADVANCE_AXISDATA(axisdata, 1);
        }
    }

    /*
     * Process the rest of the operands, using the broadcasting rules
     * to combine them.
     */
    for (iiter = 1; iiter < niter; ++iiter) {
        if (op_axes == NULL || op_axes[iiter] == NULL) {
            axisdata = axisdata0;
            ondim = (op[iiter] == NULL) ? 0 : PyArray_NDIM(op[iiter]);
            odataptr = op_dataptr[iiter];
            /* Possible if op_axes are being used, but op_axes[iiter] is NULL */
            if (ondim > ndim) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator input has more dimensions than allowed "
                        "by the 'op_axes' specified");
                return 0;
            }
            for (idim = 0; idim < ondim; ++idim) {
                npy_intp shape;
                
                /* op[iiter] != NULL, because we set ondim to 0 in that case */
                shape = PyArray_DIM(op[iiter], ondim-idim-1);

                if (shape == 1) {
                    NAD_STRIDES(axisdata)[iiter] = 0;
                }
                else {
                    if (NAD_SHAPE(axisdata) == 1) {
                        NAD_SHAPE(axisdata) = shape;
                    }
                    else if (NAD_SHAPE(axisdata) != shape) {
                        PyErr_SetString(PyExc_ValueError,
                                "Iterator input objects cannot be broadcast "
                                "to a single shape");
                        return 0;
                    }
                    NAD_STRIDES(axisdata)[iiter] = PyArray_STRIDE(
                                                      op[iiter], ondim-idim-1);
                }
                NAD_PTRS(axisdata)[iiter] = odataptr;

                NIT_ADVANCE_AXISDATA(axisdata, 1);
            }
            for (idim = ondim; idim < ndim; ++idim) {
                NAD_STRIDES(axisdata)[iiter] = 0;
                NAD_PTRS(axisdata)[iiter] = odataptr;

                NIT_ADVANCE_AXISDATA(axisdata, 1);
            }
        }
        else {
            npy_intp *axes = op_axes[iiter];

            /* Use op_axes to choose the axes */
            axisdata = axisdata0;
            ondim = (op[iiter] == NULL) ? ndim : PyArray_NDIM(op[iiter]);
            odataptr = op_dataptr[iiter];
            for (idim = 0; idim < ndim; ++idim) {
                npy_intp i = axes[ndim-idim-1];
                if (i < 0) {
                    NAD_STRIDES(axisdata)[iiter] = 0;
                    NAD_PTRS(axisdata)[iiter] = odataptr;
                }
                else if (i < ondim) {
                    npy_intp shape;
                    
                    if (op[iiter] != NULL) {
                        shape = PyArray_DIM(op[iiter], i);
                    }
                    else {
                        shape = 1;
                    }

                    if (shape == 1) {
                        NAD_STRIDES(axisdata)[iiter] = 0;
                    }
                    else {
                        if (NAD_SHAPE(axisdata) == 1) {
                            NAD_SHAPE(axisdata) = shape;
                        }
                        else if (NAD_SHAPE(axisdata) != shape) {
                            PyErr_SetString(PyExc_ValueError,
                                    "Iterator input objects cannot be "
                                    "broadcast to a single shape");
                            return 0;
                        }
                        NAD_STRIDES(axisdata)[iiter] =
                                                PyArray_STRIDE(op[iiter], i);
                    }
                    NAD_PTRS(axisdata)[iiter] = odataptr;
                }
                else {
                    PyErr_Format(PyExc_ValueError,
                            "Iterator input op_axes[%d][%d] (==%d) is not a "
                            "valid axis of op[%d], which has %d dimensions ",
                            (int)iiter, (int)(ndim-idim-1), (int)i,
                            (int)iiter, (int)ondim);
                    return 0;
                }

                NIT_ADVANCE_AXISDATA(axisdata, 1);
            }
        }
    }

    /* Go through and check for operands with NPY_ITER_NO_BROADCAST set */
    for (iiter = 0; iiter < niter; ++iiter) {
        if (op_flags[iiter]&NPY_ITER_NO_BROADCAST) {
            npy_intp *axes;

            if (op[iiter] != NULL && PyArray_NDIM(op[iiter]) != ndim) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator input has the NO_BROADCAST flag set, but "
                        "has a different number of dimensions than the "
                        "iterator");
                return 0;
            }

            axes = op_axes ? op_axes[iiter] : NULL;
            axisdata = axisdata0;
            for (idim = 0; idim < ndim; ++idim) {
                npy_intp i;
                if (axes) {
                    i = axes[ndim-idim-1];
                }
                else {
                    i = ndim-idim-1;
                }
                if (op[iiter] != NULL &&
                            PyArray_DIM(op[iiter], i) != NAD_SHAPE(axisdata)) {
                    PyErr_SetString(PyExc_ValueError,
                            "Iterator input has the NO_BROADCAST flag set, "
                            "but has different dimensions than the final "
                            "broadcast shape of the iterator");
                    return 0;
                }
                
                NIT_ADVANCE_AXISDATA(axisdata, 1);
            }
        }
    }

    /* Now fill in the ITERSIZE member */
    NIT_ITERSIZE(iter) = 1;
    axisdata = axisdata0;
    for (idim = 0; idim < ndim; ++idim) {
        NIT_ITERSIZE(iter) *= NAD_SHAPE(axisdata);

        NIT_ADVANCE_AXISDATA(axisdata, 1);
    }
    /* The range defaults to everything */
    NIT_ITERSTART(iter) = 0;
    NIT_ITEREND(iter) = NIT_ITERSIZE(iter);

    return 1;
}

/*
 * Replaces the AXISDATA for the iiter'th operand, broadcasting
 * the dimensions as necessary.  Assumes the replacement array is
 * exactly the same shape as the original array used when
 * npy_fill_axisdata was called.
 *
 * If op_axes is not NULL, it should point to an ndim-sized
 * array.
 */
static void
npyiter_replace_axisdata(NpyIter *iter, npy_intp iiter,
                      PyArrayObject *op,
                      npy_intp op_ndim, char *op_dataptr,
                      npy_intp *op_axes)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    NpyIter_AxisData *axisdata0, *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp *perm;
    npy_intp baseoffset = 0;

    perm = NIT_PERM(iter);
    axisdata0 = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);

    /*
     * Replace just the strides which were non-zero, and compute
     * the base data address.
     */
    axisdata = axisdata0;

    for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        npy_intp i, p, shape;
        
        /* Apply the perm to get the original axis */
        p = perm[idim];
        if (p < 0) {
            i = ndim+p;
        }
        else {
            i = ndim-p-1;
        }

        /* Apply op_axes */
        if (op_axes != NULL) {
            i = op_axes[i];
        }
        else {
            i -= (ndim - op_ndim);
        }

        if (i >= 0 && i < op_ndim) {
            shape = PyArray_DIM(op, i);
            if (shape != 1) {
                npy_intp stride = PyArray_STRIDE(op, i);
                if (p < 0) {
                    /* If the perm entry is negative, flip the axis */
                    NAD_STRIDES(axisdata)[iiter] = -stride;
                    baseoffset += stride*(shape-1);
                }
                else {
                    NAD_STRIDES(axisdata)[iiter] = stride;
                }
            }
        }
    }

    op_dataptr += baseoffset;

    /* Now the base data pointer is calculated, set it everywhere it's needed */
    NIT_RESETDATAPTR(iter)[iiter] = op_dataptr;
    NIT_BASEOFFSETS(iter)[iiter] = baseoffset;
    axisdata = axisdata0;
    for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        NAD_PTRS(axisdata)[iiter] = op_dataptr;
    }
}

/*
 * Computes the iterator's index strides and initializes the index values
 * to zero.
 *
 * This must be called before the axes (i.e. the AXISDATA array) may
 * be reordered.
 */
static void
npyiter_compute_index_strides(NpyIter *iter, npy_uint32 flags)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp indexstride;
    NpyIter_AxisData *axisdata;
    npy_intp sizeof_axisdata;

    /*
     * If there is only one element being iterated, we just have
     * to touch the first AXISDATA because nothing will ever be
     * incremented.
     */
    if (NIT_ITERSIZE(iter) == 1) {
        if (itflags&NPY_ITFLAG_HASINDEX) {
            axisdata = NIT_AXISDATA(iter);
            NAD_PTRS(axisdata)[niter] = 0;
        }
        return;
    }

    if (flags&NPY_ITER_C_INDEX) {
        sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
        axisdata = NIT_AXISDATA(iter);
        indexstride = 1;
        for(idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
            npy_intp shape = NAD_SHAPE(axisdata);

            if (shape == 1) {
                NAD_STRIDES(axisdata)[niter] = 0;
            }
            else {
                NAD_STRIDES(axisdata)[niter] = indexstride;
            }
            NAD_PTRS(axisdata)[niter] = 0;
            indexstride *= shape;
        }
    }
    else if (flags&NPY_ITER_F_INDEX) {
        sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
        axisdata = NIT_INDEX_AXISDATA(NIT_AXISDATA(iter), ndim-1);
        indexstride = 1;
        for(idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, -1)) {
            npy_intp shape = NAD_SHAPE(axisdata);

            if (shape == 1) {
                NAD_STRIDES(axisdata)[niter] = 0;
            }
            else {
                NAD_STRIDES(axisdata)[niter] = indexstride;
            }
            NAD_PTRS(axisdata)[niter] = 0;
            indexstride *= shape;
        }
    }
}

/*
 * If the order is NPY_KEEPORDER, lets the iterator find the best
 * iteration order, otherwise forces it.  Indicates in the itflags that
 * whether the iteration order was forced.
 */
static void
npyiter_apply_forced_iteration_order(NpyIter *iter, NPY_ORDER order)
{
    /*npy_uint32 itflags = NIT_ITFLAGS(iter);*/
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    switch (order) {
    case NPY_CORDER:
        NIT_ITFLAGS(iter) |= NPY_ITFLAG_FORCEDORDER;
        break;
    case NPY_FORTRANORDER:
        NIT_ITFLAGS(iter) |= NPY_ITFLAG_FORCEDORDER;
        /* Only need to actually do something if there is more than 1 dim */
        if (ndim > 1) {
            npyiter_reverse_axis_ordering(iter);
        }
        break;
    case NPY_ANYORDER:
        NIT_ITFLAGS(iter) |= NPY_ITFLAG_FORCEDORDER;
        /* Only need to actually do something if there is more than 1 dim */
        if (ndim > 1) {
            PyArrayObject **op = NIT_OBJECTS(iter);
            int forder = 1;

            /* Check that all the array inputs are fortran order */
            for (iiter = 0; iiter < niter; ++iiter, ++op) {
                if (*op && !PyArray_CHKFLAGS(*op, NPY_F_CONTIGUOUS)) {
                   forder = 0;
                   break;
                }
            }

            if (forder) {
                npyiter_reverse_axis_ordering(iter);
            }
        }
        break;
    case NPY_KEEPORDER:
        /* Don't set the forced order flag here... */
        break;
    }
}


/*
 * This function negates any strides in the iterator
 * which are negative.  When iterating more than one
 * object, it only flips strides when they are all
 * negative or zero.
 */
static void
npyiter_flip_negative_strides(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp istrides, nstrides = NAD_NSTRIDES();
    NpyIter_AxisData *axisdata, *axisdata0;
    npy_intp *baseoffsets;
    npy_intp sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
    int any_flipped = 0;

    axisdata0 = axisdata = NIT_AXISDATA(iter);
    baseoffsets = NIT_BASEOFFSETS(iter);
    for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        npy_intp *strides = NAD_STRIDES(axisdata);
        int any_negative = 0;

        /*
         * Check the signs of all the strides, excluding
         * the index stride at the end.
         */
        for (iiter = 0; iiter < niter; ++iiter) {
            if (strides[iiter] < 0) {
                any_negative = 1;
            }
            else if (strides[iiter] != 0) {
                break;
            }
        }
        /*
         * If at least on stride is negative and none are positive,
         * flip all the strides for this dimension.
         */
        if (any_negative && iiter == niter) {
            npy_intp shapem1 = NAD_SHAPE(axisdata) - 1;

            for (istrides = 0; istrides < nstrides; ++istrides) {
                npy_intp stride = strides[istrides];

                /* Adjust the base pointers to start at the end */
                baseoffsets[istrides] += shapem1 * stride;
                /* Flip the stride */
                strides[istrides] = -stride;
            }
            /* Make the perm entry negative, so getcoords knows it's  flipped */
            NIT_PERM(iter)[idim] = -1-NIT_PERM(iter)[idim];

            any_flipped = 1;
        }
    }

    /*
     * If any strides were flipped, the base pointers were adjusted
     * in the first AXISDATA, and need to be copied to all the rest
     */
    if (any_flipped) {
        char **resetdataptr = NIT_RESETDATAPTR(iter);

        for (istrides = 0; istrides < nstrides; ++istrides) {
            resetdataptr[istrides] += baseoffsets[istrides];
        }
        axisdata = axisdata0;
        for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
            char **ptrs = NAD_PTRS(axisdata);
            for (istrides = 0; istrides < nstrides; ++istrides) {
                ptrs[istrides] = resetdataptr[istrides];
            }
        }
        /*
         * Indicate that some of the perm entries are negative,
         * and that it's not (strictly speaking) the identity perm.
         */
        NIT_ITFLAGS(iter) = (NIT_ITFLAGS(iter)|NPY_ITFLAG_NEGPERM) &
                            ~NPY_ITFLAG_IDENTPERM;
    }
}

static void
npyiter_reverse_axis_ordering(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp i, temp, size;
    npy_intp* first;
    npy_intp* last;

    /* Need at least two dimensions for reversing to change things */
    if (ndim < 2) {
        return;
    }

    size = NIT_AXISDATA_SIZEOF(itflags, ndim, niter)/NPY_SIZEOF_INTP;
    first = (npy_intp*)NIT_AXISDATA(iter);
    last = first + (ndim-1)*size;

    /* This loop reverses the order of the AXISDATA array */
    while (first < last) {
        for (i = 0; i < size; ++i) {
            temp = first[i];
            first[i] = last[i];
            last[i] = temp;
        }
        first += size;
        last -= size;
    }

    /* Store the perm we applied */
    first = NIT_PERM(iter);
    for(i = ndim-1; i >= 0; --i, ++first) {
        *first = i;
    }

    NIT_ITFLAGS(iter) &= ~NPY_ITFLAG_IDENTPERM;
}

static npy_intp intp_abs(npy_intp x)
{
    return (x < 0) ? -x : x;
}

static void 
npyiter_find_best_axis_ordering(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    npy_intp i0, i1, ipos, j0, j1;
    npy_intp *perm;
    NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
    int permuted = 0;

    perm = NIT_PERM(iter);
    
    /*
     * Do a custom stable insertion sort.  Note that because
     * the AXISDATA has been reversed from C order, this
     * is sorting from smallest stride to biggest stride.
     */
    for (i0 = 1; i0 < ndim; ++i0) {
        npy_intp *strides0;

        /* 'ipos' is where perm[i0] will get inserted */
        ipos = i0;
        j0 = perm[i0];

        strides0 = NAD_STRIDES(NIT_INDEX_AXISDATA(axisdata, j0));
        for (i1 = i0-1; i1 >= 0; --i1) {
            int ambig = 1, shouldswap = 0;
            npy_intp *strides1;

            j1 = perm[i1];

            strides1 = NAD_STRIDES(NIT_INDEX_AXISDATA(axisdata, j1));

            for (iiter = 0; iiter < niter; ++iiter) {
                if (strides0[iiter] != 0 && strides1[iiter] != 0) {
                    if (intp_abs(strides1[iiter]) <=
                                            intp_abs(strides0[iiter])) {
                        /*
                         * Set swap even if it's not ambiguous already,
                         * because in the case of conflicts between
                         * different operands, C-order wins.
                         */
                        shouldswap = 0;
                    }
                    else {
                        /* Only set swap if it's still ambiguous */
                        if (ambig) {
                            shouldswap = 1;
                        }
                    }
                    
                    /*
                     * A comparison has been done, so it's
                     * no longer ambiguous
                     */
                    ambig = 0;
                }
            }
            /*
             * If the comparison was unambiguous, either shift
             * 'ipos' to 'i1' or stop looking for an insertion
             * point
             */
            if (!ambig) {
                if (shouldswap) {
                    ipos = i1;
                }
                else {
                    break;
                }
            }
        }

        /* Insert perm[i0] into the right place */
        if (ipos != i0) {
            for (i1 = i0; i1 > ipos; --i1) {
                perm[i1] = perm[i1-1];
            }
            perm[ipos] = j0;
            permuted = 1;
        }
    }

    /* Apply the computed permutation to the AXISDATA array */
    if (permuted == 1) {
        npy_intp i, size = sizeof_axisdata/NPY_SIZEOF_INTP;
        NpyIter_AxisData *ad_i;

        /* Use the coord as a flag, set each to 1 */
        ad_i = axisdata;
        for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(ad_i, 1)) {
            NAD_COORD(ad_i) = 1;
        }
        /* Apply the permutation by following the cycles */
        for (idim = 0; idim < ndim; ++idim) {
            ad_i = NIT_INDEX_AXISDATA(axisdata, idim);

            /* If this axis hasn't been touched yet, process it */
            if (NAD_COORD(ad_i) == 1) {
                npy_intp pidim = perm[idim], qidim, tmp;
                NpyIter_AxisData *ad_p, *ad_q;

                if (pidim != idim) {
                    /* Follow the cycle, copying the data */
                    for (i = 0; i < size; ++i) {
                        qidim = idim;
                        pidim = perm[idim];
                        ad_q = ad_i;
                        tmp = *((npy_intp*)ad_q + i);
                        while (pidim != idim) {
                            ad_p = NIT_INDEX_AXISDATA(axisdata, pidim);
                            *((npy_intp*)ad_q + i) = *((npy_intp*)ad_p + i);

                            qidim = pidim;
                            ad_q = ad_p;
                            pidim = perm[pidim];
                        }
                        *((npy_intp*)ad_q + i) = tmp;
                    }
                    /* Follow the cycle again, marking it as done */
                    pidim = perm[idim];

                    while (pidim != idim) {
                        NAD_COORD(NIT_INDEX_AXISDATA(axisdata, pidim)) = 0;
                        pidim = perm[pidim];
                    }
                }
                NAD_COORD(ad_i) = 0;
            }
        }
        /* Clear the identity perm flag */
        NIT_ITFLAGS(iter) &= ~NPY_ITFLAG_IDENTPERM;
    }
}

static void 
npyiter_coalesce_axes(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp istrides, nstrides = NAD_NSTRIDES();
    NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
    NpyIter_AxisData *ad_compress;
    npy_intp new_ndim = 1;

    /* The HASCOORDS or IDENTPERM flags do not apply after coalescing */
    NIT_ITFLAGS(iter) &= ~(NPY_ITFLAG_IDENTPERM|NPY_ITFLAG_HASCOORDS);

    axisdata = NIT_AXISDATA(iter);
    ad_compress = axisdata;

    for (idim = 0; idim < ndim-1; ++idim) {
        int can_coalesce = 1;
        npy_intp shape0 = NAD_SHAPE(ad_compress);
        npy_intp shape1 = NAD_SHAPE(NIT_INDEX_AXISDATA(axisdata, 1));
        npy_intp *strides0 = NAD_STRIDES(ad_compress);
        npy_intp *strides1 = NAD_STRIDES(NIT_INDEX_AXISDATA(axisdata, 1));

        /* Check that all the axes can be coalesced */
        for (istrides = 0; istrides < nstrides; ++istrides) {
            if (!((shape0 == 1 && strides0[istrides] == 0) ||
                  (shape1 == 1 && strides1[istrides] == 0)) &&
                     (strides0[istrides]*shape0 != strides1[istrides])) {
                can_coalesce = 0;
                break;
            }
        }

        if (can_coalesce) {
            npy_intp *strides = NAD_STRIDES(ad_compress);

            NIT_ADVANCE_AXISDATA(axisdata, 1);
            NAD_SHAPE(ad_compress) *= NAD_SHAPE(axisdata);
            for (istrides = 0; istrides < nstrides; ++istrides) {
                if (strides[istrides] == 0) {
                    strides[istrides] = NAD_STRIDES(axisdata)[istrides];
                }
            }
        }
        else {
            NIT_ADVANCE_AXISDATA(axisdata, 1);
            NIT_ADVANCE_AXISDATA(ad_compress, 1);
            if (ad_compress != axisdata) {
                memcpy(ad_compress, axisdata, sizeof_axisdata);
            }
            ++new_ndim;
        }
    }

    /*
     * If the number of axes shrunk, reset the perm and
     * compress the data into the new layout.
     */
    if (new_ndim < ndim) {
        npy_intp *perm = NIT_PERM(iter);

        /* Reset to an identity perm */
        for (idim = 0; idim < new_ndim; ++idim) {
            perm[idim] = idim;
        }
        npyiter_shrink_ndim(iter, new_ndim);
    }
}

static void
npyiter_shrink_ndim(NpyIter *iter, npy_intp new_ndim)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char *to, *from;
    npy_intp size;

    /*
     * The only place that will shift is perm[ndim], so only one
     * memmove is needed for the data after that.  Also, any
     * extra buffers stored after the main iterator structure shouldn't
     * be moved.
     */
    from = &iter->iter_flexdata + NIT_DTYPES_OFFSET(itflags, ndim, niter);
    to = &iter->iter_flexdata + NIT_DTYPES_OFFSET(itflags, new_ndim, niter);
    size = ((char *)iter) + NIT_SIZEOF_ITERATOR(itflags, new_ndim, niter) - to;
    memmove(to, from, size);
    
    NIT_NDIM(iter) = new_ndim;
}

/*
 * Allocates a temporary array which can be used to replace op
 * in the iteration.  Its dtype will be op_dtype.
 *
 * The result array has a memory ordering which matches the iterator,
 * which may or may not match that of op.  The parameter 'shape' may be
 * NULL, in which case it is filled in from the iterator's shape.
 *
 * This function must be called before any axes are coalesced.
 */
static PyArrayObject *
npyiter_new_temp_array(NpyIter *iter, PyTypeObject *subtype,
                npy_intp op_ndim, npy_intp *shape,
                PyArray_Descr *op_dtype, npy_intp *op_axes)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    npy_intp *perm = NIT_PERM(iter);
    npy_intp new_shape[NPY_MAXDIMS], strides[NPY_MAXDIMS],
             stride = op_dtype->elsize;
    char reversestride[NPY_MAXDIMS], anyreverse = 0;
    NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
    npy_intp sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);

    PyArrayObject *ret;

    /* Initially no strides have been set */
    for (idim = 0; idim < op_ndim; ++idim) {
        strides[idim] = 0;
        reversestride[idim] = 0;
    }

    for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        npy_intp i, p;
        
        /* Apply the perm to get the original axis */
        p = perm[idim];
        if (p < 0) {
            i = ndim+p;
        }
        else {
            i = ndim-p-1;
        }

        /* Apply op_axes */
        if (op_axes != NULL) {
            i = op_axes[i];
        }
        else {
            i -= (ndim - op_ndim);
        }

        if (i >= 0) {
            strides[i] = stride;
            if (p < 0) {
                reversestride[i] = 1;
                anyreverse = 1;
            }
            if (shape == NULL) {
                new_shape[i] = NAD_SHAPE(axisdata);
                stride *= new_shape[i];
            }
            else {
                stride *= shape[i];
            }
        }
    }

    /*
     * If custom axes were specified, some dimensions may not have been used.
     * Throw an error if no shape was specified (i.e. an allocated output),
     * otherwise fill in the rest.
     */
    for (idim = 0; idim < op_ndim; ++idim) {
        if (strides[idim] == 0) {
            npy_intp factor, new_strides[NPY_MAXDIMS],
                     itemsize;
            if (shape == NULL) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator automatically allocated output "
                        "had custom axes specified which don't cover "
                        "all the input axes");
                return NULL;
            }

            /* Fill in the missing strides in C order */
            factor = 1;
            itemsize = op_dtype->elsize;
            for (idim = op_ndim-1; idim >= 0; --idim) {
                if (strides[idim] == 0) {
                    new_strides[idim] = factor * itemsize;
                    factor *= shape[idim];
                }
            }

            /*
             * Copy the missing strides, and multiply the existing strides
             * by the calculated factor.  This way, the missing strides
             * are tighter together in memory, which is good for nested
             * loops.
             */
            for (idim = 0; idim < op_ndim; ++idim) {
                if (strides[idim] == 0) {
                    strides[idim] = new_strides[idim];
                }
                else {
                    strides[idim] *= factor;
                }
            }

            break;
        }
    }

    /* If shape was NULL, set it to the shape we calculated */
    if (shape == NULL) {
        shape = new_shape;
    }

    /* Allocate the temporary array */
    Py_INCREF(op_dtype);
    ret = (PyArrayObject *)PyArray_NewFromDescr(subtype, op_dtype, op_ndim,
                               shape, strides, NULL, 0, NULL);
    if (ret == NULL) {
        return NULL;
    }
    
    /* If there are any reversed axes, create a view that reverses them */
    if (anyreverse) {
        char *dataptr = PyArray_DATA(ret);
        PyArrayObject *newret;

        for (idim = 0; idim < op_ndim; ++idim) {
            if (reversestride[idim]) {
                dataptr += strides[idim]*(shape[idim]-1);
                strides[idim] = -strides[idim];
            }
        }
        Py_INCREF(op_dtype);
        newret = (PyArrayObject *)PyArray_NewFromDescr(subtype,
                              op_dtype, op_ndim,
                              shape, strides, dataptr,
                              NPY_WRITEABLE, NULL);
        if (newret == NULL) {
            Py_DECREF(ret);
            return NULL;
        }
        newret->base = (PyObject *)ret;
        ret = newret;
    }

    /* Make sure all the flags are good */
    PyArray_UpdateFlags(ret, NPY_UPDATE_ALL);

    /* Double-check that the subtype didn't mess with the dimensions */
    if (subtype != &PyArray_Type) {
        if (PyArray_NDIM(ret) != op_ndim ||
                    !PyArray_CompareLists(shape, PyArray_DIMS(ret), op_ndim)) {
            PyErr_SetString(PyExc_RuntimeError,
                    "Iterator automatic output has an array subtype "
                    "which changed the dimensions of the output");
            Py_DECREF(ret);
            return NULL;
        }
    }

    return ret;
}

static int
npyiter_allocate_arrays(NpyIter *iter,
                        PyArray_Descr **op_dtype, PyTypeObject *subtype,
                        npy_uint32 *op_flags, char *op_itflags,
                        npy_intp **op_axes, int output_scalars)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
    PyArrayObject **op = NIT_OBJECTS(iter);

    for (iiter = 0; iiter < niter; ++iiter) {
        if (op[iiter] == NULL) {
            PyArrayObject *out;
            PyTypeObject *op_subtype;
            npy_intp ondim = output_scalars ? 0 : ndim;

            /* Check whether the subtype was disabled */
            op_subtype = (op_flags[iiter]&NPY_ITER_NO_SUBTYPE) ?
                                                &PyArray_Type : subtype;

            /* Allocate the output array */
            out = npyiter_new_temp_array(iter, op_subtype,
                                        ondim,
                                        NULL,
                                        op_dtype[iiter],
                                        op_axes ? op_axes[iiter] : NULL);
            if (out == NULL) {
                return 0;
            }

            op[iiter] = out;

            /*
             * Now we need to replace the pointers and strides with values
             * from the new array.
             */
            npyiter_replace_axisdata(iter, iiter, op[iiter], ondim,
                    PyArray_DATA(op[iiter]), op_axes ? op_axes[iiter] : NULL);

            /* New arrays are aligned and need no swapping or casting */
            op_itflags[iiter] |= NPY_OP_ITFLAG_ALIGNED;
            op_itflags[iiter] &= ~NPY_OP_ITFLAG_CAST;
        }
        else if ((op_itflags[iiter]&NPY_OP_ITFLAG_CAST) &&
                              (op_itflags[iiter]&NPY_OP_ITFLAG_COPY)) {
            PyArrayObject *temp;
            npy_intp ondim = PyArray_NDIM(op[iiter]);

            /* Allocate the temporary array, if possible */
            temp = npyiter_new_temp_array(iter, &PyArray_Type,
                                        ondim,
                                        PyArray_DIMS(op[iiter]),
                                        op_dtype[iiter],
                                        op_axes ? op_axes[iiter] : NULL);
            if (temp == NULL) {
                return 0;
            }

            /* If the data will be read, copy it into temp */
            if (op_itflags[iiter]&NPY_OP_ITFLAG_READ) {
                if (PyArray_CopyInto(temp, op[iiter]) != 0) {
                    Py_DECREF(temp);
                    return 0;
                }
            }
            /* If the data will be written to, set UPDATEIFCOPY */
            if (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE) {
                PyArray_FLAGS(temp) |= NPY_UPDATEIFCOPY;
                PyArray_FLAGS(op[iiter]) &= ~NPY_WRITEABLE;
                Py_INCREF(op[iiter]);
                temp->base = (PyObject *)op[iiter];
            }

            Py_DECREF(op[iiter]);
            op[iiter] = temp;

            /*
             * Now we need to replace the pointers and strides with values
             * from the temporary array.
             */
            npyiter_replace_axisdata(iter, iiter, op[iiter], ondim,
                    PyArray_DATA(op[iiter]), op_axes ? op_axes[iiter] : NULL);

            /* The temporary copy is aligned and needs no swap or cast */
            op_itflags[iiter] |= NPY_OP_ITFLAG_ALIGNED;
            op_itflags[iiter] &= ~NPY_OP_ITFLAG_CAST;
        }
        else {
            /*
             * Buffering must be enabled for casting/conversion if copy
             * wasn't specified.
             */
            if ((op_itflags[iiter]&NPY_OP_ITFLAG_CAST) &&
                                  !(itflags&NPY_ITFLAG_BUFFER)) {
                PyErr_SetString(PyExc_TypeError,
                        "Iterator operand required copying or buffering, "
                        "but neither copying nor buffering was enabled");
                return 0;
            }

            /*
             * If the operand is aligned, any buffering can use aligned
             * optimizations.
             */
            if (PyArray_ISALIGNED(op[iiter])) {
                op_itflags[iiter] |= NPY_OP_ITFLAG_ALIGNED;
            }
        }

        /*
         * If no alignment, byte swap, or casting is needed, and
         * the inner stride of this operand works for the whole
         * array, we can set NPY_OP_ITFLAG_BUFNEVER.
         * But, if buffering is enabled, write-buffering must be
         * one-to-one, because the buffering write back won't combine
         * values correctly. This test doesn't catch everything, but it will
         * catch the most common case of a broadcasting a write-buffered
         * dimension.
         */
        if ((itflags&NPY_ITFLAG_BUFFER) &&
                        (!(op_itflags[iiter]&NPY_OP_ITFLAG_CAST) ||
                          (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE))) {
            int is_one_to_one = 1;
            npy_intp stride, shape, innerstride = 0, innershape;
            NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
            npy_intp sizeof_axisdata =
                                NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
            /* Find stride of the first non-empty shape */
            for (idim = 0; idim < ndim; ++idim) {
                innershape = NAD_SHAPE(axisdata);
                if (innershape != 1) {
                    innerstride = NAD_STRIDES(axisdata)[iiter];
                    if (innerstride == 0) {
                        is_one_to_one = 0;
                    }
                    break;
                }
                NIT_ADVANCE_AXISDATA(axisdata, 1);
            }
            ++idim;
            NIT_ADVANCE_AXISDATA(axisdata, 1);
            /* Check that everything could have coalesced together */
            for (; idim < ndim; ++idim) {
                stride = NAD_STRIDES(axisdata)[iiter];
                shape = NAD_SHAPE(axisdata);
                if (shape != 1) {
                    if (stride == 0) {
                        is_one_to_one = 0;
                    }
                    /*
                     * If N times the inner stride doesn't equal this
                     * stride, the multi-dimensionality is needed.
                     */
                    if (innerstride*innershape != stride) {
                        break;
                    }
                    else {
                        innershape *= shape;
                    }
                }
                NIT_ADVANCE_AXISDATA(axisdata, 1);
            }
            /*
             * If we looped all the way to the end, one stride works.
             * Set that stride, because it may not belong to the first
             * dimension.
             */
            if (idim == ndim && !(op_itflags[iiter]&NPY_OP_ITFLAG_CAST)) {
                op_itflags[iiter] |= NPY_OP_ITFLAG_BUFNEVER;
                NBF_STRIDES(bufferdata)[iiter] = innerstride;
            }
            else if (!is_one_to_one &&
                        (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE)) {
                PyErr_SetString(PyExc_ValueError,
                        "Iterator operand requires write buffering, "
                        "but has dimensions which have been broadcasted "
                        "and would be combined incorrectly");
                return 0;
            }
        }
    }

    return 1;
}

/*
 * The __array_priority__ attribute of the inputs determines
 * the subtype of any output arrays.  This function finds the
 * subtype of the input array with highest priority.
 */
static void
npyiter_get_priority_subtype(npy_intp niter, PyArrayObject **op,
                            char *op_itflags,
                            double *subtype_priority,
                            PyTypeObject **subtype)
{
    npy_intp iiter;

    for (iiter = 0; iiter < niter; ++iiter) {
        if (op_itflags[iiter]&NPY_OP_ITFLAG_READ) {
            double priority = PyArray_GetPriority((PyObject *)op[iiter], 0.0);
            if (priority > *subtype_priority) {
                *subtype_priority = priority;
                *subtype = Py_TYPE(op[iiter]);
            }
        }
    }
}

/*
 * Calculates a dtype that all the types can be promoted to, using the
 * ufunc rules.  If only_inputs is 1, it leaves any operands that
 * are not read from out of the calculation.
 */
static PyArray_Descr *
npyiter_get_common_dtype(npy_intp niter, PyArrayObject **op,
                        char *op_itflags, PyArray_Descr **op_dtype,
                        int only_inputs, int output_scalars)
{
    PyArray_Descr *scalar_dtype = NULL, *array_dtype = NULL;
    int scalar_kind = NPY_NOSCALAR;
    npy_intp iiter;
    /* 0 = don't use, 1 = scalar, 2 = array */
    char op_category[NPY_MAXARGS];

    /* Determine which operands are scalars and which dtypes to use */
    for (iiter = 0; iiter < niter; ++iiter) {
        if (op_dtype[iiter] != NULL &&
                    (!only_inputs || (op_itflags[iiter]&NPY_OP_ITFLAG_READ))) {
            if ((op[iiter] == NULL && output_scalars) ||
                    PyArray_NDIM(op[iiter]) == 0) {
                op_category[iiter] = 1;
            }
            else {
                op_category[iiter] = 2;
            }
        }
        else {
            op_category[iiter] = 0;
        }
    }

    /* First promote all the scalar dtypes */
    for (iiter = 0; iiter < niter; ++iiter) {
        /* Process all the scalar inputs (category = 1) */
        if (op_category[iiter] == 1) {
            if (scalar_dtype == NULL) {
                scalar_dtype = op_dtype[iiter];
                Py_INCREF(scalar_dtype);
                /*
                 * Note: because dtype and op[iiter] may not have the same
                 *       type, this value could be (slightly) wrong in
                 *       very few cases.
                 */
                scalar_kind = PyArray_ScalarKind(scalar_dtype->type_num,
                                                            &op[iiter]);
            }
            else {
                PyArray_Descr *dtype = op_dtype[iiter];
                int kind, typenum;

                if (PyTypeNum_ISEXTENDED(scalar_dtype->type_num) ||
                                PyTypeNum_ISEXTENDED(dtype->type_num)) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_DECREF(scalar_dtype);
                    return NULL;
                }
                /*
                 * Note: because dtype and op[iiter] may not have the same
                 *       type, this value could be (slightly) wrong in
                 *       very few cases.
                 */
                kind = PyArray_ScalarKind(dtype->type_num, &op[iiter]);
                if (kind > scalar_kind) {
                    scalar_kind = kind;
                }
                typenum = npyiter_promote_types(scalar_dtype->type_num,
                                                  dtype->type_num);
                if (typenum == NPY_NOTYPE) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_DECREF(scalar_dtype);
                    return NULL;
                }

                dtype = PyArray_DescrFromType(typenum);
                Py_DECREF(scalar_dtype);
                scalar_dtype = dtype;
            }
        }
    }

    /* Then promote all the array dtypes */
    for (iiter = 0; iiter < niter; ++iiter) {
        /* Process all the array inputs (category = 2) */
        if (op_category[iiter] == 2) {
            if (array_dtype == NULL) {
                array_dtype = op_dtype[iiter];
                Py_INCREF(array_dtype);
            }
            else {
                PyArray_Descr *dtype = op_dtype[iiter];
                int typenum;

                if (PyTypeNum_ISEXTENDED(array_dtype->type_num) ||
                                PyTypeNum_ISEXTENDED(dtype->type_num)) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_XDECREF(scalar_dtype);
                    return NULL;
                }
                typenum = npyiter_promote_types(array_dtype->type_num,
                                                  dtype->type_num);
                if (typenum == NPY_NOTYPE) {
                    PyErr_SetString(PyExc_TypeError,
                            "Iterator allocated output could not "
                            "determine a data type based on the inputs");
                    Py_XDECREF(scalar_dtype);
                    return NULL;
                }
                            
                dtype = PyArray_DescrFromType(typenum);
                Py_DECREF(array_dtype);
                array_dtype = dtype;
            }
        }
    }

    /* Now combine the scalar and array types */
    if (scalar_dtype != NULL && array_dtype != NULL) {
        PyArray_Descr *dtype;
        if (PyArray_CanCoerceScalar(scalar_dtype->type_num,
                                    array_dtype->type_num,
                                    scalar_kind)) {
            dtype = PyArray_DescrFromType(array_dtype->type_num);
            Py_DECREF(scalar_dtype);
            Py_DECREF(array_dtype);
            return dtype;
        }
        else {
            int typenum;

            typenum = npyiter_promote_types(scalar_dtype->type_num,
                                            array_dtype->type_num);
            Py_DECREF(scalar_dtype);
            Py_DECREF(array_dtype);
            return PyArray_DescrFromType(typenum);
        }
    }
    else if (array_dtype != NULL) {
        return array_dtype;
    }
    else if (scalar_dtype != NULL) {
        return scalar_dtype;
    }
    else {
        PyErr_SetString(PyExc_TypeError,
                "Iterator allocated output could not "
                "determine a data type based on the inputs");
        return NULL;
    }
}

/*
 * TODO This is a slow way to do it and depends on the type
 * number ordering.  Should be based on a table like scalar kinds.
 */
static int
npyiter_promote_types(int type1, int type2)
{
    int i;

    for(i = 0; i < NPY_NTYPES; ++i) {
        if (PyArray_CanCastSafely(type1, i) &&
                            PyArray_CanCastSafely(type2, i)) {
            return i;
        }
    }

    return NPY_NOTYPE;
}

static int
npyiter_allocate_buffers(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter = 0, niter = NIT_NITER(iter);

    npy_intp i;
    char *op_itflags = NIT_OPITFLAGS(iter);
    NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
    NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);
    PyArrayObject **op = NIT_OBJECTS(iter);
    PyArray_Descr **op_dtype = NIT_DTYPES(iter);
    npy_intp *strides = NAD_STRIDES(axisdata), op_stride;
    npy_intp buffersize = NBF_BUFFERSIZE(bufferdata);
    char *buffer, **buffers = NBF_BUFFERS(bufferdata);
    PyArray_StridedTransferFn *readtransferfn = NBF_READTRANSFERFN(bufferdata),
                        *writetransferfn = NBF_WRITETRANSFERFN(bufferdata);
    void **readtransferdata = NBF_READTRANSFERDATA(bufferdata),
         **writetransferdata = NBF_WRITETRANSFERDATA(bufferdata);

    PyArray_StridedTransferFn stransfer = NULL;
    void *transferdata = NULL;

    for (iiter = 0; iiter < niter; ++iiter) {
        char flags = op_itflags[iiter];
        op_stride = strides[iiter];

        /*
         * If we have determined that a buffer may be needed,
         * allocate one.
         */
        if (!(flags&NPY_OP_ITFLAG_BUFNEVER)) {
            npy_intp itemsize = op_dtype[iiter]->elsize;
            buffer = PyArray_malloc(itemsize*buffersize);
            if (buffer == NULL) {
                goto fail;
            }
            buffers[iiter] = buffer;

            /* Also need to get an appropriate transfer functions */
            if (flags&NPY_OP_ITFLAG_READ) {
                if (PyArray_GetDTypeTransferFunction(
                                        (flags&NPY_OP_ITFLAG_ALIGNED) != 0,
                                        op_stride,
                                        op_dtype[iiter]->elsize,
                                        PyArray_DESCR(op[iiter]),
                                        op_dtype[iiter],
                                        &stransfer,
                                        &transferdata) != NPY_SUCCEED) {
                    goto fail;
                }
                readtransferfn[iiter] = stransfer;
                readtransferdata[iiter] = transferdata;
            }
            else {
                readtransferfn[iiter] = NULL;
            }
            if (flags&NPY_OP_ITFLAG_WRITE) {
                if (PyArray_GetDTypeTransferFunction(
                                        (flags&NPY_OP_ITFLAG_ALIGNED) != 0,
                                        op_dtype[iiter]->elsize,
                                        op_stride,
                                        op_dtype[iiter],
                                        PyArray_DESCR(op[iiter]),
                                        &stransfer,
                                        &transferdata) != NPY_SUCCEED) {
                    goto fail;
                }
                writetransferfn[iiter] = stransfer;
                writetransferdata[iiter] = transferdata;
            }
            else {
                writetransferfn[iiter] = NULL;
            }
        }
        else {
            readtransferfn[iiter] = NULL;
            writetransferfn[iiter] = NULL;
        }
    }

    return 1;

fail:
    for (i = 0; i < iiter; ++i) {
        if (buffers[i] != NULL) {
            PyArray_free(buffers[i]);
            buffers[i] = NULL;
        }
        if (readtransferdata[iiter] != NULL) {
            PyArray_FreeStridedTransferData(readtransferdata[iiter]);
            readtransferdata[iiter] = NULL;
        }
        if (writetransferdata[iiter] != NULL) {
            PyArray_FreeStridedTransferData(writetransferdata[iiter]);
            writetransferdata[iiter] = NULL;
        }
    }
    PyErr_NoMemory();
    return 0;
}

/*
 * This sets the AXISDATA portion of the iterator to the specified
 * iterindex, updating the pointers as well.  This function does
 * no error checking.
 */
static void
npyiter_goto_iterindex(NpyIter *iter, npy_intp iterindex)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp niter = NIT_NITER(iter);

    char **dataptr;
    NpyIter_AxisData *axisdata;
    npy_intp sizeof_axisdata;
    npy_intp istrides, nstrides, i, shape;

    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
    nstrides = NAD_NSTRIDES();

    NIT_ITERINDEX(iter) = iterindex;

    if (iterindex == 0) {
        dataptr = NIT_RESETDATAPTR(iter);

        for (idim = 0; idim < ndim; ++idim) {
            char **ptrs;
            NAD_COORD(axisdata) = 0;
            ptrs = NAD_PTRS(axisdata);
            for (istrides = 0; istrides < nstrides; ++istrides) {
                ptrs[istrides] = dataptr[istrides];
            }

            NIT_ADVANCE_AXISDATA(axisdata, 1);
        }
    }
    else {
        /*
         * Set the coordinates, from the fastest-changing to the
         * slowest-changing.
         */
        axisdata = NIT_AXISDATA(iter);
        shape = NAD_SHAPE(axisdata);
        i = iterindex;
        iterindex /= shape;
        NAD_COORD(axisdata) = i - iterindex * shape;
        for (idim = 0; idim < ndim-1; ++idim) {
            NIT_ADVANCE_AXISDATA(axisdata, 1);

            shape = NAD_SHAPE(axisdata);
            i = iterindex;
            iterindex /= shape;
            NAD_COORD(axisdata) = i - iterindex * shape;
        }

        dataptr = NIT_RESETDATAPTR(iter);

        /*
         * Accumulate the successive pointers with their
         * offsets in the opposite order, starting from the
         * original data pointers.
         */
        for (idim = 0; idim < ndim; ++idim) {
            npy_intp *strides;
            char **ptrs;

            strides = NAD_STRIDES(axisdata);
            ptrs = NAD_PTRS(axisdata);

            i = NAD_COORD(axisdata);

            for (istrides = 0; istrides < nstrides; ++istrides) {
                ptrs[istrides] = dataptr[istrides] + i*strides[istrides];
            }

            dataptr = ptrs;
            
            NIT_ADVANCE_AXISDATA(axisdata, -1);
        }
    }
}

/*
 * This gets called after the the buffers have been exhausted, and
 * their data needs to be written back to the arrays.  The coordinates
 * must be positioned for the beginning of the buffer.
 */
static void
npyiter_copy_from_buffers(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);
    NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
    NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);

    PyArray_Descr **dtypes = NIT_DTYPES(iter);
    npy_intp transfersize = NBF_SIZE(bufferdata);
    npy_intp *strides = NBF_STRIDES(bufferdata),
             *ad_strides = NAD_STRIDES(axisdata);
    char **ptrs = NBF_PTRS(bufferdata), **ad_ptrs = NAD_PTRS(axisdata);
    char **buffers = NBF_BUFFERS(bufferdata);

    PyArray_StridedTransferFn stransfer = NULL;
    void *transferdata = NULL;
    
    npy_intp axisdata_incr = NIT_AXISDATA_SIZEOF(itflags, ndim, niter) /
                                NPY_SIZEOF_INTP;

    /* If we're past the end, nothing to copy */
    if (NBF_SIZE(bufferdata) == 0) {
        return;
    }

    for (iiter = 0; iiter < niter; ++iiter) {
        stransfer = NBF_WRITETRANSFERFN(bufferdata)[iiter];
        transferdata = NBF_WRITETRANSFERDATA(bufferdata)[iiter];
        if ((stransfer != NULL) && (op_itflags[iiter]&NPY_OP_ITFLAG_WRITE)) {
            /* Copy back only if the pointer was pointing to the buffer */
            npy_intp delta = (ptrs[iiter] - buffers[iiter]);
            if (0 <= delta && delta <= transfersize*dtypes[iiter]->elsize) {
                PyArray_TransferStridedToNDim(ndim,
                        ad_ptrs[iiter], &ad_strides[iiter], axisdata_incr,
                        buffers[iiter], strides[iiter],
                        &NAD_COORD(axisdata), axisdata_incr,
                        &NAD_SHAPE(axisdata), axisdata_incr,
                        transfersize, dtypes[iiter]->elsize,
                        stransfer,
                        transferdata);
            }
        }
    }
}

/*
 * This gets called after the iterator has been positioned to coordinates
 * for the start of a buffer.  It decides which operands need a buffer,
 * and copies the data into the buffers.
 */
static void
npyiter_copy_to_buffers(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    char *op_itflags = NIT_OPITFLAGS(iter);
    NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
    NpyIter_AxisData *axisdata = NIT_AXISDATA(iter);

    PyArray_Descr **dtypes = NIT_DTYPES(iter);
    npy_intp *strides = NBF_STRIDES(bufferdata),
             *ad_strides = NAD_STRIDES(axisdata);
    char **ptrs = NBF_PTRS(bufferdata), **ad_ptrs = NAD_PTRS(axisdata);
    char **buffers = NBF_BUFFERS(bufferdata);
    npy_intp iterindex, iterend, transfersize, singlestridesize;
    int is_onestride = 0, any_buffered = 0;

    PyArray_StridedTransferFn stransfer = NULL;
    void *transferdata = NULL;

    npy_intp axisdata_incr = NIT_AXISDATA_SIZEOF(itflags, ndim, niter) /
                                NPY_SIZEOF_INTP;

    /* Calculate the size if using any buffers */
    iterindex = NIT_ITERINDEX(iter);
    iterend = NIT_ITEREND(iter);
    transfersize = NBF_BUFFERSIZE(bufferdata);
    if (transfersize > iterend - iterindex) {
        transfersize = iterend - iterindex;
    }
    NBF_SIZE(bufferdata) = transfersize;
    NBF_BUFITEREND(bufferdata) = iterindex + transfersize;

    /* Calculate the maximum size if using a single stride and no buffers */
    singlestridesize = NAD_SHAPE(axisdata)-NAD_COORD(axisdata);
    if (singlestridesize > iterend - iterindex) {
        singlestridesize = iterend - iterindex;
    }
    if (singlestridesize >= transfersize) {
        is_onestride = 1;
    }

    for (iiter = 0; iiter < niter; ++iiter) {
        /*
         * If the buffer is write-only, these two are NULL, and the buffer
         * pointers will be set up but the read copy won't be done
         */
        stransfer = NBF_READTRANSFERFN(bufferdata)[iiter];
        transferdata = NBF_READTRANSFERDATA(bufferdata)[iiter];
        switch (op_itflags[iiter]&
                        (NPY_OP_ITFLAG_BUFNEVER|NPY_OP_ITFLAG_CAST)) {
            /* never need to buffer this operand */
            case NPY_OP_ITFLAG_BUFNEVER:
                ptrs[iiter] = ad_ptrs[iiter];
                /*
                 * Should not adjust the stride - ad_strides[iiter]
                 * could be zero, but strides[iiter] was initialized
                 * to the first non-trivial stride.
                 */
                stransfer = NULL;
                break;
            /* just a copy */
            case 0:
                /*
                 * No copyswap or cast was requested, so all we're
                 * doing is copying the data to fill the buffer and
                 * produce a single stride.  If the underlying data
                 * already does that, no need to copy it.
                 */
                if (is_onestride) {
                    ptrs[iiter] = ad_ptrs[iiter];
                    strides[iiter] = ad_strides[iiter];
                    stransfer = NULL;
                }
                else {
                    /* In this case, the buffer is being used */
                    ptrs[iiter] = buffers[iiter];
                    strides[iiter] = dtypes[iiter]->elsize;
                }
                break;
            default:
                /* In this case, the buffer is being used */
                ptrs[iiter] = buffers[iiter];
                strides[iiter] = dtypes[iiter]->elsize;
                break;
        }

        if (stransfer != NULL) {
            /*printf("transfer %p -> %p\n", ad_ptrs[iiter], ptrs[iiter]);*/
            any_buffered = 1;
            PyArray_TransferNDimToStrided(ndim,
                    ptrs[iiter], strides[iiter],
                    ad_ptrs[iiter], &ad_strides[iiter], axisdata_incr,
                    &NAD_COORD(axisdata), axisdata_incr,
                    &NAD_SHAPE(axisdata), axisdata_incr,
                    transfersize, dtypes[iiter]->elsize,
                    stransfer,
                    transferdata);
        }

    }

    /*
     * If buffering wasn't needed, we can grow the inner
     * loop to as large as possible.
     */
    if (!any_buffered && (itflags&NPY_ITFLAG_GROWINNER)) {
        if (singlestridesize > transfersize) {
            NBF_SIZE(bufferdata) = singlestridesize;
            NBF_BUFITEREND(bufferdata) = iterindex + singlestridesize;
        }
    }
}

/*NUMPY_API
 * For debugging
 */
NPY_NO_EXPORT void
NpyIter_DebugPrint(NpyIter *iter)
{
    npy_uint32 itflags = NIT_ITFLAGS(iter);
    npy_intp idim, ndim = NIT_NDIM(iter);
    npy_intp iiter, niter = NIT_NITER(iter);

    NpyIter_AxisData *axisdata;
    npy_intp sizeof_axisdata;

    printf("\n------ BEGIN ITERATOR DUMP ------\n");
    printf("Iterator Address: %p\n", iter);
    printf("ItFlags: ");
    if (itflags&NPY_ITFLAG_IDENTPERM)
        printf("IDENTPERM ");
    if (itflags&NPY_ITFLAG_NEGPERM)
        printf("NEGPERM ");
    if (itflags&NPY_ITFLAG_HASINDEX)
        printf("HASINDEX ");
    if (itflags&NPY_ITFLAG_HASCOORDS)
        printf("HASCOORDS ");
    if (itflags&NPY_ITFLAG_FORCEDORDER)
        printf("FORCEDORDER ");
    if (itflags&NPY_ITFLAG_NOINNER)
        printf("NOINNER ");
    if (itflags&NPY_ITFLAG_RANGE)
        printf("RANGE ");
    if (itflags&NPY_ITFLAG_BUFFER)
        printf("BUFFER ");
    if (itflags&NPY_ITFLAG_GROWINNER)
        printf("GROWINNER ");
    if (itflags&NPY_ITFLAG_ONEITERATION)
        printf("ONEITERATION ");
    if (itflags&NPY_ITFLAG_DELAYBUF)
        printf("DELAYBUF ");
    printf("\n");
    printf("NDim: %d\n", (int)ndim);
    printf("NIter: %d\n", (int)niter);
    printf("IterSize: %d\n", (int)NIT_ITERSIZE(iter));
    printf("IterStart: %d\n", (int)NIT_ITERSTART(iter));
    printf("IterEnd: %d\n", (int)NIT_ITEREND(iter));
    printf("IterIndex: %d\n", (int)NIT_ITERINDEX(iter));
    printf("Iterator SizeOf: %d\n",
                            (int)NIT_SIZEOF_ITERATOR(itflags, ndim, niter));
    printf("BufferData SizeOf: %d\n",
                            (int)NIT_BUFFERDATA_SIZEOF(itflags, ndim, niter));
    printf("AxisData SizeOf: %d\n",
                            (int)NIT_AXISDATA_SIZEOF(itflags, ndim, niter));
    printf("\n");

    printf("Perm: ");
    for (idim = 0; idim < ndim; ++idim) {
        printf("%d ", (int)NIT_PERM(iter)[idim]);
    }
    printf("\n");
    printf("DTypes: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_DTYPES(iter)[iiter]);
    }
    printf("\n");
    printf("DTypes: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        if (NIT_DTYPES(iter)[iiter] != NULL)
            PyObject_Print((PyObject*)NIT_DTYPES(iter)[iiter], stdout, 0);
        else
            printf("(nil) ");
        printf(" "); 
    }
    printf("\n");
    printf("InitDataPtrs: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_RESETDATAPTR(iter)[iiter]);
    }
    printf("\n");
    printf("BaseOffsets: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%i ", (int)NIT_BASEOFFSETS(iter)[iiter]);
    }
    printf("\n");
    if (itflags&NPY_ITFLAG_HASINDEX) {
        printf("InitIndex: %d\n",
                        (int)(npy_intp)NIT_RESETDATAPTR(iter)[niter]);
    }
    printf("Objects: ");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("%p ", NIT_OBJECTS(iter)[iiter]);
    }
    printf("\n");
    printf("OpItFlags:\n");
    for (iiter = 0; iiter < niter; ++iiter) {
        printf("  Flags[%d]: ", (int)iiter);
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_READ)
            printf("READ ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_WRITE)
            printf("WRITE ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_COPY)
            printf("COPY ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_CAST)
            printf("CAST ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_BUFNEVER)
            printf("BUFNEVER ");
        if ((NIT_OPITFLAGS(iter)[iiter])&NPY_OP_ITFLAG_ALIGNED)
            printf("ALIGNED ");
        printf("\n");
    }
    printf("\n");

    if (itflags&NPY_ITFLAG_BUFFER) {
        NpyIter_BufferData *bufferdata = NIT_BUFFERDATA(iter);
        printf("BufferData:\n");
        printf("  BufferSize: %d\n", (int)NBF_BUFFERSIZE(bufferdata));
        printf("  Size: %d\n", (int)NBF_SIZE(bufferdata));
        printf("  BufIterEnd: %d\n", (int)NBF_BUFITEREND(bufferdata));
        printf("  Strides: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%d ", (int)NBF_STRIDES(bufferdata)[iiter]);
        printf("\n");
        printf("  Ptrs: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_PTRS(bufferdata)[iiter]);
        printf("\n");
        printf("  ReadTransferFn: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_READTRANSFERFN(bufferdata)[iiter]);
        printf("\n");
        printf("  ReadTransferData: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_READTRANSFERDATA(bufferdata)[iiter]);
        printf("\n");
        printf("  WriteTransferFn: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_WRITETRANSFERFN(bufferdata)[iiter]);
        printf("\n");
        printf("  WriteTransferData: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_WRITETRANSFERDATA(bufferdata)[iiter]);
        printf("\n");
        printf("  Buffers: ");
        for (iiter = 0; iiter < niter; ++iiter)
            printf("%p ", NBF_BUFFERS(bufferdata)[iiter]);
        printf("\n");
        printf("\n");
    }

    axisdata = NIT_AXISDATA(iter);
    sizeof_axisdata = NIT_AXISDATA_SIZEOF(itflags, ndim, niter);
    for (idim = 0; idim < ndim; ++idim, NIT_ADVANCE_AXISDATA(axisdata, 1)) {
        printf("AxisData[%d]:\n", (int)idim);
        printf("  Shape: %d\n", (int)NAD_SHAPE(axisdata));
        printf("  Coord: %d\n", (int)NAD_COORD(axisdata));
        printf("  Strides: ");
        for (iiter = 0; iiter < niter; ++iiter) {
            printf("%d ", (int)NAD_STRIDES(axisdata)[iiter]);
        }
        printf("\n");
        if (itflags&NPY_ITFLAG_HASINDEX) {
            printf("  Index Stride: %d\n", (int)NAD_STRIDES(axisdata)[niter]);
        }
        printf("  Ptrs: ");
        for (iiter = 0; iiter < niter; ++iiter) {
            printf("%p ", NAD_PTRS(axisdata)[iiter]);
        }
        printf("\n");
        if (itflags&NPY_ITFLAG_HASINDEX) {
            printf("  Index Value: %d\n",
                               (int)((npy_intp*)NAD_PTRS(axisdata))[niter]);
        }
    }

    printf("------- END ITERATOR DUMP -------\n");
}

